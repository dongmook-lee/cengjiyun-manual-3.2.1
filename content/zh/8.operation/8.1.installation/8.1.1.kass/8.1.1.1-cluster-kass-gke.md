---
title: "8.1.1.1 GKE 클러스터 만들기"
---

## GKE 클러스터를 배포하는 방법을 알아봅니다.

### Google Kubernetes Engine(GKE)에 로그인

<https://cloud.google.com/kubernetes-engine/> 에서 Google cloud에 로그인합니다.

### 서비스 계정 만들기

### 1. IAM 및 관리자에서 Service Account 만들기.

* 서비스 계정에는 다음 역할이 필요합니다.

    * project/viewer

    * kubernetes-engine/admin

    * service-account/user

    * 역할 기반 액세스 제어(Identity and Access Management)를 사용하기 위한 필요 조건:  
    **rbac 생성시 사전 필수 요소 필요**.

        * 다음 명령어를 실행하여 Kubernetes에서 역할을 만들 수 있는 능력을 사용자에게 부여해야 합니다. [USER_ACCOUNT]는 사용자의 이메일 주소입니다.  
        참조: <https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control>

        ```bash
        kubectl create clusterrolebinding cluster-admin-binding \
        --clusterrole cluster-admin --user [USER_ACCOUNT]
        ```

        or

        ```bash
        kubectl create clusterrolebinding cluster-admin-binding \
        --clusterrole cluster-admin \
        --user $(gcloud config get-value account)
        ```


* 서비스 계졍 만들기를 선택합니다.  
**참조:** [서비스 계정으로 Cloud Platform 인증](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform)

    * 서비스 계정 세부 정보 

        * **서비스 계정 이름:**  
        이 서비스 계정의 표시 이름입니다.

        * **만들기**를 선택합니다.

    * 서비스 계정 권한(선택사항)

        * **역할:**  
        이 사용 설명서에서는 편의상 소유자 권한을 부여합니다.

    * Private key 만들기

        * **CREATE KEY** 선택합니다.

            * **CREATE** 합니다.  
            서비스 계정이 생성되었으면 서비스 계정의 사용자 인증 정보가 포함된 JSON 키 파일이 컴퓨터에 다운로드됩니다. 이 키 파일은 사용자 API에 인증을 수행하도록 애플리케이션을 구성하기 위해 사용됩니다.

    ![gke-create-service-account-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-service-account-1.png)
    ![gke-create-service-account-2]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-service-account-2.png)
    ![gke-create-service-account-3]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-service-account-3.png)


### GKE 클러스터 만들기
**참조:** [클러스터 아키텍처](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture)

### 1. 프로젝트 만들기
**참조:** <https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster>

* 프로젝트 만들기를 선택합니다.[자세히 알아보기](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

    * **Project Name:**  
    프로젝트 이름을 입력합니다.

        * **Preject ID**  
        프로젝트 ID는 프로젝트의 글로벌 고유 식별자입니다. 프로젝트를 만든 후에는 프로젝트 ID를 변경할 수 없습니다.  
        프로젝트를 만들 때 또는 프로젝트 ID를 만들어야 하는 API를 활성화할 때 선택한 맞춤설정된 이름입니다. 삭제된 프로젝트의 프로젝트 ID는 재사용할 수 없습니다.

    * **Location:**  
    상위 조직 또는 폴더를 선택 합니다.

    ![gke-create-project-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-project-1-1.png)


### 1.1 Google Kubernetes Engine API가 사용 설정되었는지 확인합니다.

    **GOOGLE KUBERNETES ENGINEAPI 사용 설정됨** 을 확인 합니다.

    ![gke-create-api-lib-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-api-lib-1.png)

* [Cloud SDK](https://cloud.google.com/sdk/downloads?hl=ko)가 설치되었는지 확인합니다.

    * gcloud 명령줄 도구의 기본값을 설정합니다.  
    gcloud 명령줄 도구에서 **프로젝트 ID** 및 **Compute Engine 영역** 옵션을 입력하는 시간을 절약하기 위해 기본값을 설정할 수 있습니다.

        ```
        gcloud config set project [PROJECT_ID]
        gcloud config set compute/zone us-central1-b
        ```

    * gcloud를 최신 버전으로 업데이트합니다.

        ```
        gcloud components update
        ```

### 2. VPC 네트워크 만들기

* VPC 네트워크 만들기 선택

    * **이름 :**  
    VPC에 고유 이름을 부여합니다 

    * **서브넷 생성**:  
    서브넷을 사용하면 Google Cloud 내에 자체 비공개 클라우드 토폴로지를 만들 수 있습니다. 각 지역에 서브넷을 만들려면 '자동'을 클릭하고, 서브넷을 직접 정의하려면 '맞춤설정'을 클릭하세요. [자세히 알아보기](https://cloud.google.com/compute/docs/subnetworks)

        * **이름:**  
            서브넷 고유 이름을 부여합니다 

        * **Region:**  
            이 사용 설명서에서는 asia-northeast1(Tokyo)를 사용합니다.  
            [자세히 알아보기](https://cloud.google.com/compute/docs/regions-zones/regions-zones)  

        * **IP 주소 범위:**  
            CIDR 표기법으로 나타낸 이 서브넷의 주소 범위입니다. 표준 사설 VPC 네트워크 주소 범위(예: 10.0.0.0/9)를 사용하세요.  
            [자세히 알아보기](https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips)
        
        * **완료**를 선택합니다.

        * 위와 같은 방법으로 subnet을 추가 합니다.

    * **만들기**를 선택합니다.

    ![gke-create-vpc-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-vpc-1.png)
    ![gke-create-vpc-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-vpc-2.png)
    ![gke-create-vpc-3]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-vpc-3.png)

### 3. 클러스터 템플릿
GCP 콘솔을 사용해서 새 클러스터를 만들 경우 사용 가능한 모든 클러스터 템플릿이 표시됩니다. 기본적으로 표준 템플릿이 선택됩니다.

* 다음 템플릿을 사용할 수 있습니다.

    * **표준 클러스터:**  
    지속적 통합, 웹 제공, 백엔드용입니다. 추가 맞춤설정이 필요하거나 어떤 템플릿을 선택할지 확실하지 않은 경우에 선택하면 가장 적합합니다.

    * **첫 번째 클러스터:**  
    덜 강력한 노드를 실행하고 자동 확장과 같은 일부 고급 기능을 사용하지 않는 작은 클러스터입니다.

    * **CPU 집중 애플리케이션:**  
    해당 노드가 표준 클러스터보다 더 강력한 다중 코어 CPU를 제공하는 클러스터입니다.

    * **메모리 집중 애플리케이션:**  
    해당 노드가 일반적으로 강력한 다중 코어 CPU와 대용량 메모리를 제공하는 클러스터입니다.

    * **GPU 가속 컴퓨팅:**  
    기본 노드 풀이 덜 강력한 노드로 구성되었고 GPU 설정 노드 풀이 추가로 포함된 클러스터입니다. [자동 확장](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler?hl=ko)은 기본적으로 사용되지 않습니다.

    * **고가용성:**  
    클러스터가 지역 클러스터로 구성되었고, 특정 지역의 각 영역에 클러스터 마스터가 제공됩니다. 자동 확장 및 유지관리 기간이 사용 설정됩니다.

* **Kubernetes 클러스터 만들기**

    * **표준 클러스터**  
    템플릿을 선택하거나 해당 작업 부하에 적합한 템플릿을 선택합니다.

    * 필요한 경우 템플릿을 맞춤설정합니다. 다음 필드는 필수입니다.

        * **이름:**  
            클러스터 이름을 선택합니다. 프로젝트 및 영역 내에서 고유해야 합니다.

        * **위치 유형:**  
            클러스터의 모든 노드가 동일 영역에 있는지 아니면 지정된 지역의 모든 영역에 있을 수 있는지 여부입니다.

            * **영역:**  
                위치 유형이 영역인 경우 클러스터를 만들 Compute Engine [컴퓨팅 영역입](https://cloud.google.com/compute/docs/regions-zones/)니다.  
                이 사용 설명서에서는 **asia-northeast1-a**(Tokyo)를 사용합니다.


            * **지역:**  
                기본적으로 클러스터는 개발자가 생성 시 지정하는 단일 컴퓨팅 영역에 클러스터 마스터와 해당 노드를 만듭니다. 지역 클러스터를 만들어서 클러스터의 가용성 및 복구성을 향상시킬 수 있습니다.  
                [자세히 알아보기](https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters)
        
        * **마스터 버전:**  
        kubernetes version을 선택합니다.

        * **노드 풀:**  
            노드 풀은 클러스터에서 Kubernetes를 실행하는 별도의 인스턴스 그룹입니다.

            * **노드 수:**  
                클러스터에 만들 노드 수입니다. 노드 및 리소스에 대해 사용 가능한 [리소스 할당량](https://cloud.google.com/compute/quotas)이 있어야 합니다(예: 방화벽 경로).

            * **머신 유형:**  
                인스턴스에 사용할 Compute Engine [머신 유형](https://cloud.google.com/compute/docs/machine-types)입니다. 각 머신 유형은 서로 다르게 청구됩니다. 기본 머신 유형은 n1-standard-1입니다. 머신 유형 가격 정보는 [머신 유형 가격표](https://cloud.google.com/compute/pricing)를 참조하세요.

            * 고급 수정:

                * **이름:**  
                노드 풀 이름을 부여합니다.

                * **부팅 디스크 크기(GB):**  
                이 사용 설명서에서는 10GB를 사용합니다.

                * 보안

                    * **서비스 계정:**  
                    **Compute Engine default service account 를 선택 합니다.**  
                    VM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 콘솔 메뉴의 권한을 사용하여 서비스 계정을 만들거나 기본 서비스 계정이 있으면 이 계정을 사용하세요. [자세히 알아보기](https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform)

                * **저장**을 선택합니다.
            
        * **고급옵션:**
            * 네트워킹

                * **VPC 네이티브:**  
                    VPC 네이티브 사용 설정(별칭 IP 사용) 체크합니다.

                * **네트워크:**  
                    항목에서 생성한 VPC를 선택합니다.

                * **노드 서브넷:**  
                    항목에서 생성한 subnet을 선택합니다.

    * **만들기**를 선택합니다.

    ![gke-create-cluster-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-1.png)
    ![gke-create-cluster-2]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-2.png)
    ![gke-create-cluster-3]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-3.png)
    ![gke-create-cluster-4]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-4.png)

    GCP 콘솔에서 클러스터를 만든 다음에는 해당 클러스터와 상호작용하도록 kubectl을 구성해야 합니다. 자세한 내용은 [kubeconfig 항목 생성](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)을 참조하세요.

### 4. 클러스터에 연결

* **Cloud Shell에서 실행:**  

    * Kubernetes 클러스터 목록에서 생성된 클러스터 **연결**을 선택합니다.

    ![gke-create-cluster-connect-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-connect-1.png)

* **kubectl 을 사용하려면:**

    * kubectl 구성이 선행되야 합니다.  
    kubectl은 Kubernetes Engine에서 사용되는 클러스터 조정 시스템인 Kubernetes를 관리하기 위해 사용됩니다.

        * gcloud를 사용하여 kubectl을 설치할 수 있습니다.  

            ```
            gcloud components install kubectl
            ```

    * 클러스터 연결 화면에서 명령줄을 복사 콘솔에서 실행합니다.

        ![gke-create-cluster-connect-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-connect-1.png)

    * 아래 명령어로 확인할 수 있습니다.

        ```
        kubectl get svc
        ```
        ![gke-create-cluster-connect-2]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-cluster-connect-2.png)

### NFS Sever 만들기
Google Compute Engine 영구 디스크를 사용하여 네트워크 파일 시스템 (NFS) 서버를 만들어 컨테이너에 마운트하는 것입니다.

### 1. Create an instance

* **이름(Name):**  
인스턴스 이름을 부여합니다.

* **지역(Region):**  
지역은 리소스를 실행할 수 있는 특정 지리적 위치입니다.

* **영역(Zone):**  
영역은 지역 내의 격리된 위치입니다. 영역은 사용할 수 있는 컴퓨팅 리소스와 데이터를 저장하고 사용할 위치를 결정합니다.

* **머신 유형(Machine type):**  
맞춤설정을 클릭하여 코어, 메모리, CPU를 선택합니다.

* **부팅 디스크(Boot disk):**  
각 인스턴스에는 부팅을 위한 디스크가 필요합니다. 이미지나 스냅샷을 선택하여 새 부팅 디스크를 생성하거나 기존 디스크를 인스턴스에 연결하세요.  
이 사용 설명서에서는 CentOS 7 사용합니다.

* **ID 및 API 액세스(Identity and API access):**  
생성한 서비스 계정을 선택합니다.  
VM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 사용할 서비스 계정과 허용할 API 액세스 수준을 선택하세요. [자세히 알아보기](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances?hl)

    ![gke-create-instance-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-instance-1.png)

 **Management, security, disks, networking, sole tenancy 확장 섹션**을 활성화 해서 아래 설정을 합니다.
 
* **디스크(Disks) 설정**

    * Add new disk를 선택 합니다.

        * 노드에 필요한 디스크 사양을 설정 합니다.

        ![gke-create-instance-disk-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-instance-disk-1.png)

* **네트워크(Networking) 설정**  
네트워크는 인스턴스에서 액세스할 수 있는 네트워크 트래픽을 결정합니다.

    * **Network tags(네트워크 태그):**  
    네트워크 태그를 할당하여 특정 VM 인스턴스에 방화벽 규칙을 적용합니다.

    * **Network interfaces(네트워크 인터페이스)**

        * **Network(네트워크):**  
        목록에서 VPC network에서 생성한 VPC를 선택합니다.

        * **Subnetwork(하위 네트워크):**  
        목록에서 VPC network에서 생성한 subnet을 선택합니다.

        * **Primary internal IP(기본 내부 IP):**  
        임시의 경우 인스턴스를 다시 시작해도 내부 IP가 변경되지 않지만 인스턴스를 삭제하고 다시 만들면 내부 IP가 변경됩니다.  
        '임시(자동)'를 선택하여 하위 네트워크 범위의 주소를 할당하거나 '임시(커스텀)'를 선택하여 직접 입력하세요.  
        **인스턴스를 삭제하고 다시 만들 때 IP를 유지하려면 고정 내부 IP 주소를 선택하거나 만드세요.**  
        [자세히 알아보기](https://cloud.google.com/compute/docs/subnetworks)

            ![gke-create-instance-network-2]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-instance-network-2.png)

        * done(완료)**를 선택합니다.

* **Create(만들기)**를 선택합니다.

### 2. 생성된 인스턴스(노드) 방화벽 설정(SSH / NFS).

* 인스턴스 화면에서 생성한 인스턴스의 확장 메뉴에서 **View network details**를 선택.  
또는 VPC network > Firewall rules를 선택한다.

![gke-create-instance-view-network]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-instance-view-network.png)

* **SSH 방화벽 규칙을 추가 한다.**

    * **Name:**  
    예) cocktail-test-gke-demo-storage-ssh

    * **Network:**  
    인스턴스에서 설정한 VPC를 선택합니다.

    * **Targets(대상):**  
    **Specified target tags**를 선택.  
    가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.

        * **Target tags:**  
        NFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.

        * **Source filter:**  
        **IP ranges**를 선택.  
        필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.

        * **Source IP ranges:**  
        접근하고자 하는 IP ranges를 CIDR 표기법으로 추가 합니다.  
        예)10.0.10.0/24 192.168.100.5/32

        * **Protocols and ports:**  
        허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.

            * Specified protocols and ports

                * **tcp:** 22

    ![gke-create-firewall-rules-ssh]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-firewall-rules-ssh.png)

* **NFS 방화벽 규칙을 추가 한다.**

    * **Name:**  
    예) cocktail-test-gke-demo-storage-nfs

    * **Network:**  
    인스턴스에서 설정한 VPC를 선택합니다.

    * **Targets(대상):**  
    **Specified target tags**를 선택.  
    가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.

        * **Target tags:**  
        NFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.

        * **Source filter:**  
        **Subnets**를 선택.  
        필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.

        * **Subnets:**  
        이 소스 하위 네트워크의 트래픽만 허용됩니다.  

        * **Protocols and ports:**  
        허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.

            * Allow all 선택  
            이 사용 설명서에서는 편의상 Alloww all을 선택합니다.

    ![gke-create-firewall-rules-nfs]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-firewall-rules-nfs.png)

### 3. 생성된 인스턴스(노드)에 SSH 연결.

* 인스턴스 화면에서 생성된 인스턴스(노드)의 **SSH**항목을 선택 -> **View gcloud command**를 선택 한다(자동생성).

* **Run IN CLOUD SHELL**을 실행 한다.

    * RSA key pair 가 자동생성 된다.  
    아래명령으로 생성된 key pair 을 확인할 수 있다.

        ```
        cd ~/.ssh
        ```

    * RSA key pair GCP의 Compute Engine > Metadata > SSH Keys에 자동 등록 된다.  
    아래 그림처럼 등록된 SSH Key를 확인 및 편집할 수 있다.

![gke-create-connect-node-ssh-1]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-connect-node-ssh-1.png)

* 또는 RSA key pair 수동 생성 / 등록 방법.

    * 아래 명령으로 RSA key pair를 생성한다.

        ```
        ssh-keygen -t rsa -f ./rsa-gcp-key -C"<Your-email.com>"
        ```
    
    ![gke-create-connect-node-ssh-2]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-connect-node-ssh-2.png)

    * 생성된 RSA Key 내용을 복사 해서 Compute Engine > Metadata > SSH Keys 에 등록 한다.

        ```
        cat ./rsa-gcp-key.pub
        ```

        ![gke-create-connect-node-ssh-3]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-connect-node-ssh-3.png)
        ![gke-create-connect-node-ssh-4]({{< param imageRootDir >}}/assets/ZH/{{< param version >}}/GKE/gke-create-connect-node-ssh-4.png)


### 4. 생성된 인스턴스(노드)에 NFS 서버 설치 / 구성

* 노드 OS 확인.

    ```
    sudo grep . /etc/*-release
    ```

* 노드 NFS 패키지 확인.

    ```
    sudo rpm -qa | grep nfs
    ```

* 노드 NFS 패키지 설치.

    ```
    sudo yum install nfs-utils nfs-utils-lib
    ```

* 클러스터(마스터)에서 NFS에 사용될 공유 디렉토리를 생성 한다.  
**provisioner Deployment**에서 **NFS_PATH**에 지정된 디렉토리를 생성한다.

    ```
    sudo mkdir /storage/shared
    ```

* 공유 디렉토리 퍼미션 설정.

    ```
    sudo chmod -R 777 /storage
    ```

* 공유 디렉토리 NFS 권한을 부여하고, 동기화를 한다.  
인스턴스(노드)에서 설정한 **Subnet 또는 Ip ranges**를 부여한다.

    ```
    # sudo vi /etc/exports

    /storage/shared 10.0.10.0/24(rw,sync)
    ```

* NFS service를 재시작 한다.

    ```
    sudo service nfs restart
    ```

### 5. 클러스터(마스터)에 NFS-Client Provisioner 구성.  
**참조:** [Kubernetes NFS-Client Provisioner](https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client)

* **Setup authorization:**  
클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.

    * **kubectl apply -f 1-rbac.yaml**  
    Namespace, ServiceAccount, Setup authorization 적용

    ```yaml
    apiVersion: v1
    kind: Namespace
    metadata:
    name: cocktail-addon
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: nfs-client-provisioner
    namespace: cocktail-addon
    ---
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: nfs-client-provisioner-runner
    rules:
    - apiGroups: [""]
        resources: ["persistentvolumes"]
        verbs: ["get", "list", "watch", "create", "delete"]
    - apiGroups: [""]
        resources: ["persistentvolumeclaims"]
        verbs: ["get", "list", "watch", "update"]
    - apiGroups: ["storage.k8s.io"]
        resources: ["storageclasses"]
        verbs: ["get", "list", "watch"]
    - apiGroups: [""]
        resources: ["events"]
        verbs: ["list", "watch", "create", "update", "patch"]

    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: run-nfs-client-provisioner
    subjects:
    - kind: ServiceAccount
        name: nfs-client-provisioner
        # replace with namespace where provisioner is deployed
        namespace: cocktail-addon
    roleRef:
    kind: ClusterRole
    name: nfs-client-provisioner-runner
    apiGroup: rbac.authorization.k8s.io
    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: leader-locking-nfs-client-provisioner
    rules:
    - apiGroups: [""]
        resources: ["endpoints"]
        verbs: ["get", "list", "watch", "create", "update", "patch"]
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: leader-locking-nfs-client-provisioner
    subjects:
    - kind: ServiceAccount
        name: nfs-client-provisioner
        # replace with namespace where provisioner is deployed
        namespace: cocktail-addon
    roleRef:
    kind: Role
    name: leader-locking-nfs-client-provisioner
    apiGroup: rbac.authorization.k8s.io
    ```

* **Configure the NFS-Client provisioner**

    * **kubectl apply -f 2-shared-storage-provisioner-dp.yaml**  
    Deployment, 다음 프로비저 배포 파일을 편집하여 NFS 서버에 대한 연결 정보를 추가해야합니다.

    ```yaml
    kind: Deployment
    apiVersion: apps/v1
    metadata:
    name: shared-storage-provisioner
    namespace: cocktail-addon
    spec:
    replicas: 1
    selector:
        matchLabels:
        acornsoft.io/provisioner-type: NFSDYNAMIC
        app: shared-storage-provisioner
    strategy:
        type: Recreate
    template:
        metadata:
        labels:
            app: shared-storage-provisioner
            acornsoft.io/provisioner-type: NFSDYNAMIC
        spec:
        serviceAccount: nfs-client-provisioner
        containers:
            - name: shared-storage-provisioner
            image: quay.io/external_storage/nfs-client-provisioner:v2.0.1
            volumeMounts:
                - name: nfs-client-root
                mountPath: /persistentvolumes
            env:
                - name: PROVISIONER_NAME
                # YOUR PROVISIONER_NAME
                value: acornsoft.io/shared-storage-provisioner
                - name: NFS_SERVER
                # YOUR NFS SERVER HOSTNAME
                value: 10.0.10.8
                - name: NFS_PATH
                value: /storage/shared
            resources:
                limits:
                cpu: 100m
                memory: 128Mi
                requests:
                cpu: 100m
                memory: 128Mi
        tolerations:
        - key: node-role.kubernetes.io/master
            effect: NoSchedule
        volumes:
            - name: nfs-client-root
            nfs:
                # YOUR NFS SERVER HOSTNAME
                server: 10.0.10.8
                path: /storage/shared

    ```

* **Storage class 등록**  
참조: <https://kubernetes.io/docs/concepts/storage/storage-classes/>

    * **kubectl apply -f 3-single-sc.yaml**  
    single-storage : Persistent Volumes with Kubernetes on GKE

    ```yaml
    ---
    # single-storage : Persistent Volumes with Kubernetes on GKE
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
    name: single-storage
    annotations:
        storageclass.beta.kubernetes.io/is-default-class: "false"
    labels:
        acornsoft.io/provisioner-type: GCE
        acornsoft.io/type: SINGLE
        addonmanager.kubernetes.io/mode: EnsureExists
        kubernetes.io/cluster-service: "true"
    parameters:
    type: pd-standard
    provisioner: kubernetes.io/gce-pd
    reclaimPolicy: Delete
    ```

    * **kubectl apply -f 4-shared-sc.yaml**  
    shared-storage : NFS Persistent Volumes with Kubernetes on GKE

    ```yaml
    ---
    # shared-storage : NFS Persistent Volumes with Kubernetes on GKE
    apiVersion: storage.k8s.io/v1beta1
    kind: StorageClass
    metadata:
    name: shared-storage
    annotations:
        storageclass.beta.kubernetes.io/is-default-class: "false"
    labels:
        acornsoft.io/provisioner-type: NFSDYNAMIC
        acornsoft.io/total-capacity: "100"
        acornsoft.io/type: SHARED
    provisioner: acornsoft.io/shared-storage-provisioner
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    ```

### 6. 샘플 PVC 등록 / 확인.

* 아래 명령으로 샘플을 구성할 수 있다.

    * **kubectl apply -f 5-test-pod.yaml**  
    임시 파일 생성 pod를 생성 한다.

    ```yaml
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
    name: nfs-pvc
    spec:
    storageClassName: shared-storage
    accessModes:
    - ReadWriteMany
    resources:
        requests:
        storage: 1Mi
    ---
    kind: Pod
    apiVersion: v1
    metadata:
    name: test-pod
    spec:
    containers:
    - name: test-pod
        image: gcr.io/google_containers/busybox:1.24
        command:
        - "/bin/sh"
        args:
        - "-c"
        - "touch /mnt/SUCCESS && exit 0 || exit 1"
        volumeMounts:
        - name: nfs-test-pvc
            mountPath: "/mnt"
    restartPolicy: "Never"
    volumes:
        - name: nfs-test-pvc
        persistentVolumeClaim:
            claimName: nfs-pvc
    ```

    * **kubectl get pv**  
    생성된 PV를 확인 한다.

    * **NFS 노드**에 생성된 임시 파일을 확인할 수 있다.
