[
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/",
	"title": "2.1 시스템",
	"tags": [],
	"description": "List the child pages of a page",
	"content": "시스템 이용에 필요한 시스템 정보, 계정 정보, 클라우드 미터링 계정, 시스템 사용자를 관리 할 수 있다.\na) 좌측 메인메뉴 → 시스템\n   시스템 정보 설명     시스템 명 칵테일 클라우드의 시스템 이름   시스템 접속 계정 칵테일 클라우드 접속을 위한 시스템 계정   시스템 유형 칵테일 클라우드 시스템 사용 유형   기본 언어 칵테일 클라우드 기본 언어   시스템 로고 칵테일 클라우드 시스템 로고   시스템 설명 칵테일 클라우드 시스템 설명       계정 정보 설명     조직명 칵테일 클라우드 시스템 이용 조직명   계정 담당자 명 칵테일 클라우드 계정 관리 담당자 명   계정 담당자 이메일 칵테일 클라우드 계정 관리 담당자 이메일   주소 칵테일 클라우드 시스템 이용 조직 주소   라이센스 코드 칵테일 클라우드 라이센스 코드       클라우드 미터링 계정 설명     계정 이름 클라우드 미터링 계정 이름   프로바이더 클라우드 프로바이더 유형   설명 클라우드 미터링 계정 설명       시스템 사용자 설명     이름 시스템 사용자 이름   아이디 칵테일 클라우드 접속 사용자 아이디   권한 시스템 사용자 권한 (관리자, 사용자)   마지막 접속일 시스템 사용자 마지막 접속 시간   설명 시스템 사용자 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/",
	"title": "8.1 클러스터 구성 및 등록",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.1.kass/",
	"title": "8.1.1 클러스터 서비스 사용",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/",
	"title": "Understand Cocktail Cloud",
	"tags": [],
	"description": "",
	"content": " What is Cocktail? Cocktail is an all-in-one container management platform.\nAs the use of cloud computing became widespread, there has been a growing demand for infrastructure and application service management. This can be attributed to the limitations in utilizing the advantages of the cloud with past development and operations methods.\nIn the applications segment in particular, there is growing demand for automation, efficiency, and integrated management, including continuous integration and delivery/deployment (CI/CD), migration, and multi/hybrid cloud computing.\nIn this context, the proliferation of container technology can be considered a natural evolution. Many companies have adopted container technology, and the trend is only continuing. (Reference: http://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100)\nAs a technology that packages applications or services into independent, executable units, containers provide the same development and operational experience regardless of infrastructure. Therefore, cloud management can be standardized, and development and operational efforts can be reduced from infrastructure to services. Notably, containers provide the advantage of managing a multi/hybrid cloud under a cohesive environment.\nCocktail applies the advantages of containers to cloud management to streamline development and operations, providing a platform for implementing a single or multi/hybrid cloud strategy.\nThe main features of Cocktail are as follows:\n Pipeline automation from building to deployment and updating from code. Workload-oriented (service) container management: Packaging, lifecycles, resources, etc. Full-stack monitoring: Status and resource monitoring from infrastructure to containers; Alerts management. Multi/hybrid cluster provisioning and management: Bare-metal, private/public cloud.  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.2.onpremise/",
	"title": "8.1.2 클러스터 직접 구성",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/",
	"title": "시스템 관리",
	"tags": [],
	"description": "",
	"content": " 빌드 및 배포, 운영할 수 있는 워크스페이스를 할당한다. 워크스페이스 별로 사용자, 클러스터, 라이브러리 등록이 가능하다.\na) 좌측 메인메뉴 → 워크스페이스    서비스 설정 설명     워크스페이스 명 워크스페이스 이름 (사용자지정)   클러스터 수 워크스페이스에 할당된 클러스터 수   구성원 수 워크스페이스를 사용하는 구성원 수   시스템 명 시스템 명   설명 워크스페이스 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/3.dashboard/",
	"title": "대시보드",
	"tags": [],
	"description": "",
	"content": "대시보드에서는 Cocktail Cloud에 서비스된 정보를 한 눈에 확인 할 수 있다.\n 요약정보  현재 등록 관리되고 있는 서비스 개수, 워크로드 개수, 등록된 클러스터 수, 현재 발생한 알람의 개수를 볼 수 있다.\n 자원현황  CPU, Memory의 전체량, 요청량, 사용량과 Storage(Node, Pv) 전체, 사용량을 확인 할 수 있다.\n 클러스터 현황  등록된 클러스터 별로 서비스 개수, 워크로드 개수를 확인 할 수 있다.\n 클러스터 비용 현황  사용 중인 클러스터의 비용을 확인 할 수 있다.\n 클러스터 자원 할당률  사용 중인 클러스터의 클러스터 자원 할당률(request)을 확인 할 수 있다.\n 서비스 자원 할당량  사용 중인 서비스 별로 자원(CPU/Memory) 할당량을 확인 할 수 있다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/",
	"title": "서비스",
	"tags": [],
	"description": "",
	"content": "서비스 맵과 워크로드를 생성, 관리 하는 기능이다.    목록 설명      서비스 맵 그룹 액션메뉴    서비스 맵 생성 팝업 호출    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/",
	"title": "빌드 관리",
	"tags": [],
	"description": "",
	"content": " 도커(Docker) 이미지에 대한 빌드 작업을 관리한다.\n어플리케이션을 다운로드, 소스 파일을 빌드, 이미지 파일을 생성하여 Harbor 레지스트리에 업로드한다.\na) 좌측 메인 메뉴 → 빌드 를 클릭하여 해당 페이지로 이동한다.    목록 설명     + 빌드 생성 버튼   Filter 검색을 통해 필요한 빌드 조회   빌드명 빌드의 이름   이미지명 저장된 도커 이미지 이름   이미지 사이즈 저장된 도커 이미지의 크기. MB단위로 표기   최근 액션 빌드의 상태를 표시   상태 빌드의 실행 상태를 표시   날짜 빌드가 실행된 날짜를 표시    빌드 액션 메뉴(작업 설정 편집, 히스토리, 빌드 실행, 빌드 취소, 로그 보기, 작업 삭제)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/",
	"title": "카탈로그",
	"tags": [],
	"description": "",
	"content": "서비스 구성을 템플릿화하여 쉽게 워크로드 생성 및 배포, 관리가 가능한 기능이다.\n여러 워크로드들을 한 번에 묶어 사용하고자 하는 경우 카탈로그 기능을 사용한다.\n카탈로그는 서비스 카탈로그와 플랫폼 카탈로그로 구분된다.\n   카탈로그 메뉴 설명     서비스 카탈로그 사용자가 생성한 카탈로그   플렛폼 카탈로그 Cocktail Cloud 기본 제공 공식 카탈로그   Filter Catalog 검색을 통해 필요한 템플릿 조회   템플릿 서비스를 구성하고 있는 워크로드 모음   배포 카탈로그를 통해 워크로드를 배포   편집 카탈로그 내용을 편집    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/",
	"title": "클러스터",
	"tags": [],
	"description": "",
	"content": "클러스터의 등록관리와 노드와 서비스 단위의 모니터링, 볼륨, 알람, 미터링 서비스가 제공된다.   클러스터 메뉴 설명     노드 전체 노드의 CPU, Memory, Network 사용량 및 개별 노드의 모니터링 서비스   서비스 전체 서비스의 CPU, Memory, Network 사용량 및 개별 서비스의 모니터링 서비스   볼륨 스토리지 및 볼륨 현황   알람 클러스터의 이벤트 정보 관련 알람 서비스   미터링 클러스터 비용과 리소스별 비용 서비스   등록관리 클러스터 정보 조회    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/",
	"title": "시스템 운영",
	"tags": [],
	"description": "",
	"content": "\u0026mdash; |\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/9.member/",
	"title": "구성원",
	"tags": [],
	"description": "",
	"content": "현재 워크스페이스에 속한 구성원들 정보를 보여준다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/10.terminology/",
	"title": "용어설명",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud 활용의 이해를 돕기 위한 Kubernetes 및 청지원 용어 정리이다. |\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/11.support/",
	"title": "고객지원 서비스",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud을 이용해 주셔서 감사합니다.\n제품 구매 기능상담 등의 기술 지원 서비스 및 각종 혜택을 받을 수 있습니다. 제공하는 각종 서비스는 다음과 같습니다.\n 방문 서비스를 통하여 Q\u0026amp;A, 제품에 관한 정보 및 기타 문의를 할 수 있습니다. 고객 등록을 마친 제품과 상담이 가능한 제품에 한하여 전화나 팩스를 통하여 전문 기술상담 서비스를 받을 수 있습니다. 각종 행사 초청 및 정보를 제공합니다.  서비스 지원\n이메일 : byoungoh.lee@namutech.co.kr\n제품 개발사\n아콘소프트(주)\n 주소 : 서울특별시 강남구 역삼로 239 화광센터 4층 TEL : 02-554-0301 FAX : 02-554-0302  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/10.terminology/10.1-kubenetes/",
	"title": "10.1 Kubernetes(k8s)",
	"tags": [],
	"description": "",
	"content": " 1.Cluster    용어 설명     Namespace k8s의 가상 클러스터로 사용자들이 여러팀 또는 프로젝트로 분산되어 작업 할 수 있는 별도의 환경   Nodes k8s의 클러스터링되어 있는 어플리케이션을 구동하는 물리서버 또는 VM   Persistent Volumes(PV) 외부 저장소. NFS, iSCSI, 클라우드에서 제공하는 스토리지 시스템을 지원   Roles 사용의 따라 권한을 부여.   Storage Classes PV를 동적으로 Provisioning 할 경우, NFS 서버를 식별하기 위한 식별자    2.Workloads    용어 설명     Cron Jobs \u0026lsquo;특정 시점에서 한 번\u0026rsquo; 또는 \u0026lsquo;특정 시점에서 반복\u0026rsquo;과 같이 시간 기반의 관리 작업   Deployments Pod 생성을 위한 설정값의 등록 정보   Jobs Job은 일괄처리를 수행하는 Pod의 Supervisor. 즉, 특정한 계산 또는 백업과 같이 특정 시간 동안만 실행되는 프로세스.   Pods 1개 이상의 컨테이너로 구성되는 k8s에서 deploy를 위한 최소단위. Pod는 어플리케이션 스택에서 서로 다른 Docker image를 혼합하여 구성   Replica Sets 지정된 Pod 복제본이 항상 실행(유지)되도록 한다.   Replication Controllers 지정된 수의 Pod 복제본이 실행되고 있는지 체크   Stateful Sets Pod의 배포 및 확장을 관리하고, Pod의 순서와 특성에 대한 설정을 제공    3.Discovery and load balancing    용어 설명     Ingresses 외부접근에 대한 요청을 name-based로 가상 Hosting을 제공   Services 여러 컨테이너를 하나의 논리적인 단위로 그룹화하여, 외부에서 접속할 수 있는 단일 EndPoint를 제공    4.config and storage    용어 설명     Config Maps 데이터를 저장하여 사용할 수 있게 지원   Persistent Volume Claime(PVC) 사용자가 PV에 대한 요청. Pod는 PVC를 통하여 리소스(CPU 및 메모리)를 요청할 수 있고, 저장공간의 사이즈 및 접근 권한(rw, readOnly \u0026hellip;)을 요청할 수 있다.   Secrets Password 또는 OAuth 토큰 및 ssh key 등의 저장소.    5.settings K8s의Global Settings값을 조정 할 수 있습니다.\n   옵션 설명     Cluster name Kubernetes Cluster의 이름값을 조정할 수 있다.   Items per page 1페이지에 나타낼 수 있는 Items의 값을 조정할 수 있다.   Auto-refresh time interval Log를 자동으로 새로고침하는 시간을 조정할 수 있다.    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/10.terminology/10.2-system/",
	"title": "10.2 Cocktail Cloud",
	"tags": [],
	"description": "",
	"content": " 1.환경설정 Cocktail Cloud을 이용하기 위해서 사전에 정보를 등록하는 서비스\n   용어 설명     사용자 Cocktail Cloud을 이용하기 위한 사용자 관리를 지원   계정 k8s Cluster에 대한 사용자 인증 정보에 대한 관리를 지원   서비스 업무 또는 프로젝트 단위로 작업을 분산하여 관리. 관리자는 일반사용자에 대하여 서비스 단위로 접근권한을 부여할 수 있다.    C.클러스터 : k8s를 사용하기 위한 k8s정보를 관리하는 페이지\n2.서비스 서비스 단위로 작업을 세분화가 가능하며, 빌드를 통한 Docker 이미지를 생성 할 수 있다. 빌드된 이미지를 통하여 생성, 조회, 수정, 삭제등 관리가 이루어지는 서비스.\n   용어 설명     서비스 맵 k8s의 Namespace   빌드 Docker image를 만들기 위한 작업을 지원하는 서비스   모니터링 서비스 안에서 워크로드를 Pod, Container 단위의 자원 모니터링을 제공   파이프라인 빌드부터 배포까지 한 번에 편리한 실행이 가능한 서비스   설정 서비스 단위로 변수값 또는 파일들을 저장하여 사용할 수 있는 서비스    3.카탈로그 워크로드 구성을 템플릿화 하여 쉽게 워크로드생성 및 배포, 편집 관리가 가능한 서비스.\n4.클러스터 클러스터, 노드, 워크로드, 볼륨/스토리지, 알람, 미터링 등을 등록관리 및 현황 조회가 가능한 서비스\n   용어 설명     클러스터 k8s 오케스트레이션을 구성하는 서버의 패키지   노드 k8s 클러스터의 서버의 개별 서버. Master와 Worker로 구분   워크로드 1개 이상의 컨테이너로 구성되는 서비스   볼륨/스토리지 외부 스토리지 정보   알람 클러스터에 대한 이벤트 알람   미터링 클러스터에 대한 요금정보    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/10.terminology/10.3-error-code/",
	"title": "10.3 Cocktail Cloud System Error Code",
	"tags": [],
	"description": "",
	"content": " 1. 에러 코드    코드 메시지     CCCM001 구체적 오류 번호를 밝힐 수 없음.   CCCM002 알수 없는 오류 입니다   CCCM004 잘못된 함수 호출 인자입니다   CCCM005 인증되지 않은 요청입니다   CCCM006 전달된 정보가 올바르지 않습니다   CCCM007 서버, 작업 등이 올바르지 않거나 요청을 처리할 수 없습니다   CCCM019 내부 서버 오류   CCCM020 외부 호출 API가 오류를 반환 또는 호출이 실패 했습니다   CCCM023 수행 할 작업이 없습니다   CCCM026 오류가 발생하였습니다.   CCCM027 등록 중 오류가 발생하였습니다.   CCCM028 수정 중 오류가 발생하였습니다.   CCCM029 삭제 중 오류가 발생하였습니다.   CCCM030 조회 중 오류가 발생하였습니다.   CDCM031 데이터 베이스 작업 중 오류가 발생했습니다. 잠시 후 다시 시도 해 주세요. 문제가 지속될 경우 관리자에게 문의 해 주세요.   CDCM032 데이터 베이스 연결 중 오류가 발생했습니다. 잠시 후 다시 시도 해 주세요. 가능한 경우 데이터 베이스 연결 상태를 점검하거나 관리자에게 문의 해 주세요.   CKSY001 Cube Cluster API 오류가 발생하였습니다.   CKSY002 Cube Cluster의 버전이 Cocktail Cloud에서 지원하지 않습니다.   CKSY003 암호화 중 오류가 발생하였습니다.   CCUS001 사용자 ID가 등록되어 있지 않습니다   CCUS002 사용자 암호가 맞지 않습니다   CCUS003 지정한 사용자 권한이 사용자에게 할당되어 있지 않습니다   CCUS004 사용자가 이미 등록되어 있습니다   CCUS005 사용 중지된 계정입니다   CCUS006 Root Admin 사용자는 조작할 수 없습니다   CCAC001 프로바이더 credential의 형식이 올바르지 않습니다   CCAC002 프로바이더 credential이 없습니다   CCAC003 프로바이더 access key 또는 secret key가 올바르지 않습니다   CCAC004 새로 입력하려는 프로바이더 키 값이 이전과 같지 않습니다   CCAC005 프로바이더를 찾을 수 없습니다   CCAC006 해당 프로바이더를 사용하는 클러스터가 존재합니다   CCCT001 클러스터 접속 계정이 올바르지 않습니다   CCCT002 해당 마스터 URL로 등록된 클러스터가 이미 존재합니다   CCCT003 클러스터 인증서가 올바르지 않습니다   CCCT005 클러스터가 서버를 포함하고 있어 클러스터를 수정 또는 삭제할 수 없습니다   CCCT006 클러스터 현황 조회 중 오류가 발생하였습니다.   CCCT007 어플리케이션 현황 조회 중 오류가 발생하였습니다.   CCCT008 볼륨 현황 조회 중 오류가 발생하였습니다.   CCGP001 그룹이 서버를 포함하고 있어 클러스터를 삭제할 수 없습니다   CCGP002 그룹명이 중복되었습니다   CCSV001 서비스에 클러스터가 등록되어 있습니다   CCSV002 어플리케이션 맵이 클러스터를 사용 중 입니다   CCBD001 빌드 Job 생성 오류   CCBD002 빌드 Job 삭제 오류   CCBD003 이미 레지스트리에 등록된 이미지명입니다 다른 이미지명을 입력하시기 바랍니다   CCBD004 이미 등록된 이미지입니다   CCBD005 빌드 서버 접속이 원활하지 않습니다   CCBD006 빌드 조회 중 오류가 발생하였습니다.   CCBD007 빌드 생성 중 오류가 발생하였습니다.   CCBD008 빌드 수정 중 오류가 발생하였습니다.   CCBD009 빌드 삭제 중 오류가 발생하였습니다.   CCBD010 빌드 히스토리 조회 중 오류가 발생하였습니다.   CCCV001 클러스터 볼륨의 이름이 없거나 올바르지 않습니다   CCCV002 클러스터 볼륨의 스토리지 클래스 이름이 없거나 올바르지 않습니다   CCCV003 클러스터 볼륨의 플러그인 이름이 없습니다   CCCV004 클러스터 볼륨의 리클레임 정책 이름이 없습니다   CCCV005 클러스터 볼륨의 클러스터 일련번호가 없습니다   CCCV006 클러스터 볼륨을 찾을 수 없습니다   CCCV007 동일한 설정의 볼륨이 이미 존재합니다   CCCV008 클러스터 볼륨 유형을 찾을 수 없습니다   CCCV009 클러스터 볼륨이 사용중이어서 삭제 할 수 없습니다   CCCV010 파라미터는 필수 입니다   CCCV011 동일한 설정의 Static 볼륨이 이미 존재합니다   CCCV012 해당 클러스터 볼륨은 고정 클러스터 볼륨이 아닙니다   CKCF001 K8S ConfigMap 생성 실패   CKCF002 지정한 K8S ConfigMap을 찾을 수 없습니다   CCCF003 컨피그맵의 이름이 없거나 올바르지 않습니다   CCCF004 컨피그맵의 키가 올바르지 않습니다   CCCF005 추가하려는 컨피그맵의 이름이 이미 존재합니다   CCCF006 컨피그맵 데이터가 올바르지 않습니다   CKCF007 컨피그맵을 사용하는 서버가 존재합니다   CKST001 K8S Secret 생성 실패   CCST002 지정한 K8S Secret을 찾을 수 없습니다   CCST003 Secret의 이름이 없거나 올바르지 않습니다   CCST004 시크릿 데이터가 올바르지 않습니다   CCST005 추가하려는 시크릿의 이름이 존재합니다   CCST006 시크릿을 사용하는 서버가 존재합니다   CCCL001 작업 대상 카탈로그의 일련번호가 없습니다   CCCL002 카탈로그를 적용할 기존 어플리케이션 맵의 일련 번호가 없습니다   CCCL003 동일한 카탈로그의 이름이 이미 존재합니다   CCCL004 동일한 카탈로그의 버전이 이미 존재합니다   CCCL005 카탈로그 등록 중 오류가 발생했습니다   CCCL006 카탈로그 삭제 중 오류가 발생했습니다   CCCL007 카탈로그 수정 중 오류가 발생했습니다   CCCL008 카탈로그 배포 중 오류가 발생했습니다   CCCL010 카탈로그가 사용하는 레지스트리를 찾을 수 없습니다   CCCL011 카탈로그가 사용하는 클러스터 볼륨을 찾을 수 없습니다   CCCL012 등록할 카탈로그의 이름이 없거나 올바르지 않습니다   CCCL013 카탈로그를 적용할 새 어플리케이션 맵의 이름이 없거나 올바르지 않습니다   CCCL015 새 네임스페이스의 이름이 없거나 올바르지 않습니다   CCSR001 실행하려는 Job 타입이 올바르지 않습니다   CCSR002 task가 실행 중이라 새로운 task를 실행할 수 없습니다   CCSR003 서버가 지정한 동작을 실행할 수 없는 상태입니다   CCSR004 현재 상태에서 실행할 수 없는 액션입니다   CCSR006 실행할 업무(task)가 존재하지 않습니다   CCSR007 사용 가능한 메모리 양을 초과하였습니다   CCSR008 사용 가능한 CPU 양을 초과하였습니다   CCSR009 사용 가능한 Pod 수를 초과하였습니다   CCSR010 서버 생성 준비 중 오류가 발생했습니다   CCSR011 서버를 수정할 수 없는 상태입니다   CCSR012 서버 수정 준비 중 오류가 발생했습니다   CCSR013 서버 종료 준비 중 오류가 발생했습니다   CCSR014 서버 재시작 준비 중 오류가 발생했습니다   CCSR015 이미 사용 중인 이름입니다   CCSR016 조회하려는 리소스의 갯수가 없습니다   CCSR017 알 수 없는 리소스 타입입니다   CCSR018 지원하지 않는 서버 형식입니다   CCSR019 지원하지 않는 볼륨 플러그인 입니다   CKSR020 K8S Deployment 생성을 기다리던 중 timeout이 발생했습니다   CKSR021 K8S Pod 생성을 기다리던 중 timeout이 발생했습니다   CKSR022 K8S Deployment 삭제 실패했습니다   CKSR023 K8S Replicatset 삭제 실패했습니다   CKSR024 K8S Service 또는 Load Balancer 생성을 기다리던 중 timeout이 발생했습니다   CCSR025 K8S Pod를 찾을 수 없습니다   CKSR026 K8S Object 상태 조회 결과에 status가 없습니다   CKSR027 K8S PersistentVolume 생성 실패 했습니다   CKSR028 K8S PersistentVolume을 찾을 수 없습니다   CKSR029 K8S PersistentVolumeClaim 생성 실패 했습니다   CKSR030 K8S PersistentVolumeClaim 생성 중 timeout이 발생했습니다   CKSR031 지정한 K8S Namespace를 찾을 수 없습니다   CKSR032 K8S Deployment 생성 실패 했습니다   CKSR033 K8S Service 생성 실패 했습니다   CKSR034 K8S Ingress 생성 실패 했습니다   CKSR035 K8S HorizontalPodAutoscaler 생성 실패 했습니다   CCSR036 같은 이름을 가진 볼륨이 이미 존재합니다   CCSR037 지정한 서버를 찾을 수 없습니다   CCSR038 서버가 Cube 타입이 아닙니다   CKSR039 서버 설정 수정에 실패 했습니다   CCSR040 서버 생성/수정 전 k8s 자원 검사에 실패 했습니다   CKSR042 K8S Deployment 정보를 찾을 수 없습니다   CCSR043 서버타입 변경 불가   CCSR044 서버 컨테이너가 없습니다   CCSR045 호스트 포트 중복입니다   CCSR046 인그레스 경로가 이미 사용중입니다   CCSR047 K8S의 로그 타입이 아닙니다   CCSR048 준비상태에서 서버 삭제 실패   CCSR050 노드 포트가 지정 범위를 벗어났습니다   CCSR051 노드포트 중복   CCSR053 볼륨을 사용 할 수 없는 상태여서 재시작을 실행할수 없습니다   CCSR054 컨테이너명이 중복되었습니다   CCSR055 네임스페이스명이 중복되었습니다   CCSR056 서버 중지 가능한 상태가 아닙니다.   CCSR057 서버 시작 가능한 상태가 아닙니다.   CCSR058 서버 재시작 가능한 상태가 아닙니다.   CCSR059 서버 삭제 가능한 상태가 아닙니다.   CKSR062 Cube Cluster 정보 조회 중 오류가 발생하였습니다.   CKSR063 Cube Cluster 정보 생성 중 오류가 발생하였습니다.   CKSR064 Cube Cluster 정보 수정 중 오류가 발생하였습니다.   CKSR065 Cube Cluster 정보 삭제 중 오류가 발생하였습니다.   CCSR066 서버 포트가 올바르지 않습니다. ( 범위 : 1 ~ 65535 ).   CCSR067 서버 포트 범위가 올바르지 않습니다.   CCSR068 서버 포트 범위 형식이 올바르지 않습니다. ( e.g. 8080-8082 )   CCSR069 서버 타겟 포트와 포트의 범위가 동일하지 않습니다. ( e.g. 8080-8082, 9090-9092 )   CKSR070 Cube Cluster의 클러스터 리소스 제한량 조회 중 오류가 발생하였습니다.   CCSR071 서비스가 존재하지 않습니다.   CCSR072 서버 실행 중 오류가 발생하였습니다.   CCSR073 서버 볼륨 설정이 올바르지 않습니다. 다시 확인 해주세요.   CCSR074 Port Range는 지정 노드 포트를 설정하실 수 없습니다.   CCSR075 지정 노드포트 값이 없습니다.   CCSR076 서버 제거 준비 중 오류 발생.   CCPL001 서버가 실행중 상태가 아니어서 파이프라인을 실행할 수 없습니다   CCPL002 파이프라인이 실행중 상태여서 다른 동작을 수행할 수 없습니다   CCPL003 지정한 파이프라인을 찾을 수 없습니다   CCPL004 파이프라인 생성 실패   CCPL005 파이프라인 수정 실패   CCPL006 파이프라인 실행 중 동일한 빌드 작업이 존재하여 실행할 수 없습니다. 동일한 빌드를 포함하는 파이프라인은 각각 실행하여 주세요.   CCPL007 파이프라인 실행 중 오류가 발생하였습니다.   CCRG001 레지스트리에서 이미지 목록을 받을 수 없습니다   CCRG002 레지스트리에 로그인 할 수 없습니다   CCRG003 레지스트리에서 이미지 태그 목록을 받을 수 없습니다   CCRG004 레지스트리에 사용자를 등록 할 수 없습니다   CCRG005 레지스트리에 로그인하려는 사용자가 등록되어 있지 않습니다   CCRG006 레지스트리 사용자의 암호를 수정 할 수 없습니다   CCRG008 레지스트리에 등록된 사용자를 프로젝트에 할당 할 수 없습니다   CCRG009 레지스트리에 등록된 사용자를 프로젝트에서 제외 할 수 없습니다   CCRG010 레지스트리에 프로젝트를 등록할 수 없습니다   CCRG011 레지스트리에 프로젝트가 이미 존재 합니다   CCRG012 레지스트리에 프로젝트를 삭제할 수 없습니다    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.1-edit/",
	"title": "2.1.1 시스템 편집",
	"tags": [],
	"description": "",
	"content": " 시스템 이용에 필요한 시스템 정보, 계정 정보를 수정 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 우측 상단 편집 버튼 시스템 정보(기본 언어, 시스템 로고, 시스템 설명)와 계정 정보(조직명, 계정 담당자 명, 계정 담당자 이메일, 주소)를 수정 할 수 있다.\nb) 좌측 메인메뉴 → 시스템 → 편집 → 수정완료 후 수정버튼을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.2-meteringadd/",
	"title": "2.1.2 클라우드 미터링 계정 등록",
	"tags": [],
	"description": "",
	"content": "클라우드 미터링 계정 등록 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 계정 등록  Amazon Web Service  Google Cloud Plalform   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.3-meteringedit/",
	"title": "2.1.3 클라우드 미터링 계정 편집",
	"tags": [],
	"description": "",
	"content": "클라우드 미터링 계정 편집 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 계정 액션 메뉴 → 계정 편집 → 수정완료 후 수정버튼을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.4-meteringdelete/",
	"title": "2.1.4 클라우드 미터링 계정 삭제",
	"tags": [],
	"description": "",
	"content": "클라우드 미터링 계정을 삭제 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 계정 액션 메뉴 → 계정 삭제 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.5-useradd/",
	"title": "2.1.5 시스템 사용자 추가",
	"tags": [],
	"description": "",
	"content": "시스템 사용자를 추가 하는 기능이다.\na) 좌측 메인메뉴 → 시스템 → 사용자 추가를 클릭한다.    시스템 사용자 설명     이름 시스템 사용자 이름   아이디 칵테일 클라우드 접속 사용자 아이디   설명 시스템 사용자 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.6-useredit/",
	"title": "2.1.6 시스템 사용자 편집",
	"tags": [],
	"description": "",
	"content": "시스템 사용자 정보(이름, 설명)를 편집 하는 기능이다.\na) 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 편집를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.7-userreset/",
	"title": "2.1.7 시스템 사용자 비밀번호 초기화",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud 사용자 비밀번호를 초기화 하는 기능이다. (초기화 후 제공받는 초기 비밀번호는 \u0026lsquo;Pass0000\u0026rsquo;이다.)\na) 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 비밀번호 초기화를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.1.system/2.1.8-userdelete/",
	"title": "2.1.8 시스템 사용자 삭제",
	"tags": [],
	"description": "",
	"content": "시스템 사용자를 삭제하는 기능이다.\na) 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/",
	"title": "2.2 클러스터",
	"tags": [],
	"description": "",
	"content": "클러스터의 등록관리와 노드와 어플리케이션 단위의 모니터링, 볼륨, 알람, 미터링 서비스가 제공된다.    클러스터 메뉴 설명     노드 전체 노드의 CPU, Memory, Network 사용량 및 개별 노드의 모니터링 서비스   어플리케이션 전체 어플리케이션의 CPU, Memory, Network 사용량 및 개별 어플리케이션의 모니터링 서비스   볼륨 스토리지 관리 및 볼륨 현황   알람 클러스터의 이벤트 정보 관련 알람 서비스   미터링 클러스터 비용과 리소스별 비용 서비스   등록관리 클러스터 조회 및 수정 및 삭제    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.1-add/",
	"title": "2.2.1 클러스터 등록",
	"tags": [],
	"description": "",
	"content": "클러스터를 추가 등록할 수 있는 기능이다.\na) 클러스터 → 오른쪽 상단 + 클러스터 등록 버튼을 클릭한다. b) 기본 정보(이름, 쿠버네티스 버전, 아이디, 설명)를 입력한다.\n   기본 정보 설명     이름 등록할 클러스터의 이름   쿠버네티스 버전 클러스터에 설치된 Kubernetes의 버전정보. e.g) 1.8.13   아이디 클러스터 고유 아이디(사용자 지정), 알람 메시지 Redirect시 필요   설명 클러스터에 대한 사용자 설명    c) 프로바이더 정보(프로바이더, 유형, 리전)를 입력한다. 계정의 프로바이더와 유형에 따라 입력란이 변경된다.\n   프로바이더 설명     프로바이더 프로바이더를 선택   유형 Kubernetes의 사용 유형으로 MANAGED, PROVIER, GKE, EKS, AKS 선택   리전 Kubernetes가 설치된 서버의 리전 선택       추가입력 설명     Billing Account 클라우드 미터링 계정 선택   프로젝트 아이디 Google Cloud Platform의 계정이 사용할 프로젝트의 아이디(GKE를 사용할 프로젝트)    d) 엔드포인트 정보(마스터 URL, Ingress Host, 노드 포트 호스트 주소, 노드 포트 범위)를 입력한다.\n   엔드포인트 설명     마스터 주소 Kubernetes API 주소. \u0026ldquo;https://host:port\u0026rdquo; 형식을 사용한다.   인그레스 호스트 주소 인그레스 방식에 사용할 Host IP Address 서비스(Master IP or Loadbelancer IP)   노드 포트 호스트 주소 노드에 포트를 붙여 서비스 노출하는 방식에서 포트 앞에 사용할 IP서비스(Master IP or Loadbelancer IP)   노드 포트 범위 노드에 포트를 붙여 서비스 노출하는 방식에서 IP뒤에 사용할 포트의 범위.(30000~32767 권장)    e) 클러스터 유형을 입력한다.\n   인증 정보 설명     Cluster CA Certification 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 ca.crt파일값 입력   Client Certificate Data 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 admin.crt파일 값 입력   Client Key Data 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 admin.key파일 값 입력    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.2-update/",
	"title": "2.2.2 클러스터 수정",
	"tags": [],
	"description": "",
	"content": "클러스터의 기본정보, 프로바이더, 모니터링, 클러스터 유형을 수정하는 기능이다. 프로바이더의 계정, 유형, 리전은 변경할 수 없다.\na) 클러스터 → 클러스터 선택 → 등록관리 → 수정버튼 → 수정 완료 후 저장버튼을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.3-delete/",
	"title": "2.2.3 클러스터 삭제",
	"tags": [],
	"description": "",
	"content": "클러스터 정보를 삭제하는 기능이다. 해당 클러스터를 사용하고 있는 어플리케이션 맵이 있을 경우 삭제할 수 없다.\na) 클러스터 → 클러스터 선택 → 등록관리 → 삭제버튼을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.4-volume/",
	"title": "2.2.4 볼륨",
	"tags": [],
	"description": "",
	"content": "Public Cloud 스토리지 및 외부 스토리지 정보를 등록하고, 볼륨 현황에 대한 모니터링 서비스를 제공한다.\na) 클러스터 → 클러스터 선택 → 볼륨을 클릭한다.    스토리지 설정 정보 설명     + 버튼 스토리지 추가 페이지로 이동   이름 스토리지 이름(사용자지정)   타입 스토리지 종류(NFS/EBS/Google Persistent Disk/Azure Disk)   스토리지 클래스 이름 k8s에 등록된 Class Name   정책 스토리지 볼륨의 정책 설정(Retain,Recyle,Delete)   상태 스토리지 사용 상태       볼륨 설정 정보 설명     볼륨이름 PVC 이름   상태 PVC 마운트 상태   사용량 PV의 할당된 가용량과 사용량   접속모드 PV 접속 권한   Age PVC 생성 경과 시간    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.5-add/",
	"title": "2.2.5 스토리지 추가",
	"tags": [],
	"description": "",
	"content": "볼륨을 추가하여 사용 할 수 있다.\na) 클러스터 → 클러스터 선택 → 볼륨을 클릭 → 오른쪽 상단 + 모양의 버튼을 클릭한다. b) 클러스터, 이름, 설명, 스토리지 플러그인, 정책, 스토리지 클래스 이름을 차례로 기입한다.    스토리지 등록 정보 설명     이름 PV 이름   설명 PV 사용자 설명   유형 스리지 유형 (Single, Shared)   스토리지 플러그인 스토리지 종류(NFS, NFS Named, EBS, Google Persistent Disk, Azure Disk)   정책 PV 사용 정책(RETAIN/DELETE)       유형 설명     Single 워크로드 하나에서만 사용 가능한 스토리   Shared 워크로드 간에 공유하여 사용 가능한 스토리지       정책 설명     Retain PersistentVolumeClaim(PVC)가 삭제되도 PersistentVolume(PV)안에 데이터가 남는다. 추후 재사용 가능하지만 재사용시 PV를 다시 등록해야 한다.   Delete PVC가 삭제되면서 해당 PV도 함께 삭제된다.    c) 사용하는 스토리지 플러그인에 따라 스토리지 클래스와 파라미터 설정.\n NFS 스토리지 플러그인 사용 시      스토리지 플러그인 NFS     총용량 스토리지 총용량(Gb)   정책 Retain과 Delete 정책을 지원   스토리지 클래스 이름 default-storage(기본값)   스토리지 프로비저너 스토리지 프로비저닝 값 입력     NFS Named 스토리지 플러그인 사용 시      스토리지 플러그인 NFS Named     정책 NFS Named 플러그인은 Retain 단일 정책을 지원   총용량 스토리지 총용량(Gb)   server 스토리지의 IP Address   path mount 경로    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.6-edit/",
	"title": "2.2.6 스토리지 편집",
	"tags": [],
	"description": "",
	"content": "등록된 스토리지 정보를 편집하는 기능이다. 단, NFS Named 타입의 스토리지는 편집할 수 없다.\na) 클러스터 → 클러스터 선택 → 볼륨 → 볼륨 액션 버튼 → 스토리지 편집을 클릭한다. b) 이름, 설명, 정책, 스토리지 클래스 이름, 파라미터값을 수정 할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.2.cluster/2.2.7-delete/",
	"title": "2.2.7 스토리지 삭제",
	"tags": [],
	"description": "",
	"content": "등록된 스토리지 정보를 삭제하는 기능이다.\na) 클러스터 → 클러스터 선택 → 볼륨 → 볼륨 액션버튼 → 볼륨 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.3.workspace/",
	"title": "2.3 워크스페이스",
	"tags": [],
	"description": "",
	"content": "빌드 및 배포, 운영할 수 있는 워크스페이스를 할당한다. 워크스페이스 별로 사용자, 클러스터, 라이브러리 등록이 가능하다.\na) 좌측 메인메뉴 → 워크스페이스    서비스 설정 설명     워크스페이스 명 워크스페이스 이름 (사용자지정)   클러스터 수 워크스페이스에 할당된 클러스터 수   구성원 수 워크스페이스를 사용하는 구성원 수   시스템 명 시스템 명   설명 워크스페이스 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.3.workspace/2.3.1-add/",
	"title": "2.3.1 워크스페이스 생성",
	"tags": [],
	"description": "",
	"content": "워크스페이스를 생성 하는 기능이다. 워크스페이스 별로 구성원 등록 및 클러스터, 레지스트리(저장소) 등록을 할 수 있다.\na) 좌측 메인메뉴 → 워크스페이스 → 오른쪽 상단 워크스페이스 생성 버튼을 클릭한다. b) 서비스에 사용할 이름, 설명, 색상, 레지스트리 및 구성원, 클러스터 정보를 등록한다. c) 구성원, 클러스터 추가는 체크박스가 선택 된것만 반영이 된다.\n 클러스터 편집  구성원 편집   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.3.workspace/2.3.2-edit/",
	"title": "2.3.2 워크스페이스 편집",
	"tags": [],
	"description": "",
	"content": "워크스페이스 정보를 편집하는 기능이다. 워크스페이스 이름, 설명, 색상 변경이 가능하며 해당 서비스를 사용할 구성원 편집, 워크스페이스에 등록할 클러스터 편집이 가능하다.\na) 좌측 메인메뉴 → 워크스페이스 → 워크스페이스 액션 메뉴 → 워크스페이스 편집 → 수정완료 후 수정버튼을 클릭한다. b) 워크스페이스 명, 설명, 색상 및 구성원, 클러스터 수정이 가능하다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/2.system-management/2.3.workspace/2.3.3-delete/",
	"title": "2.3.3 워크스페이스 삭제",
	"tags": [],
	"description": "",
	"content": "워크스페이스를 삭제 하는 기능이다. 삭제 시, 워크스페이스에 클러스터가 등록되어 있을 경우 삭제가 불가능하며 워크스페이스 편집에서 클러스터를 제거 후 삭제를 진행할 수 있다.\na) 좌측 메인메뉴 → 워크스페이스 → 워크스페이스 액션 메뉴 → 워크스페이스 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/3.dashboard/3.1-user/",
	"title": "3.1 사용자 관리",
	"tags": [],
	"description": "",
	"content": " 현재 사용자 정보를 확인 및 관리 할 수 있다.\na) 대시보드 -\u0026gt; 페이지 왼쪽 하단 사용자 모양 버튼을 클릭    목록 설명     ID 현재 사용중인 계정의 ID를 확인할 수 있다.   사용자 언어 선택 Korean, Japanese, English, Chinese 중 언어를 선택할 수 있다.   사용자 타임존 선택 Seoul, Tokyo, NewYork, Shanghai 중 타임존을 선택할 수 있다.   테마 Light와 Dark 테마 변경할 수 있다.   비밀번호 변경 기존 비밀번호, 신규 비밀번호를 넣어 비밀번호 변경이 가능하다.   도움말 Cocktail Cloud 기능 사용을 위한 도움말 매뉴얼이 제공된다.   제품명 제품명을 보여준다.   버전 현재 칵테일 버전을 확인할 수 있다.   로그아웃 현재 사용중인 계정을 로그아웃 한다.    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/",
	"title": "4.1 应用程序管理",
	"tags": [],
	"description": "",
	"content": "管理应用程序表和相应的工作负载群组以及工作负载。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/",
	"title": "4.1.1 应用程序表",
	"tags": [],
	"description": "",
	"content": " 应用程序的组成单位。\n作为一种逻辑单位建群，并部署群组的工作负载。\n可查看组成应用程序表的集群计量信息或工作负载的监控信息。\n可通过部署作业，创建工作负载或更新。\na) 选择服务→应用程序表点击。    目录 说明      由逻辑单位组成的群组。如激活操作菜单，可修改群组名称和列数， 以及往右或往左移、右或左边添加群组、删除群组。点击+按钮可 创建工作负载。    被部署在应用程序里的工作负载。可利用拖\u0026amp;放自由移动。    当前正在部署的工作负载如上图，一目了然地简单显示。    被部署在应用程序的工作负载单位，对正使用的 CPU、内存、网 络的监控服务菜单。    Build 映像，可一次性执行到部署作业，可修改正在部署中的工作 负载映像版本重新部署，同时批量部署多个工作负载。    可保存使用配置图等多个设置文件。    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.1-servicemapgroup/",
	"title": "4.1.1-1 서비스 맵 그룹관리",
	"tags": [],
	"description": "",
	"content": " 서비스맵에 대하여 그룹으로 관리한다. 해당 그룹에 서비스맵을 생성할 수 있다. 그룹명, 그룹색상을 변경하거나, 그룹 추가, 이동, 삭제 가능하다.\na) 좌측 메인메뉴 → 서비스 → 서비스맵 그룹의 ▼(액션 버튼) 클릭하여 메뉴를 활성화한다.    목록 설명     그룹명 변경 그룹명, 그룹색상을 변경   위로 이동 그룹을 위로 이동   아래로 이동 그룹을 아래로 이동   위에 그룹 추가 그룹 위에 새로운 그룹을 생성   아래에 그룹 추가 그룹 아래에 새로운 그룹을 생성   그룹 삭제 그룹을 삭제(그룹에 서비스맵이 존재하면 삭제가 불가능)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.2-add/",
	"title": "4.1.1-2 应用程序表创建",
	"tags": [],
	"description": "",
	"content": " 服务里创建新的应用程序表。\na) 点击服务→服务右侧的+。 b) 选择需要使用的集群，指定表的名称和命名空间名称，并创建应用程序表。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.2-add_ko/",
	"title": "4.1.1-2 서비스 맵 생성",
	"tags": [],
	"description": "",
	"content": " 서비스에 새로운 서비스 맵을 생성 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스의 우측 + 버튼을 클릭한다. b) 사용하고자 하는 클러스터를 선택하고, 맵 이름과 네임스페이스 이름을 지정하여 서비스 맵을 생성한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.3-rename/",
	"title": "4.1.1-3 应用程序表名称修改",
	"tags": [],
	"description": "",
	"content": " 修改应用程序表的名称。\na) 点击服务→用用程序表右侧的✎。 b) 被激活的名称输入栏里输入修改内容后，按 Enter 键修改应用程序表的名称。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.3-rename_ko/",
	"title": "4.1.1-3 서비스 맵 이름변경",
	"tags": [],
	"description": "",
	"content": " 서비스 맵의 이름을 변경한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵의 우측 ✎ 버튼 ㅡ을 클릭한다. b) 활성화된 이름 입력란에 내용을 변경 후, Enter 키를 눌러 서비스 맵의 이름을 변경한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.4-delete/",
	"title": "4.1.1-4 应用程序表删除",
	"tags": [],
	"description": "",
	"content": " 删除应用程序表的功能。为了删除应用程序表，需先删除正在部署中的工作负载。\na) 服务→应用程序表选择→应用程序表删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.4-delete_ko/",
	"title": "4.1.1-4 서비스 맵 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스 맵을 삭제하는 기능이다. 서비스 맵을 삭제하기 위해서는 배포 중인 워크로드가 없어야 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 서비스 맵 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.1.servicemanagement/4.1.2-group/",
	"title": "4.1.2 群组管理",
	"tags": [],
	"description": "",
	"content": " 对工作负载进行群组管理。可以针对相应的群组，创建工作负载。可以修改群组名称，添加、移动、删除群组，以及修改列数。\na) 服务→选择应用程序表→点击工作负载群组的▼(操作按钮)，激活菜单。    目录 说明     群组名称修改 修改群组名称   列数修改 修改列数(最小:1~最多:8)   向左移动 群组向左移   向右移动 群组向右移   左边添加群组 群组左边创建新的群组   右边添加群组 群组右边创建新的群组   群组删除 删除群组(如群组有工作负载，则不能删除)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/",
	"title": "4.2 工作负载",
	"tags": [],
	"description": "",
	"content": " 组成应用程序的最小单位。可由 1 个工作负载组成应用程序，也可以多个工作负载组成 1 个应 用程序。\na) 选择服务→应用程序表，可查看表里面的工作负载。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.1-add/",
	"title": "4.2.1 工作负载创建",
	"tags": [],
	"description": "",
	"content": " 在群组管理创建和运行工作负载。\na) 点击服务→应用程序表选择→工作负载群组+按钮(工作负载创建)。 选择工作负载类型。 b) 输入基本信息、容器、实例、更新策略、服务端口、卷的内容，点击创建按钮生成工作负 载。  工作负载基本信息定义      项目 说明     名称 需要创建的工作负载名称   群组 创建工作负载的工作负载群组   类型 工作负载的实例类型(Single/Multi)   说明 工作负载的说明     容器生成\n 点击容器右‘+’键  输入容器名称，在容器的 Build 或映像中选择需要使用的映像，定义需要使用的 CPU/ 内存的资源值。  一旦容器创建，可输入需要使用的命令。  输入容器的环境参数值，或者可选择 Secret 设置。  可设置安全策略  可设置容器的健康检查   实例(只适用于服务器类型为 Multi 时)\n 点击实例项目的‘✎’       项目 说明     自动缩放类型 定义自动缩放的基准   CPU 使用率(百分比) 自动缩放的基准(使用率)   内存使用率(百分比) 自动缩放的基准(使用率)   最大个数 缩放实例最大值   实例数 默认实例台数     更新策略(只适用于服务器类型为 Multi 时)\n 点击更新策略项目的‘✎’按钮       项目 说明     更新策略 定义滚动更新/重建。通过滚动更新定义，中断 Pod 和扩展 Pod，可无 中断更新以增强资源效率，也可通过重建一次性更新   服务去除(中断 Pod 比率) 滚动更新时，被中断的 Pod 比率   扩展 Pod 比率 滚动更新时，能增加 Pod 比率     服务端口  部署在 Baremetal(使用节点端口时，Ingress 使用为选项)  部署在公共云(使用节点端口时，Ingress 使用为必项)      项目 说明     Internal 容器内部通信网络   Cluster IP Pod内部通信网络   Node Port 对外泄露网络   Loadbalancer 负载均衡通信网络   Sticky Session 按照特定客户端的请求，保持 Session 的选项       项目 说明     名称 定义端口名称   目标端口 定义容器端口   端口 定义外部端口   指定与否 是否指定节点端口使用   Ingress 使用与否 Ingress 使用与否。(部署到公共云时，负载均衡类型不能使用 Ingress， 使用节点端口类型时，Ingress 是必项。但，Ingress 在 Baremetal 里是作 为选项，可同样使用)     卷      项目 说明     卷 在工作负载创建需要使用的卷   挂载卷 工作负载挂在已创建卷上   持久化挂载卷 工作负载挂在已创建持久化卷上       卷种类 说明     Empty Dir 容器里面的临时目录卷   Host Path 挂在容器的集群服务器卷   Config Map 使用工作负载上已注册的配置文件   Secret 使用工作负载上已注册的 Secret 文件(可设置有无权限使用，使用 CHMOD 规 则。不使用时的默认权限为 644)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.1-add_ko/",
	"title": "4.2.1 워크로드 생성",
	"tags": [],
	"description": "",
	"content": " 그룹관리에서 워크로드를 생성하고 실행한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 그룹 + 버튼(워크로드 생성)을 클릭한다. 워크로드 유형을 선택 한다. b) 기본정보, 컨테이너, 인스턴스, 업데이트 정책, 서비스 포트, 볼륨 내용을 작성하고 생성버튼을 눌러 워크로드를 생성한다.  워크로드 기본 정보 정의      항목 설명     이름 생성하는 워크로드의 이름   그룹 워크로드가 생성되는 워크로드 그룹   유형 워크로드의 인스턴스 유형(Single/Multi)   설명 워크로드의 설명   서비스 계정 명 서비스 계정 명   노드 셀렉터 키 노드 셀렉터 키   노드 셀렉터 값 노드 셀렉터 값     컨테이너 생성\n 컨테이너 우측 \u0026lsquo;+ 버튼\u0026rsquo; 클릭  컨테이너 이름을 입력하고, 컨테이너로 사용할 이미지를 빌드나 이미지에서 선택하고, 사용할 CPU/Memory 리소스값을 정의한다.  컨테이너가 생성하면서 사용할 커맨드를 입력 할 수 있다.  컨테이너의 환경변수 값을 입력하거나 시크릿을 선택하여 설정할 수 있다.  보안정책을 설정 할 수 있다.  컨테이너의 헬스체크를 설정 할 수 있다.   인스턴스\n 인스턴스 항목의 \u0026lsquo;✎ 버튼\u0026rsquo; 클릭  서버 유형이 Single인 경우 인스턴스 종료 대기시간만 조절 가능  (서버 유형이 Multi일 경우만 해당)       항목 설명     오토스케일링 유형 오토스케일링을 하기 위한 기준을 정의   CPU 사용율(백분율) 오토스케일링의 기준(사용율)   메모리 사용률(백분율) 오토스케일링의 기준(사용율)   최대개수 스케일링의 인스턴스 최댓값   인스턴스 수 기본 인스턴스의 대수     업데이트 정책(서버 유형이 Multi일 경우만 해당)\n 업데이트 정책 항목의 \u0026lsquo;✎ 버튼\u0026rsquo; 클릭       항목 설명     업데이트 정책 Rolling Update/Recreate로 정의. 롤링업데이트를 통해 중단 Pod와 확장 Pod를 정의하여 자원의 효율성을 증대한 무중단 업데이트가 가능하고, Recreate 통하여 한 번에 업데이트가 가능   서비스 제거(중단Pod비율) 롤링업데이트를 할 경우, 중단되는 Pod의 비율   확장 Pod비율 롤링업데이트를 할 경우, 늘어날 수 있는 Pod의 비율     서비스 포트      항목 설명     Internal 컨테이너 내부통신 네트워크   Cluster IP Pod 내부통신 네트워크   Node Port 외부노출 네트워크   Loadbalancer 로드밸런스 통신 네트워크   Sticky Session 특정 클라이언트 요청에 세션을 유지하는 옵션   타임아웃(초) Sticky Session 유지 시간       항목 설명     이름 포트 이름을 정의   타겟 포트 컨테이너 포트를 정의   포트 외부 포트를 정의   지정 여부 node port를 지정하여 사용할 것인지에 대한 여부   인그레스 사용 여부 인그레스를 사용할 것인지에 대한 여부. (Public Cloud로 배포할 경우 Loadbalancer 타입으로는 인그레스 사용이 불가)     볼륨      항목 설명     볼륨 워크로드에서 사용할 볼륨을 생성   볼륨 마운트 생성된 볼륨에 워크로드를 마운트   퍼시스턴트 볼륨 마운트 생성된 퍼시스턴트 볼륨에 워크로드를 마운트       볼륨 종류 설명     Empty Dir 컨테이너 안에 임시 디렉토리 볼륨   Host Path 컨테이너가 올라가 있는 클러스터 서버 볼륨   Config Map 워크로드에 등록 되어있는 Config File을 사용   Secret 워크로드에 등록 되어있는 Secret File을 사용 (권한사용 유무 설정이 가능하고, CHMOD 룰을 사용. 미사용시 Default 권한은 644)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.2-edit/",
	"title": "4.2.2 工作负载编辑",
	"tags": [],
	"description": "",
	"content": " 编辑和重新部署运行或者停止状态的工作负载。\na) 服务→选择应用程序表→点击工作负载。 b) 编辑基本信息、容器、实例、更新策略、服务端口、卷内容后，按修改重新部署工作负载。 如果工作负载正在运行中，不能修改工作负载类型和容器映像。但，可以修改容器映像标签。 工作负载中断后，可以编辑以及修改。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.2-edit_ko/",
	"title": "4.2.2 워크로드 편집",
	"tags": [],
	"description": "",
	"content": " Running 또는 Stopped 상태의 워크로드를 편집하고 재배포 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드를 클릭한다. b) 기본정보, 컨테이너, 인스턴스, 업데이트 정책, 서비스 포트, 볼륨 내용을 편집하고 수정버튼을 눌러 워크로드를 재배포 한다. 워크로드 Running 중이라면 워크로드유형과 컨테이너의 이미지 변경이 불가능. 단, 컨테이너 이미지태그 변경은 가능. 이를 변경하기 위해서는 워크로드 중지 후 편집하면 편집이 가능 하다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.3-instance/",
	"title": "4.2.3 工作负载实例信息查询",
	"tags": [],
	"description": "",
	"content": " 可查询工作负载实例信息。\n   实例 说明     Pods 详细信息、状态信息、事件信息、部署信息、网络终端(容器)、日 志(容器)   Services 详细信息、事件信息、部署信息   Persistent Volume Claims 详细信息、卷详细信息、部署信息   Deployments 详细信息、事件信息、部署信息、自动缩放信息(选项)   Replica Sets 详细信息、事件信息、部署信息   Ingresses 详细信息、部署信息    a) 服务→选择应用程序表→点击工作负载。  在实例页面点击刷新时，只重新查询实例信息。   b) 实例 Pod→容器和网络终端连接 c) 实例 Pod→容器日志 d) 服务→选择应用程序表→选择工作负载→点击实例的‘操作’按钮(激活)→点击详细信 e) 服务→选择应用程序表→选择工作负载→点击实例的‘操作’按钮(激活)→点击事件信 息 f) 服务→选择应用程序表→选择工作负载→点击实例的‘操作’按钮(激活)→点击部署信息 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.3-instance_ko/",
	"title": "4.2.3 워크로드 인스턴스 정보 조회",
	"tags": [],
	"description": "",
	"content": " 워크로드 인스턴스의 정보를 조회 할 수 있다.\n   인스턴스 정보     Instance 상세 정보, 상태 정보, 이벤트 정보, 배포 정보, 웹터미널(컨테이너), 로그(컨테이너)   Services 상세 정보, 이벤트 정보, 배포 정보   Persistent Volume Claims 상세 정보, 볼륨 상세 정보, 배포 정보   Controller 상세 정보, 이벤트 정보, 배포 정보, 오토스케일러(Deployments만 해당) 정보(옵션)   Ingresses 상세 정보, 배포 정보    a) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 선택  상태에서 새로고침 클릭시 상태정보(서버상태, 사용량, 인스턴스 등등)가 재조회 된다.   b) 인스턴스 이름 클릭 c) 인스턴스 → 컨테이너와 웹 터미널 연결 d) 인스턴스 → 컨테이너의 로그 인스턴스 액션메뉴 e) 인스턴스의 액션 버튼을 클릭(활성화) → 상태 정보 클릭 f) 인스턴스의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭 g) 인스턴스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 h) 컨트롤러 이름 클릭 컨트롤러 액션메뉴  Type Replicaset  Type Deployments   i) 컨트롤러의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭 j) 컨트롤러의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 k) 컨트롤러의 액션 버튼을 클릭(활성화) → 오토스켈러 정보(Type: Deployments) 클릭 l) 오토스켈러 정보 팝업 → 이름 클릭 오토스켈러 팝업 액션메뉴 m) 오토스켈러 팝업 액션 버튼을 클릭(활성화) → 상태 정보 클릭 n) 오토스켈러 팝업 액션 버튼을 클릭(활성화) → 배포 정보 클릭 o) 서비스 이름 클릭 서비스 액션메뉴 p) 서비스의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭 q) 서비스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 r) 볼륨 요청 이름 클릭 s) 볼륨 요청 볼륨 클릭 볼륨 요청 액션메뉴 t) 볼륨 요청의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 u) 인그레스 이름 클릭 인그레스 액션메뉴 v) 인그레스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.4-stop/",
	"title": "4.2.4 工作负载中断",
	"tags": [],
	"description": "",
	"content": " 中断已部署的工作负载。如果中断工作负载，POD 被删除。\na) 服务→选择应用程序表→点击运行状态的工作路径操作菜单(激活)→点击中断  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.4-stop_ko/",
	"title": "4.2.4 워크로드 중지",
	"tags": [],
	"description": "",
	"content": " 배포된 워크로드를 중지한다. 워크로드를 중지하면 POD는 삭제된다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Running 상태의 워크로드 액션메뉴를 클릭(활성화) → 중지를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.5-start/",
	"title": "4.2.5 工作负载开启",
	"tags": [],
	"description": "",
	"content": " 开启已中断的工作负载。\na) 服务→选择应用程序表→点击停止状态的工作路径操作菜单(激活)→点击开始 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.5-start_ko/",
	"title": "4.2.5 워크로드 시작",
	"tags": [],
	"description": "",
	"content": " 중지된 워크로드를 시작한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Stopped 상태의 워크로드 액션메뉴를 클릭(활성화) → 시작을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.6-restart/",
	"title": "4.2.6 工作负载重启",
	"tags": [],
	"description": "",
	"content": " 重启工作负载。类似计算机重启的概念。\na) 服务→选择应用程序表→点击运行状态的工作路径操作菜单(激活)→点击重启 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.6-restart_ko/",
	"title": "4.2.6 워크로드 재시작",
	"tags": [],
	"description": "",
	"content": " 워크로드를 재시작한다. 컴퓨터의 Rebooting과 같은 개념이다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Running 상태의 워크로드 액션메뉴를 클릭(활성화) → 재시작을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.7-delete/",
	"title": "4.2.7 工作负载删除",
	"tags": [],
	"description": "",
	"content": " 删除已中断的工作负载。\na) 服务→选择应用程序表→点击停止状态的工作路径操作菜单(激活)→点击删除 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.2.workload/4.2.7-delete_ko/",
	"title": "4.2.7 워크로드 제거",
	"tags": [],
	"description": "",
	"content": " 중지된 워크로드를 제거한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Stopped 상태의 워크로드 액션메뉴를 클릭(활성화) → 제거를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.3.monitoring/",
	"title": "4.3 监控",
	"tags": [],
	"description": "",
	"content": " 根据不同的工作负载、POD、容器，可查询正在使用的 CPU、内存、网络使用量。\na) 服务→选择应用程序表→点击监控 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.4.pipeline/",
	"title": "4.4 管道",
	"tags": [],
	"description": "",
	"content": " 通过管道功能，可一次性进行映像 Build 作业到工作负载部署作业。\n并且，可修改正在部署的工作负载映像版本再重新部署。\na) 服务→选择应用程序表→点击管道    管道菜单 说明     批量运行 批量运行管道作业   运行 运行相应管道作业       映像菜单 Build 说明     映像标签 X 输入注册表标签时，部署相应版本的工作负载   指定 ⃝ 映像版本中部署指定版本的工作负载   最新 ⃝ 映像版本中部署最新版本的工作负载   Build\u0026amp;部署 ⃝ 新的 Build 作业后，部署相应映像版本的工作负载    b) 管道运行 1. 使用通用映像创建工作负载时 输入映像标签后，点击‘运行’或‘批量运行’(只有部署版本和输入版本不一致时可以运行。 但 latest 除外) 2. 使用 Build 映像创建工作负载时  使用指定映像部署  在管道作业目录右侧点击‘指定’以及选择映像后，点击‘运行’或者‘批量运行’(只有部 署版本和输入版本不一致时可以运行)  使用指定映像部署  在管道作业目录右侧选择‘Build\u0026amp;部署’后，确认‘运行与否’。之后点击‘运行’或‘批量 运行’(只有确认运行与否时可以运行)  使用最新映像部署  在管道作业目录右侧选择‘最新’后，点击‘运行’或‘批量运行’。(只有部署版本和输入版 本不一致时可以运行)  切换到 Build 编辑画面及日志查看  点击 Build 映像名称时，切换到 Build 编辑画面。 点击 Build 标签名称时，可以看到日志。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.5.persistencevolume/",
	"title": "4.5 持久化卷",
	"tags": [],
	"description": "",
	"content": " 提供持久化卷的现状以及创建和删除。\na) 服务→应用程序表→持久化卷    项目 说明     卷名称 点击已创建持久化卷名称时，切换到持久化卷详细页面   卷类型 卷类型(Single、Shared)   状态 卷的状态(Ready, Mounted)   使用量 使用量、请求量、总量   Age 卷创建需要时间   Filter 通过搜索查询所必要的持久化卷   + 按钮 切换到持久化卷创建页面     持久化卷详细  持久化卷操作菜单   a) 卷请求信息 b) 卷信息 c) 存储信息 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.5.persistencevolume/4.5.1-add/",
	"title": "4.5.1 持久化卷创建",
	"tags": [],
	"description": "",
	"content": " 可以创建持久化卷。\na) 服务→应用程序表→持久化卷    项目 说明     持久化卷类型 卷类型(Single, Shared)   存储 显示集群里已注册的存储目录   访问模式 持久化卷类型为 Single 时和 ReadWriteOnce, Shard 时分别选择 ReadWriteMany、ReadOnlyMany   名称 需要创建的持久化卷名称   容量 需要创建的持久化卷容量(GB)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.5.persistencevolume/4.5.1-add_ko/",
	"title": "4.5.1 퍼시스턴트 볼륨 생성",
	"tags": [],
	"description": "",
	"content": " 퍼시스턴트 볼륨 생성 할 수 있다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨    항목 설명     퍼시스턴트 볼륨 타입 볼륨 타입(Single, Shared)   스토리지 클러스터에 등록된 스토리지 목록 노출   엑세스 모드 퍼시스턴트 볼륨 타입이 Single일 경우 ReadWriteOnce, Shard일 경우 ReadWriteMany, ReadOnlyMany를 선택   이름 생성할 퍼시스턴트 볼륨의 이름   용량 생성할 퍼시스턴트 볼륨의 용량(GB)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.5.persistencevolume/4.5.2-delete/",
	"title": "4.5.2 持久化卷删除",
	"tags": [],
	"description": "",
	"content": " 删除持久化卷。\na) 点击服务→应用程序表→持久化卷→持久化操作菜单(激活)→删除 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.5.persistencevolume/4.5.2-delete_ko/",
	"title": "4.5.2 퍼시스턴트 볼륨 삭제",
	"tags": [],
	"description": "",
	"content": " 퍼시스턴트 볼륨을 삭제한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨 → 퍼시스턴트 액션메뉴(활성화) → 삭제 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/",
	"title": "4.6.1 配置图",
	"tags": [],
	"description": "",
	"content": " 可将配置文件以 Map 形式保存于服务器，如同卷使用。\na) 服务→选择应用程序表→设置→点击配置图    项目 说明      切换到配置图添加页面   名称 配置图名称(用户指定)   说明 配置图说明(用户指定)    配置图的操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/4.6.1.1-add/",
	"title": "4.6.1.1 配置图添加",
	"tags": [],
	"description": "",
	"content": " 服务里添加配置图添加。\na) 服务→选择应用程序表→设置→配置图→点击配置图创建按钮，切换到注册页面。 b) 输入名称和数据值，按右上角创建按钮，创建配置图。    项目 说明     名称 输入名称的栏。之后不能编辑   说明 输入配置图说明的栏。   数据添加按钮 能添加配置图数据的按钮   KEY 配置图的秘钥值   VALUE 配置图的卷值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/4.6.1.1-add_ko/",
	"title": "4.6.1.1 컨피그 맵 추가",
	"tags": [],
	"description": "",
	"content": " 서비스에 컨피그 맵을 추가한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 생성 버튼을 클릭하여 등록페이지로 이동한다. b) 이름과 데이터의 값을 입력하고 우측 상단 생성 버튼을 눌러 컨피그 맵을 생성한다.    항목 설명     이름 이름을 입력하는 란입니다. 추후 편집이 불가   설명 컨피그 맵의 설명을 입력하는 란입니다.   데이터 추가 버튼 컨피그 맵의 데이터 추가할 수 있는 버튼   KEY 컨피그 맵의 Key 값   VALUE 컨피그 맵의 Value 값    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/4.6.1.2-edit/",
	"title": "4.6.1.2 配置图修改",
	"tags": [],
	"description": "",
	"content": " 可修改服务里已添加的配置图数据。(名称不能修改)\na) 服务→选择应用程序表→设置→配置图→配置图操作菜单(激活)→点击配置图修改 b) 修改数据值后，点击修改按钮，编辑配置图。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/4.6.1.2-edit_ko/",
	"title": "4.6.1.2 컨피그 맵 수정",
	"tags": [],
	"description": "",
	"content": " 서비스에 추가 된 컨피그 맵의 데이터를 수정 할 수 있다.(이름 변경불가)\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 액션메뉴(활성화) → 컨피그 맵 수정을 클릭 b) 데이터 값 변경 후, 수정 버튼을 클릭하여 컨피그 맵을 편집한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/4.6.1.3-delete/",
	"title": "4.6.1.3 配置图删除",
	"tags": [],
	"description": "",
	"content": " 删除服务里已添加的配置图。\na) 服务→选择应用程序表→设置→配置图→配置图操作菜单(激活)→点击配置图删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.1.configmap/4.6.1.3-delete_ko/",
	"title": "4.6.1.3 컨피그 맵 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스에 컨피그 맵을 삭제한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 액션메뉴(활성화) → 컨피그 맵 삭제 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/",
	"title": "4.6.2 Secret",
	"tags": [],
	"description": "",
	"content": " 以 Secret 文件形式保存于服务器，如同环境参数及卷使用。\na) 服务→选择应用程序表→设置→点击 Secret。    项目 说明      切换到 Secret 添加页面   名称 Secret 名称(用户指定)   说明 Secret 说明(用户指定)    Secret 的操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/",
	"title": "4.6.2 Secret",
	"tags": [],
	"description": "",
	"content": " 以 Secret 文件形式保存于服务器，如同环境参数及卷使用。\na) 服务→选择应用程序表→设置→点击 Secret。    项目 说明      切换到 Secret 添加页面   名称 Secret 名称(用户指定)   说明 Secret 说明(用户指定)    Secret 的操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/4.6.2.1-add/",
	"title": "4.6.2.1 Secret 添加",
	"tags": [],
	"description": "",
	"content": " 服务里添加 Secret。\na) 服务→选择应用程序表→设置→Secret→点击 Secret 创建按钮，切换到注册页面。 b) 输入名称和数据值，点击右上角创建按钮，创建 Secret。    项目 说明     名称 输入名称的栏。之后不能编辑   说明 输入 Secret 说明的栏。   数据添加按钮 能添加 Secret 数据的按钮   KEY Secret 的秘钥值   VALUE Secret 的卷值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/4.6.2.1-add_ko/",
	"title": "4.6.2.1 시크릿 추가",
	"tags": [],
	"description": "",
	"content": " 서비스에 시크릿을 추가한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 생성 버튼을 클릭하여 등록페이지로 이동한다. b) 이름과 데이터의 값을 입력하고 우측 상단 생성 버튼을 눌러 시크릿을 생성한다.    항목 설명     이름 이름을 입력하는 란입니다. 추후 편집이 불가   설명 시크릿의 설정의 설명 입력 란입니다.   데이터 추가 버튼 시크릿의 데이터를 추가할 수 있는 버튼   KEY 시크릿의 Key 값   VALUE 시크릿의 Value 값    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/4.6.2.2-edit/",
	"title": "4.6.2.2 Secret 修改",
	"tags": [],
	"description": "",
	"content": " 可修改服务里已添加的 Secret 数据。(名称不能修改)\na) 服务→选择应用程序表→设置→Secret→Secret 操作菜单(激活)→点击 Secret 修改。 b) 修改数据值后，点击修改按钮，编辑 Secret。(为了安全考虑 Secret 值不显示) "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/4.6.2.2-edit_ko/",
	"title": "4.6.2.2 시크릿 수정",
	"tags": [],
	"description": "",
	"content": " 서비스에 추가 된 시크릿의 데이터를 수정 할 수 있다.(이름 변경불가)\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 액션메뉴(활성화) → 시크릿 수정을 클릭 b) 데이터 값 변경 후, 수정 버튼을 클릭하여 시크릿을 편집한다.(시크릿 값은 보안상 보여주지 않음) "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/4.6.2.3-delete/",
	"title": "4.6.2.3 Secret 删除",
	"tags": [],
	"description": "",
	"content": " 删除服务里的 Secret。\na) 服务→选择应用程序表→设置→Secret→Secret 操作菜单(激活)→点击 Secret 删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/4.service/4.6.config/4.6.2.secret/4.6.2.3-delete_ko/",
	"title": "4.6.2.3 시크릿 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스에 시크릿을 삭제한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 액션메뉴(활성화) → 시크릿 삭제를 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.1-add/",
	"title": "5.1 빌드 추가",
	"tags": [],
	"description": "",
	"content": " 도커 이미지에 대한 빌드 작업을 추가한다. 어플리케이션 다운로드, 빌드, 이미지 빌드로 총 3가지를 선택 할 수 있다.\na) 좌측 메인 메뉴 → 빌드 화면에서 [+] 버튼을 클릭하여 해당 페이지로 이동한다. b) 기본 정보, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드 내용을 작성하고 [생성] 버튼을 눌러 빌드를 생성한다.    빌드 생성 단계 설명     어플리케이션 다운로드 빌드에 필요한 소스를 다운로드   어플리케이션 빌드 소스의 컴파일이 필요할 경우 사용커맨드 - 빌드하면서 실행될 작업호스트 경로 - 작업이 이루어질 컨테이너 경로와 그와 마운트되는 호스트경로Working dir -실제 컨테이너안에서 작업할 경로로 컨테이너 경로와 맞춤이미지 - 빌드시 사용될 이미지   이미지 빌드 앞서 작업된 소스로 도커파일을 만들어 이미지를 만들고 해당이미지를 레지스트리 저장소에 저장하는 단계     빌드 기본 정보     기본 정보 설명     이름 생성할 빌드의 이름   최근 액션 빌드의 최근 동작 상태   상태 빌드의 현재 상태     어플리케이션 다운로드     어플리케이션 다운로드 설명     리파지토리 종류 소스파일을 내려받을 버전관리툴의 종류. 현재 버전에서는 GIT만을 지원   프로토콜 유형 리파지토리에서 제공하는 HTTP/HTTPS 프로토콜을 지원   깃 저장소 유형 Private/Common을 지원   리파지토리 URL 다운로드 하기위한 저장소 URL   리파지토리 User ID 저장소의 권한을 갖는 ID   리파지토리 패스워드 저장소의 권한을 갖는 ID의 패스워드   타겟 브랜치(Branch) 다운로드할 소스의 브랜치 정보     어플리케이션 빌드     어플리케이션 빌드 설명     커맨드 빌드하면서 실행될 명령어   호스트경로 다운로드 된 소스가 있는 경로   컨테이너 경로 작업이 이루어질 컨테이너 경로   Working dir 실제 컨테이너안에서 작업할 경로로 컨테이너 경로와 맞춤   이미지 빌드를 실행할 컨테이너 이미지     이미지 빌드     이미지 빌드 설명     Dockerfile 이미지를 만들기 위한 Dockerfile   레지스트리명 생성한 이미지를 저장하는 레지스트리를 선택   이미지 생성한 이미지를 레지스트리에 저장 할 이미지 명   버전 생성한 이미지를 레지스트리에 저장 할 태그    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.2-edit/",
	"title": "5.2 작업 설정 편집",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 편집한다.\na) 좌측 메인 메뉴 → 빌드 → 빌드 액션 메뉴(활성화) → [작업 설정 편집]을 선택하여 해당 페이지로 이동한다. b) 기본 정보, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드 내용을 수정하고 [수정] 버튼을 눌러 작업 설정을 편집한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.3-history/",
	"title": "5.3 히스토리",
	"tags": [],
	"description": "",
	"content": " 해당 빌드의 히스토리 페이지로 이동하여 빌드 실행 항목에 대하여 히스토리 정보를 볼 수 있다.\na) 좌측 메인 메뉴 → 빌드 화면에서 빌드 액션 메뉴(활성화) → [히스토리]를 클릭하여 해당 페이지로 이동한다. b) 액션 메뉴(활성화) → [로그보기]를 클릭하여 빌드 로그를 확인할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.4-run/",
	"title": "5.4 빌드 실행",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 실행한다. 빌드 실행을 하면, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드를 한 번에 실행한다.\na) 빌드 목록에서 실행할 빌드 작업의 빌드 액션 메뉴(활성화) → [빌드 실행]을 선택하면 빌드실행 창이 활성화된다. b|) 설명을 입력 후, [승인] 버튼을 클릭하면 빌드 작업을 실행한다. c) 빌드 실행 시, 현재 액션이 변경되면서 빌드 로그 창이 활성화 된다. 빌드생성시 체크박스에서 선택된 순서대로 순차적으로 실행되며 각 단계별로 빌드 과정을 확인 할 수 있다.\n(DOWN 단계에서 오류가 발생하면 다음 단계로 진행이 되지 않는다.)\n빌드 실행 작업이 \u0026ldquo;CREATE IMAGE STEP DONE.\u0026rdquo; 이라는 로그가 보인다면 이미지 빌드는 성공적으로 마무리된 것이다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.5-cancel/",
	"title": "5.5 빌드 취소",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 취소한다. 빌드작업이 실행 상태(RUNNING)인 것만 취소할 수 있다.\na) 빌드 목록에서 취소할 빌드 작업의 빌드 액션 메뉴(활성화) → [빌드 취소]를 선택하여 빌드 작업을 취소한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.6-log/",
	"title": "5.6 로그 보기",
	"tags": [],
	"description": "",
	"content": " 빌드작업의 최근 실행 로그를 보여준다. 빌드실행 도중이라면 실행 중인 로그를 볼 수 있다.\na) 빌드 목록에서 로그 보기할 빌드 작업의 빌드 액션 메뉴(활성화) → [로그 보기]를 선택하여 해당 빌드 작업의 빌드 실행 로그를 볼 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/5.build/5.7-delete/",
	"title": "5.7 작업 삭제",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 삭제한다.\na) 빌드 목록에서 삭제할 빌드 작업의 빌드 액션 메뉴(활성화) → [작업 삭제]를 선택하여 해당 빌드 작업을 삭제할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/6.1-add/",
	"title": "6.1 카탈로그 저장",
	"tags": [],
	"description": "",
	"content": " 카탈로그를 생성하여 필요한 서버들을 묶어 한 번에 배포 할 수 있다. 워크로드 컨테이너 이미지가 빌드 인 경우에는 이미지만을 저장하여 공용이미지로 변환하여 카탈로그로 저장한다. 저장된 카탈로그는 서비스 카탈로그에서 확인 할 수 있다.\n카탈로그 저장시 설정의 컨피그 맵, 시크릿도 템플릿으로 저장한다. (단, 시크릿은 보안상 값은 저장하지 않는다.)\na) 서비스에서 카탈로그로 저장하는 서비스 맵을 선택 b) 화면 우측 상단 카탈로그로 저장\u0026rsquo; 버튼을 클릭 c) 신규로 템플릿 생성 시 \u0026lsquo;신규\u0026rsquo; 카테고리를, 기존 템플릿을 업데이트 할 시 \u0026lsquo;기존\u0026rsquo; 카테고리 선택 후 버전, 요약정보, 편집 내용 작성 후 아래 \u0026lsquo;저장\u0026rsquo; 버튼을 클릭. (편집 입력필드는 Mark Down파일 형식으로 해당파일 작성 방법은 페이지 우측 \u0026lsquo;Mark Down support\u0026rsquo; 를 눌러 확인 할 수 있다.) "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/6.2-deploy/",
	"title": "6.2 카탈로그 배포",
	"tags": [],
	"description": "",
	"content": " 생성된 카탈로그를 배포할 수 있다. 필요 시 불필요한 인스턴스를 제외하거나 편집하여 배포 가능하다.\na) 카탈로그 → 배포하고자 하는 템플릿의 \u0026lsquo;배포\u0026rsquo; 버튼을 클릭 b) 템플릿 카테고리에서 서비스 맵 그룹, 클러스터, 서비스 맵 선택 후 \u0026lsquo;배포\u0026rsquo; 클릭. 원하지 않는 인스턴스는 제외하고 배포할 수 있다.  신규 Service Map 배포 시 서비스 맵 그룹, 클러스터를 선택하고 서비스 맵, 네임스페이스 이름을 작성 후 배포할 수 있다.  기존 Serivce Map 배포 시 서비스 맵 그룹, 서비스 맵 선택 후 배포할 수 있다.\n   c) 시크릿의 경우 보안상 값은 저장하지 않기에 배포 전 값을 입력해주어야 한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/6.3-edit/",
	"title": "6.3 카탈로그 편집",
	"tags": [],
	"description": "",
	"content": " 생성한 카탈로그 내용을 편집할 수 있다. 템플릿의 버전, 요약정보, 편집 내용을 수정할 수 있으며 각 인스턴스의 배포 정보를 수정할 수 있다.\na) 카탈로그 → 편집하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭 b) 수정할 템플릿의 버전, 요약정보, 편집 내용을 수정하고, 화면 우측 하단 \u0026lsquo;편집\u0026rsquo;버튼 클릭하여 인스턴스의 배포 정보를 수정할 수 있다. c) 편집 화면에서 컨피그맵과 시크릿은 편집은 불가하고 내용만 확인 가능하다. d) 워크로드 편집시 컨테이너 환경설정과 볼륨, 볼륨마운트는 편집은 불가하고 내용만 확인 가능하다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/6.4-delete/",
	"title": "6.4 카탈로그 삭제",
	"tags": [],
	"description": "",
	"content": " 불필요한 카탈로그를 삭제할 수 있다. (플랫폼 카탈로그는 삭제 불가)\na) 카탈로그 → 삭제하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭 b) 삭제할 템플릿의 버전을 선택 후, \u0026lsquo;삭제\u0026rsquo; 버튼을 클릭한다. 해당 템플릿 버전에 대하여 삭제된다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/6.5-export/",
	"title": "6.5 카탈로그 내보내기",
	"tags": [],
	"description": "",
	"content": " 존재하는 카탈로그를 내보내기할 수 있다. (플랫폼 카탈로그는 내보내기 불가)\na) 카탈로그 → 내보내기 하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭 b) 화면 왼쪽 상단의 \u0026lsquo;내보내기\u0026rsquo; 버튼을 클릭 한다. 해당 카탈로그가 카탈로그명-버전.zip 로 다운로드 된다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/6.catalog/6.6-import/",
	"title": "6.6 카탈로그 가져오기",
	"tags": [],
	"description": "",
	"content": " 내보내기된 카탈로그 파일과 가져오기 기능을 통해 카탈로그를 생성할 수 있다. (플랫폼 카탈로그는 가져오기 불가)\na) 카탈로그 → \u0026lsquo;가져오기\u0026rsquo; 버튼을 클릭 b) \u0026lsquo;Choose File\u0026rsquo; 버튼을 클릭해 가져오기 할 카탈로그 압축파일을 선택한 후 \u0026lsquo;저장\u0026rsquo; 버튼을 누른다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.1-cluster/",
	"title": "7.1 클러스터 목록",
	"tags": [],
	"description": "",
	"content": " 시스템관리자에 의해 등록된 Kubernetes 클러스터 목록 정보를 조회 하는 기능이다.\na) 클러스터 → 정렬 버튼을 클릭하여 원하시는 정렬기준을 선택한다.     클러스터 설정 설명     클러스터 클러스터 종류   노드 클러스터의 노드 숫자   CPU 클러스터의 전체 CPU 용량, 요청량, 사용량   메모리 클러스터의 전체 메모리 용량, 요청량, 사용량   서비스 클러스터의 서비스 맵(네임스페이스)의 숫자   워크로드 클러스터에서 배포 중인 워크로드의 숫자    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.2-node/",
	"title": "7.2 노드",
	"tags": [],
	"description": "",
	"content": " 전체 노드의 총 Disk, CPU, Memory, Network 사용량과 개별 노드의 라벨, 상태정보 등을 조회할 수 있는 화면이다. 노드 명을 클릭하면 노드 상세 정보를 조회 할 수 있다.\na) 클러스터 → 클러스터 선택 → 노드를 클릭한다. b) 노드명을 클릭하여 노드 상세 정보를 확인할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.3-application/",
	"title": "7.3 서비스",
	"tags": [],
	"description": "",
	"content": " 전체 서비스의 총 CPU, Memory, Network 사용량과 개별 서비스의 네임스페이스, 인스턴스 현황 등을 조회 할 수 있는 화면이다. 서비스 명을 클릭하면 해당 서비스 맵 페이지로 이동할 수 있다.\na) 클러스터 → 클러스터 선택 → 서비스을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.4-volume/",
	"title": "7.4 볼륨",
	"tags": [],
	"description": "",
	"content": " Public Cloud 스토리지 및 볼륨 현황에 대한 정보와 모니터링 서비스를 제공한다.\na) 클러스터 → 클러스터 선택 → 볼륨을 클릭한다.    스토리지 설정 정보 설명     이름 스토리지 이름(사용자지정)   타입 스토리지 종류(NFS/EBS/Google Persistent Disk/Azure Disk)   스토리지 클래스 이름 k8s에 등록된 Class Name   정책 스토리지 볼륨의 정책 설정(Retain,Recyle,Delete)   상태 스토리지 사용 상태       볼륨 설정 정보 설명     볼륨이름 PVC 이름   상태 PVC 마운트 상태   사용량 PV의 할당된 가용량과 사용량   접속모드 PV 접속 권한   Age PVC 생성 경과 시간    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.5-alarm/",
	"title": "7.5 알람",
	"tags": [],
	"description": "",
	"content": " 클러스터에 대한 이벤트를 알람으로 받을 수 있다.\na) 클러스터 → 클러스터 선택 → 알람을 클릭한다. b) 알람 명 클릭 시 상세 내용을 확인 할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.5.1/",
	"title": "7.5.1 알람 목록",
	"tags": [],
	"description": "",
	"content": "알람은 다음 목록에서 발생 조건이 지속 시간 만큼 계속될 경우 발생한다.\n AlertManager     알람 ID ALM-001     중요도 warning   알람 이름 AlertmanagerDown   지속 시간 5분   발생 조건 Alertmanager 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 Alertmanager의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.       알람 ID ALM-002     중요도 warning   알람 이름 AlertmanagerFailedReload   지속 시간 10분   발생 조건 Alertmanager의 설정 변경시, 설정 다시읽기 작업 실패시 발생   조치 사항 해당 Pod의 로그를 확인하여 ConfigMap의 설정 오류를 수정한다.     ETCD3     알람 ID ETC-001     중요도 critical   알람 이름 InsufficientMembers   지속 시간 3분   발생 조건 ETCD 메트릭 수집이 안 될 경우 발생   조치 사항 ETCD 클러스터의 상태를 확인한다. Prometheus의 로그 및 해당 노드의\netcd 상태를 확인한다.       알람 ID ETC-002     중요도 critical   알람 이름 NoLeader   지속 시간 1분   발생 조건 ETCD 리더가 없을 경우 발생   조치 사항 ETCD 클러스터의 상태를 확인한다. Disk Latency로 인한 문제 일 수 있으므로\n다음 명령을 ETCD 클러스터 전체 노드에서 실행한다. (ETCD Tuning)\n$ sudo ionice -c2 -n0 -p `pgrep etcd`       알람 ID ETC-003     중요도 warning   알람 이름 HighNumberOfLeaderChanges   지속 시간 즉시   발생 조건 최근 1시간 동안 3번 이상의 리더 변경이 발생할 경우   조치 사항 ETCD 클러스터의 상태를 확인한다. Disk Latency로 인한 문제 일 수 있으므로\n다음 명령을 ETCD 클러스터 전체 노드에서 실행한다. (ETCD Tuning)\n$ sudo ionice -c2 -n0 -p `pgrep etcd`       알람 ID ETC-004     중요도 warning   알람 이름 HighNumberOfFailedGRPCRequests   지속 시간 10분   발생 조건 최근 5분 이내에 gRPC 메소드 호출의 1% 이상 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-005     중요도 critical   알람 이름 HighNumberOfFailedGRPCRequests   지속 시간 5분   발생 조건 최근 5분 이내에 gRPC 메소드 호출의 5% 이상 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-006     중요도 critical   알람 이름 GRPCRequestsSlow   지속 시간 10분   발생 조건 최근 5분 동안 gRPC 메서드 요청 대기 시간 중 99 번째 백분위가 150ms보다 클 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-007     중요도 warning   알람 이름 HighNumberOfFailedHTTPRequests   지속 시간 10분   발생 조건 최근 5분 이내에 HTTP 엔드 포인트에 대한 요청의 1% 이상이 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-008     중요도 critical   알람 이름 HighNumberOfFailedHTTPRequests   지속 시간 5분   발생 조건 최근 5분 이내에 HTTP 엔드 포인트에 대한 요청의 5% 이상이 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-009     중요도 warning   알람 이름 HTTPRequestsSlow   지속 시간 10분   발생 조건 최근 5분 동안의 HTTP 요청 대기 시간 중 99번째 백분위가 150ms보다 클 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-010     중요도 warning   알람 이름 EtcdMemberCommunicationSlow   지속 시간 10분   발생 조건 최근 5분 동안의 멤버간 통신 대기 시간 중 99번째 백분위가 150ms보다 클 경우   조치 사항 ETCD 클러스터의 대역폭을 늘리거나 클러스터의 Scale-Up 필요.       알람 ID ETC-011     중요도 warning   알람 이름 HighNumberOfFailedProposals   지속 시간 즉시   발생 조건 최근 1시간 동안 5개 이상의 실패한 raft protocol 요청이 있을 경우.\n(RAFT Protocol은 ETCD 동기화 Protocol)   조치 사항 ETCD 메트릭 문서에 따르면 리더 선출의 일시적인 실패 또는 멤버 부족으로 인한\nETCD 클러스터 중단 시간이 길어질 경우 발생합니다.\n리더가 있는지, 중단된 ETCD 멤버가 있는지 확인       알람 ID ETC-012     중요도 warning   알람 이름 HighFsyncDurations   지속 시간 10분   발생 조건 최근 5분 동안의 wal fsync 지속 시간의 99번째 백분위가 500ms보다 클 경우\n(wal fsync: 로그 항목을 적용하기 전에 디스크에 저장시 호출.)   조치 사항 ETCD 메트릭 문서에 따르면 디스크에 문제가 있을 경우 발생한다고 함.       알람 ID ETC-013     중요도 warning   알람 이름 HighCommitDurations   지속 시간 10분   발생 조건 최근 5분 동안의 커밋 지속 시간 중 99번째 백분위가 250ms보다 클 경우\n(backend commit: 디스크에 대한 최근 변경 사항의 증분 스냅 샷의 커밋.)   조치 사항 ETCD 메트릭 문서에 따르면 디스크에 문제가 있을 경우 발생한다고 함.     General     알람 ID GEN-001     중요도 warning   알람 이름 TargetDown   지속 시간 10분   발생 조건 메트릭 수집 작업이 안 될 경우 발생. 어떤 작업이 실패인지 표시됨.   조치 사항 Prometheus의 로그 및 해당 작업에 해당하는 Pod의 로그 및 이벤트를 확인한다.       알람 ID GEN-002     중요도 none   알람 이름 DeadMansSwitch   지속 시간 즉시   발생 조건 DeadMansSwitch 알림.   조치 사항 해당 알람은 사용자에게 통지되지 않습니다.       알람 ID GEN-003     중요도 critical   알람 이름 TooManyOpenFileDescriptors   지속 시간 10분   발생 조건 file descriptor 사용율이 95%이상 일때 발생   조치 사항 노드의 Limit값을 변경한다.(노드의 재시작 필요)       알람 ID GEN-004     중요도 warning   알람 이름 FdExhaustionClose   지속 시간 10분   발생 조건 단순회귀분석(simple linear regression)을 이용하여 4시간 이내에\nfile descriptor 고갈이 예측될 경우 발생   조치 사항 해당 Pod의 로그 및 이벤트를 확인한다.\n필요할 경우, 노드의 Limit값을 변경한다.(노드의 재시작 필요)       알람 ID GEN-005     중요도 critical   알람 이름 FdExhaustionClose   지속 시간 10분   발생 조건 단순회귀분석(simple linear regression)을 이용하여 1시간 이내에\nfile descriptor 고갈이 예측될 경우 발생   조치 사항 해당 Pod의 로그 및 이벤트를 확인한다.\n필요할 경우, 노드의 Limit값을 변경한다.(노드의 재시작 필요)     Kube-ApiServer     알람 ID KAS-001     중요도 critical   알람 이름 K8SApiserverDown   지속 시간 5분   발생 조건 kube-apiserver 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 kube-apiserver의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.       알람 ID KAS-002     중요도 warning   알람 이름 K8SApiServerLatency   지속 시간 10분   발생 조건 최근 10분 동안의 요청 대기 시간 중 99번째 백분위가 1s보다 클 경우 발생   조치 사항 계속 발생할 경우, 마스터 노드를 증설한다.     Kube-ControllerManager     알람 ID KCM-001     중요도 critical   알람 이름 K8SControllerManagerDown   지속 시간 5분   발생 조건 kube-controller-manager 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 kube-controller-manager의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.     Kube-Scheduler     알람 ID KSC-001     중요도 critical   알람 이름 K8SSchedulerDown   지속 시간 5분   발생 조건 kube-scheduler 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 kube-scheduler의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.     Kube-State-Metrics     알람 ID KSM-001     중요도 warning   알람 이름 DeploymentGenerationMismatch   지속 시간 15분   발생 조건 Deployment에 설정한 generation과 수집된 generation이 다를 경우 발생   조치 사항 Deployment의 로그 및 이벤트를 확인한다.\n필요하면 Deployment를 재배포한다.       알람 ID KSM-002     중요도 warning   알람 이름 DeploymentReplicasNotUpdated   지속 시간 15분   발생 조건 Deployment에 설정한 replica 개수와 변경되거나 available 상태의\nreplica 개수가 다를 경우 발생   조치 사항 Deployment 수정 사항이 반영이 안 된 상태이므로 Deployment 및\nPod의 로그 및 이벤트를 확인한다.       알람 ID KSM-003     중요도 warning   알람 이름 DaemonSetRolloutStuck   지속 시간 15분   발생 조건 DaemonSet에 상태가 Ready가 아닌 Pod가 있을 경우 발생   조치 사항 해당 Daemonset과 Pod의 로그 및 이벤트를 확인한다.       알람 ID KSM-004     중요도 warning   알람 이름 K8SDaemonSetsNotScheduled   지속 시간 10분   발생 조건 DaemonSet에 실행되어 할 Pod 개수 보다 실행중인\nPod 개수가 작을 경우 발생   조치 사항 해당 Daemonset과 Pod의 로그 및 이벤트를 확인한다.\n배포가 안 된 노드가 정상인지 확인한다.\n마스터 노드가 격리된 경우, Daemonset에 toleration 설정이 되어 있는지 확인한다.       알람 ID KSM-005     중요도 warning   알람 이름 DaemonSetsMissScheduled   지속 시간 10분   발생 조건 DaemonSet에 잘못 스케쥴된 Pod가 생겼을 경우 발생   조치 사항 해당 Daemonset과 Pod의 로그 및 이벤트를 확인한다.       알람 ID KSM-006     중요도 warning   알람 이름 PodFrequentlyRestarting   지속 시간 10분   발생 조건 최근 1시간 동안 Pod 재시작 횟수가 5회 이상일 경우 발생   조치 사항 해당 Pod의 로그 및 이벤트를 확인한다. 필요하면 Pod를 재시작한다.     Kubelet     알람 ID KBL-001     중요도 warning   알람 이름 K8SNodeNotReady   지속 시간 1시간   발생 조건 Node 상태가 Ready가 아닐 경우 발생   조치 사항 해당 노드의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-002     중요도 critical   알람 이름 K8SManyNodesNotReady   지속 시간 1분   발생 조건 클러스터 전체에서 Node 상태가 Ready가 아닌 비율이 20%이상일 경우 발생   조치 사항 해당 노드들의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-003     중요도 warning   알람 이름 K8SKubeletDown   지속 시간 1시간   발생 조건 클러스터 전체에서 3%이상의 kubelet 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 해당 노드의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-004     중요도 critical   알람 이름 K8SKubeletDown   지속 시간 1시간   발생 조건 클러스터 전체에서 10%이상의 kubelet 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 해당 노드들의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-005     중요도 warning   알람 이름 K8SKubeletTooManyPods   지속 시간 즉시   발생 조건 Node의 배치된 Pod의 수가 100개가 넘으면 발생.(제한값은 110)   조치 사항 제한값에 도달할 경우, 더 이상 Pod 생성이 안됨.\n다른 노드들의 상태도 같이 확인하여 여유가 없을 경우, 노드를 증설한다.     Node     알람 ID NOD-001     중요도 warning   알람 이름 NodeExporterDown   지속 시간 10분   발생 조건 NodeExporter 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 NodeExporter의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.       알람 ID NOD-002     중요도 critical   알람 이름 K8SNodeOutOfDisk   지속 시간 즉시   발생 조건 Node 상태가 OutOfDisk일 때 발생   조치 사항 해당 노드의 디스크를 증설한다.       알람 ID NOD-003     중요도 warning   알람 이름 K8SNodeMemoryPressure   지속 시간 즉시   발생 조건 Node 상태가 MemoryPressure일 때 발생   조치 사항 해당 노드의 메모리를 증설한다.       알람 ID NOD-004     중요도 warning   알람 이름 K8SNodeDiskPressure   지속 시간 즉시   발생 조건 Node 상태가 DiskPressure일 때 발생   조치 사항 노드에서 로그, 미사용 dodkcer image, pv backup등을 삭제하여 디스크 공간을 확보한다.\n계속 발생할 경우, 해당 노드의 디스크를 증설한다.       알람 ID NOD-005     중요도 warning   알람 이름 NodeCPUUsage   지속 시간 30분   발생 조건 Node 최근 5분간 평균 CPU 사용량이 90%를 넘을 경우 발생   조치 사항 해당 노드의 CPU를 증설한다.       알람 ID NOD-006     중요도 warning   알람 이름 NodeMemoryUsage   지속 시간 30분   발생 조건 Node Memory 사용량이 90%를 넘을 경우 발생   조치 사항 해당 노드의 메모리를 증설한다.     Prometheus     알람 ID PRM-001     중요도 warning   알람 이름 PrometheusFailedReload   지속 시간 10분   발생 조건 Prometheus의 설정 변경시, 설정 다시읽기 작업 실패시 발생   조치 사항 해당 Pod의 로그를 확인하여 ConfigMap의 설정 오류를 수정한다.     System     알람 ID CKT-001     중요도 warning   알람 이름 PvLowRequestDisk   지속 시간 30분   발생 조건 PV가 요청한 디스크의 크기 대비 사용량이 80%가 넘으면 발생   조치 사항 PV의 크기를 늘린다. 단, 서버를 재배포 해야함.       알람 ID CKT-002     중요도 warning   알람 이름 PvLowTotalDisk   지속 시간 30분   발생 조건 PV가 마운트된 디스크의 크기 대비 사용량이 80%가 넘으면 발생   조치 사항 마운트된 디스크의 상태를 확인하고 미사용 PV를 제거한다.\n필요하면 디스크를 증설한다.       알람 ID CKT-003     중요도 warning   알람 이름 PodCPULimitUsage   지속 시간 30분   발생 조건 Resource Limit 설정값 대비 CPU 사용율이 90%가 넘으면 발생   조치 사항 계속 발생할 경우, Deployment의 CPU Limit 값 변경       알람 ID CKT-004     중요도 warning   알람 이름 PodMemoryLimitUsage   지속 시간 30분   발생 조건 Resource Limit 설정값 대비 Memory 사용율이 90%가 넘으면 발생   조치 사항 계속 발생할 경우, Deployment의 Memory Limit 값 변경    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.6-metering/",
	"title": "7.6 미터링",
	"tags": [],
	"description": "",
	"content": " 사용 중인 클러스터별 미터링(요금)에 대한 모니터링 서비스를 제공한다.\n미터링 서비스는 Public Cloud 중 AWS와 GCP에만 적용되는 서비스이다.\na) 클러스터 → 클러스터 선택 → 미터링을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/7.cluster/7.7-registration/",
	"title": "7.7 등록정보",
	"tags": [],
	"description": "",
	"content": " 현재 클러스터에 대한 등록정보를 확인할 수 있다.\nAccount 인증정보는 보안을 위해 노출되지 않는다.\na) 클러스터 → 클러스터 선택 → 등록관리를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.1-cluster-kass-gke/",
	"title": "8.1.1.1 GKE 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " GKE 클러스터를 배포하는 방법을 알아봅니다. Google Kubernetes Engine(GKE)에 로그인 https://cloud.google.com/kubernetes-engine/ 에서 Google cloud에 로그인합니다.\n서비스 계정 만들기 1. IAM 및 관리자에서 Service Account 만들기.  서비스 계정에는 다음 역할이 필요합니다.\n project/viewer\n kubernetes-engine/admin\n service-account/user\n 역할 기반 액세스 제어(Identity and Access Management)를 사용하기 위한 필요 조건:\nrbac 생성시 사전 필수 요소 필요.\n 다음 명령어를 실행하여 Kubernetes에서 역할을 만들 수 있는 능력을 사용자에게 부여해야 합니다. [USER_ACCOUNT]는 사용자의 이메일 주소입니다.\n참조: https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control  kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin --user [USER_ACCOUNT]  or\nkubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $(gcloud config get-value account)   서비스 계졍 만들기를 선택합니다.\n참조: 서비스 계정으로 Cloud Platform 인증\n 서비스 계정 세부 정보\n 서비스 계정 이름:\n이 서비스 계정의 표시 이름입니다.\n 만들기를 선택합니다.\n  서비스 계정 권한(선택사항)\n 역할:\n이 사용 설명서에서는 편의상 소유자 권한을 부여합니다.  Private key 만들기\n CREATE KEY 선택합니다.\n CREATE 합니다.\n서비스 계정이 생성되었으면 서비스 계정의 사용자 인증 정보가 포함된 JSON 키 파일이 컴퓨터에 다운로드됩니다. 이 키 파일은 사용자 API에 인증을 수행하도록 애플리케이션을 구성하기 위해 사용됩니다.      GKE 클러스터 만들기 참조: 클러스터 아키텍처\n1. 프로젝트 만들기 참조: https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster\n 프로젝트 만들기를 선택합니다.자세히 알아보기\n Project Name:\n프로젝트 이름을 입력합니다.\n Preject ID\n프로젝트 ID는 프로젝트의 글로벌 고유 식별자입니다. 프로젝트를 만든 후에는 프로젝트 ID를 변경할 수 없습니다.\n프로젝트를 만들 때 또는 프로젝트 ID를 만들어야 하는 API를 활성화할 때 선택한 맞춤설정된 이름입니다. 삭제된 프로젝트의 프로젝트 ID는 재사용할 수 없습니다.  Location:\n상위 조직 또는 폴더를 선택 합니다.\n    1.1 Google Kubernetes Engine API가 사용 설정되었는지 확인합니다. **GOOGLE KUBERNETES ENGINEAPI 사용 설정됨** 을 확인 합니다. ![gke-create-api-lib-1](/cocktail-manual-3.3.0-public/images/assets/KR/3.1.1/GKE/gke-create-api-lib-1.png)   Cloud SDK가 설치되었는지 확인합니다.\n gcloud 명령줄 도구의 기본값을 설정합니다.\ngcloud 명령줄 도구에서 프로젝트 ID 및 Compute Engine 영역 옵션을 입력하는 시간을 절약하기 위해 기본값을 설정할 수 있습니다.\ngcloud config set project [PROJECT_ID] gcloud config set compute/zone us-central1-b  gcloud를 최신 버전으로 업데이트합니다.\ngcloud components update    2. VPC 네트워크 만들기  VPC 네트워크 만들기 선택\n 이름 :\nVPC에 고유 이름을 부여합니다\n 서브넷 생성:\n서브넷을 사용하면 Google Cloud 내에 자체 비공개 클라우드 토폴로지를 만들 수 있습니다. 각 지역에 서브넷을 만들려면 \u0026lsquo;자동\u0026rsquo;을 클릭하고, 서브넷을 직접 정의하려면 \u0026lsquo;맞춤설정\u0026rsquo;을 클릭하세요. 자세히 알아보기\n 이름:\n서브넷 고유 이름을 부여합니다\n Region:\n이 사용 설명서에서는 asia-northeast1(Tokyo)를 사용합니다.\n자세히 알아보기\n IP 주소 범위:\nCIDR 표기법으로 나타낸 이 서브넷의 주소 범위입니다. 표준 사설 VPC 네트워크 주소 범위(예: 10.0.0.0/9)를 사용하세요.\n자세히 알아보기\n 완료를 선택합니다.\n 위와 같은 방법으로 subnet을 추가 합니다.\n  만들기를 선택합니다.\n    3. 클러스터 템플릿 GCP 콘솔을 사용해서 새 클러스터를 만들 경우 사용 가능한 모든 클러스터 템플릿이 표시됩니다. 기본적으로 표준 템플릿이 선택됩니다.\n 다음 템플릿을 사용할 수 있습니다.\n 표준 클러스터:\n지속적 통합, 웹 제공, 백엔드용입니다. 추가 맞춤설정이 필요하거나 어떤 템플릿을 선택할지 확실하지 않은 경우에 선택하면 가장 적합합니다.\n 첫 번째 클러스터:\n덜 강력한 노드를 실행하고 자동 확장과 같은 일부 고급 기능을 사용하지 않는 작은 클러스터입니다.\n CPU 집중 애플리케이션:\n해당 노드가 표준 클러스터보다 더 강력한 다중 코어 CPU를 제공하는 클러스터입니다.\n 메모리 집중 애플리케이션:\n해당 노드가 일반적으로 강력한 다중 코어 CPU와 대용량 메모리를 제공하는 클러스터입니다.\n GPU 가속 컴퓨팅:\n기본 노드 풀이 덜 강력한 노드로 구성되었고 GPU 설정 노드 풀이 추가로 포함된 클러스터입니다. 자동 확장은 기본적으로 사용되지 않습니다.\n 고가용성:\n클러스터가 지역 클러스터로 구성되었고, 특정 지역의 각 영역에 클러스터 마스터가 제공됩니다. 자동 확장 및 유지관리 기간이 사용 설정됩니다.\n  Kubernetes 클러스터 만들기\n 표준 클러스터\n템플릿을 선택하거나 해당 작업 부하에 적합한 템플릿을 선택합니다.\n 필요한 경우 템플릿을 맞춤설정합니다. 다음 필드는 필수입니다.\n 이름:\n클러스터 이름을 선택합니다. 프로젝트 및 영역 내에서 고유해야 합니다.\n 위치 유형:\n클러스터의 모든 노드가 동일 영역에 있는지 아니면 지정된 지역의 모든 영역에 있을 수 있는지 여부입니다.\n 영역:\n위치 유형이 영역인 경우 클러스터를 만들 Compute Engine 컴퓨팅 영역입니다.\n이 사용 설명서에서는 asia-northeast1-a(Tokyo)를 사용합니다.\n 지역:\n기본적으로 클러스터는 개발자가 생성 시 지정하는 단일 컴퓨팅 영역에 클러스터 마스터와 해당 노드를 만듭니다. 지역 클러스터를 만들어서 클러스터의 가용성 및 복구성을 향상시킬 수 있습니다.\n자세히 알아보기\n  마스터 버전:\nkubernetes version을 선택합니다.\n 노드 풀:\n노드 풀은 클러스터에서 Kubernetes를 실행하는 별도의 인스턴스 그룹입니다.\n 노드 수:\n클러스터에 만들 노드 수입니다. 노드 및 리소스에 대해 사용 가능한 리소스 할당량이 있어야 합니다(예: 방화벽 경로).\n 머신 유형:\n인스턴스에 사용할 Compute Engine 머신 유형입니다. 각 머신 유형은 서로 다르게 청구됩니다. 기본 머신 유형은 n1-standard-1입니다. 머신 유형 가격 정보는 머신 유형 가격표를 참조하세요.\n 고급 수정:\n 이름:\n노드 풀 이름을 부여합니다.\n 부팅 디스크 크기(GB):\n이 사용 설명서에서는 10GB를 사용합니다.\n 보안\n 서비스 계정:\nCompute Engine default service account 를 선택 합니다.\nVM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 콘솔 메뉴의 권한을 사용하여 서비스 계정을 만들거나 기본 서비스 계정이 있으면 이 계정을 사용하세요. 자세히 알아보기  저장을 선택합니다.\n   고급옵션:\n 네트워킹\n VPC 네이티브:\nVPC 네이티브 사용 설정(별칭 IP 사용) 체크합니다.\n 네트워크:\n항목에서 생성한 VPC를 선택합니다.\n 노드 서브넷:\n항목에서 생성한 subnet을 선택합니다.\n    만들기를 선택합니다.\n  GCP 콘솔에서 클러스터를 만든 다음에는 해당 클러스터와 상호작용하도록 kubectl을 구성해야 합니다. 자세한 내용은 kubeconfig 항목 생성을 참조하세요.\n  4. 클러스터에 연결  Cloud Shell에서 실행:\n Kubernetes 클러스터 목록에서 생성된 클러스터 연결을 선택합니다.   kubectl 을 사용하려면:\n kubectl 구성이 선행되야 합니다.\nkubectl은 Kubernetes Engine에서 사용되는 클러스터 조정 시스템인 Kubernetes를 관리하기 위해 사용됩니다.\n gcloud를 사용하여 kubectl을 설치할 수 있습니다.\ngcloud components install kubectl   클러스터 연결 화면에서 명령줄을 복사 콘솔에서 실행합니다.\n 아래 명령어로 확인할 수 있습니다.\nkubectl get svc     NFS Sever 만들기 Google Compute Engine 영구 디스크를 사용하여 네트워크 파일 시스템 (NFS) 서버를 만들어 컨테이너에 마운트하는 것입니다.\n1. Create an instance  이름(Name):\n인스턴스 이름을 부여합니다.\n 지역(Region):\n지역은 리소스를 실행할 수 있는 특정 지리적 위치입니다.\n 영역(Zone):\n영역은 지역 내의 격리된 위치입니다. 영역은 사용할 수 있는 컴퓨팅 리소스와 데이터를 저장하고 사용할 위치를 결정합니다.\n 머신 유형(Machine type):\n맞춤설정을 클릭하여 코어, 메모리, CPU를 선택합니다.\n 부팅 디스크(Boot disk):\n각 인스턴스에는 부팅을 위한 디스크가 필요합니다. 이미지나 스냅샷을 선택하여 새 부팅 디스크를 생성하거나 기존 디스크를 인스턴스에 연결하세요.\n이 사용 설명서에서는 CentOS 7 사용합니다.\n ID 및 API 액세스(Identity and API access):\n생성한 서비스 계정을 선택합니다.\nVM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 사용할 서비스 계정과 허용할 API 액세스 수준을 선택하세요. 자세히 알아보기\n  Management, security, disks, networking, sole tenancy 확장 섹션을 활성화 해서 아래 설정을 합니다.\n 디스크(Disks) 설정\n Add new disk를 선택 합니다.\n 노드에 필요한 디스크 사양을 설정 합니다.    네트워크(Networking) 설정\n네트워크는 인스턴스에서 액세스할 수 있는 네트워크 트래픽을 결정합니다.\n Network tags(네트워크 태그):\n네트워크 태그를 할당하여 특정 VM 인스턴스에 방화벽 규칙을 적용합니다.\n Network interfaces(네트워크 인터페이스)\n Network(네트워크):\n목록에서 VPC network에서 생성한 VPC를 선택합니다.\n Subnetwork(하위 네트워크):\n목록에서 VPC network에서 생성한 subnet을 선택합니다.\n Primary internal IP(기본 내부 IP):\n임시의 경우 인스턴스를 다시 시작해도 내부 IP가 변경되지 않지만 인스턴스를 삭제하고 다시 만들면 내부 IP가 변경됩니다.\n\u0026lsquo;임시(자동)\u0026lsquo;를 선택하여 하위 네트워크 범위의 주소를 할당하거나 \u0026lsquo;임시(커스텀)\u0026lsquo;를 선택하여 직접 입력하세요.\n인스턴스를 삭제하고 다시 만들 때 IP를 유지하려면 고정 내부 IP 주소를 선택하거나 만드세요.\n자세히 알아보기\n done(완료)**를 선택합니다.\n   Create(만들기)를 선택합니다.\n  2. 생성된 인스턴스(노드) 방화벽 설정(SSH / NFS).  인스턴스 화면에서 생성한 인스턴스의 확장 메뉴에서 View network details를 선택.\n또는 VPC network \u0026gt; Firewall rules를 선택한다.   SSH 방화벽 규칙을 추가 한다.\n Name:\n예) cocktail-test-gke-demo-storage-ssh\n Network:\n인스턴스에서 설정한 VPC를 선택합니다.\n Targets(대상):\nSpecified target tags를 선택.\n가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.\n Target tags:\nNFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.\n Source filter:\nIP ranges를 선택.\n필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.\n Source IP ranges:\n접근하고자 하는 IP ranges를 CIDR 표기법으로 추가 합니다.\n예)10.0.10.0/24 192.168.100.5\u0026frasl;32\n Protocols and ports:\n허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.\n Specified protocols and ports\n tcp: 22      NFS 방화벽 규칙을 추가 한다.\n Name:\n예) cocktail-test-gke-demo-storage-nfs\n Network:\n인스턴스에서 설정한 VPC를 선택합니다.\n Targets(대상):\nSpecified target tags를 선택.\n가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.\n Target tags:\nNFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.\n Source filter:\nSubnets를 선택.\n필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.\n Subnets:\n이 소스 하위 네트워크의 트래픽만 허용됩니다.\n Protocols and ports:\n허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.\n Allow all 선택\n이 사용 설명서에서는 편의상 Alloww all을 선택합니다.      3. 생성된 인스턴스(노드)에 SSH 연결.  인스턴스 화면에서 생성된 인스턴스(노드)의 SSH항목을 선택 -\u0026gt; View gcloud command를 선택 한다(자동생성).\n Run IN CLOUD SHELL을 실행 한다.\n RSA key pair 가 자동생성 된다.\n아래명령으로 생성된 key pair 을 확인할 수 있다.\ncd ~/.ssh  RSA key pair GCP의 Compute Engine \u0026gt; Metadata \u0026gt; SSH Keys에 자동 등록 된다.\n아래 그림처럼 등록된 SSH Key를 확인 및 편집할 수 있다.\n    또는 RSA key pair 수동 생성 / 등록 방법.\n 아래 명령으로 RSA key pair를 생성한다.\nssh-keygen -t rsa -f ./rsa-gcp-key -C\u0026quot;\u0026lt;Your-email.com\u0026gt;\u0026quot;    생성된 RSA Key 내용을 복사 해서 Compute Engine \u0026gt; Metadata \u0026gt; SSH Keys 에 등록 한다.\ncat ./rsa-gcp-key.pub     4. 생성된 인스턴스(노드)에 NFS 서버 설치 / 구성  노드 OS 확인.\nsudo grep . /etc/*-release  노드 NFS 패키지 확인.\nsudo rpm -qa | grep nfs  노드 NFS 패키지 설치.\nsudo yum install nfs-utils nfs-utils-lib  클러스터(마스터)에서 NFS에 사용될 공유 디렉토리를 생성 한다.\nprovisioner Deployment에서 NFS_PATH에 지정된 디렉토리를 생성한다.\nsudo mkdir /storage/shared  공유 디렉토리 퍼미션 설정.\nsudo chmod -R 777 /storage  공유 디렉토리 NFS 권한을 부여하고, 동기화를 한다.\n인스턴스(노드)에서 설정한 Subnet 또는 Ip ranges를 부여한다.\n# sudo vi /etc/exports /storage/shared 10.0.10.0/24(rw,sync)  NFS service를 재시작 한다.\nsudo service nfs restart   5. 클러스터(마스터)에 NFS-Client Provisioner 구성. 참조: Kubernetes NFS-Client Provisioner\n Setup authorization:\n클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.\n kubectl apply -f 1-rbac.yaml\nNamespace, ServiceAccount, Setup authorization 적용  apiVersion: v1 kind: Namespace metadata: name: cocktail-addon --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: cocktail-addon --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;] verbs: [\u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: cocktail-addon roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: cocktail-addon roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io  Configure the NFS-Client provisioner\n kubectl apply -f 2-shared-storage-provisioner-dp.yaml\nDeployment, 다음 프로비저 배포 파일을 편집하여 NFS 서버에 대한 연결 정보를 추가해야합니다.  kind: Deployment apiVersion: apps/v1 metadata: name: shared-storage-provisioner namespace: cocktail-addon spec: replicas: 1 selector: matchLabels: acornsoft.io/provisioner-type: NFSDYNAMIC app: shared-storage-provisioner strategy: type: Recreate template: metadata: labels: app: shared-storage-provisioner acornsoft.io/provisioner-type: NFSDYNAMIC spec: serviceAccount: nfs-client-provisioner containers: - name: shared-storage-provisioner image: quay.io/external_storage/nfs-client-provisioner:v2.0.1 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME # YOUR PROVISIONER_NAME value: acornsoft.io/shared-storage-provisioner - name: NFS_SERVER # YOUR NFS SERVER HOSTNAME value: 10.0.10.8 - name: NFS_PATH value: /storage/shared resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: nfs-client-root nfs: # YOUR NFS SERVER HOSTNAME server: 10.0.10.8 path: /storage/shared  Storage class 등록\n참조: https://kubernetes.io/docs/concepts/storage/storage-classes/\n kubectl apply -f 3-single-sc.yaml\nsingle-storage : Persistent Volumes with Kubernetes on GKE  --- # single-storage : Persistent Volumes with Kubernetes on GKE apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: single-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: GCE acornsoft.io/type: SINGLE addonmanager.kubernetes.io/mode: EnsureExists kubernetes.io/cluster-service: \u0026quot;true\u0026quot; parameters: type: pd-standard provisioner: kubernetes.io/gce-pd reclaimPolicy: Delete   kubectl apply -f 4-shared-sc.yaml\nshared-storage : NFS Persistent Volumes with Kubernetes on GKE  --- # shared-storage : NFS Persistent Volumes with Kubernetes on GKE apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: shared-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: NFSDYNAMIC acornsoft.io/total-capacity: \u0026quot;100\u0026quot; acornsoft.io/type: SHARED provisioner: acornsoft.io/shared-storage-provisioner reclaimPolicy: Delete allowVolumeExpansion: true   6. 샘플 PVC 등록 / 확인.  아래 명령으로 샘플을 구성할 수 있다.\n kubectl apply -f 5-test-pod.yaml\n임시 파일 생성 pod를 생성 한다.  --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: storageClassName: shared-storage accessModes: - ReadWriteMany resources: requests: storage: 1Mi --- kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - \u0026quot;/bin/sh\u0026quot; args: - \u0026quot;-c\u0026quot; - \u0026quot;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026quot; volumeMounts: - name: nfs-test-pvc mountPath: \u0026quot;/mnt\u0026quot; restartPolicy: \u0026quot;Never\u0026quot; volumes: - name: nfs-test-pvc persistentVolumeClaim: claimName: nfs-pvc   kubectl get pv\n생성된 PV를 확인 한다.\n NFS 노드에 생성된 임시 파일을 확인할 수 있다.\n   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.2-cluster-kass-aks/",
	"title": "8.1.1.2 AKS 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " AKS 클러스터를 배포하는 방법을 알아봅니다. 사용 설명서 : Azure Portal을 사용하여 AKS(Azure Kubernetes Service) 클러스터 배포\nAzure에 로그인 https://portal.azure.com 에서 Azure Portal에 로그인합니다.\nAKS 클러스터 만들기 - Create a resource Azure Portal의 화면 왼쪽 상단 + Create a resource 선택 다음 \u0026gt; Kubernetes Service를 선택 합니다.\n1. Basics - 다음 옵션을 구성 합니다. Azure Kubernetes Service (AKS)는 호스팅 된 Kubernetes 환경을 관리하므로 컨테이너 오케스트레이션 전문 지식이 없어도 컨테이너 화 된 응용 프로그램을 빠르고 쉽게 배포 및 관리 할 수 있습니다.\n또한 응용 프로그램을 오프라인으로 설정하지 않고도 필요에 따라 리소스를 프로비저닝, 업그레이드 및 확장하여 진행중인 운영 및 유지 관리의 부담을 없애줍니다. Learn more about AKS\n PROJECT DETAILS\n Subscription :\nMicrosoft Azure 선택.\nAzure 서비스에 대한 엑세스 권한을 부여합니다. 자원 사용이 보고되고 서비스에 비용이 청구되는 방법이기도 합니다.\n ResourceGroup :\nmyResourceGroup 같은 Azure 리소스 그룹을 선택하거나 만듭니다.\n리소스 그룹은 동일한 수명주기, 사용 권한 및 정책을 공유하는 리소스 모음입니다.\n  CLUSTER DETAILS\n Kubernetes cluster name :\nmyAKSCluster 같은 Azure Kubernetes 서비스 클러스터 이름을 입력합니다.\n Region :\n클러스터가 배포되어야 하는 Azure 영역입니다. 이 사용 설명서에서는 Japan East를 선택 합니다..\n Kubernetes version :\n이 클러스터에 사용해야 하는 Kubernetes의 버전. 클러스터를 만든 후에는 버전을 업그레이드 할 수 있습니다.\nview the Kubernetes changelog\n DNS name prefix :\n호스팅 된 Kubernetes API 서버 FQDN과 함께 사용 할 DNS 이름 접두사.\n클러스터를 생성 한 후 컨테이너를 관리 할 때 Kubernetes API에 연결하기 위해 이를 사용 할 것입니다.\n  SCALE\n클러스터의 노드 수와 크기 입니다. Production workloads의 경우 복구를 위해 노드를 3 개 이상 권장합니다. 개발 또는 테스트 작업 부하의 경우 1개의 노드 만 필요합니다.\n클러스터를 만든 후에 노드 크기를 변경할 수는 없지만 만든 후에는 클러스터의 노드 수 를 변경할 수 있습니다.\nLearn more about scaling in Azure Kubernetes Service\n Node size :\n클러스터에서 노드를 형성 할 가상 시스템의 크기. 클러스터를 만든 후에는 변경할 수 없습니다.\n이 사용 설명서에서는 Standard DS2 v2 : 2 vcpus, 7 GB memory 를 사용 합니다.\n Node count :\n클러스터와 함께 생성되어야하는 노드의 수. 나중에 클러스터의 노드 수 를 조정할 수 있습니다\n이 사용 설명서에서는 노드 수를 1 로 설정합니다.\n Virtual nodes (preview) :\n가상 노드를 사용하면 서버가없는 Azure 컨테이너 인스턴스가 지원하는 노드에 컨테이너를 배포하거나 버스트 아웃 할 수 있습니다.\n이렇게하면 정의 된 클러스터 크기를 초과하는 빠른 버스트 확장 옵션을 제공 할 수 있습니다. Learn more\n   2. Authentication - 다음 옵션을 구성 합니다. 클러스터 인프라 서비스 원칙은 Kubernetes 클러스터에서 클러스터에 연결된 클라우드 리소스를 관리하는 데 사용됩니다. Learn more\nKubernetes 인증 및 권한 부여 는 Kubernetes 클러스터에서 클러스터에 대한 사용자 액세스를 제어하고 사용자가 인증을 받으면 수행 할 수있는 작업을 제어하는 ​​데 사용됩니다. Learn more\n CLUSTER INFRASTRUCTURE\n Service principal :\n기존 서비스 주체의 클라이언트 ID입니다.\n이 사용 설명서 에서는 (new) default service principal 를 선택하여 자동 생성 합니다.\n클러스터가 생성되 후 Properties 화면 INFRASTRUCTURE RESOURCE GROUP 에서 확인 할 수 있습니다.\n  KUBERNETES AUTHENTICATION AND AUTHORIZATION\n클러스터 리소스에 대한 세분화 된 제어를 제공하는 Kubernetes 역할 기반 액세스 제어를 사용합니다.\n Enable RBAC :\n이러한 컨트롤을 사용하면 AKS 클러스터에 배포된 Kubernetes 리소스에 대한 액세스를 정밀하게 제어할 수 있습니다.\n이 사용 설명서에서는 Kubernetes RBAC(역할 기반 액세스 제어)에 대한 옵션을 사용하도록 설정합니다.\n   3. Networking - 다음 옵션을 구성 합니다. HTTP 응용 프로그램 라우팅을 활성화하고 \u0026ldquo;기본\u0026rdquo;또는 \u0026ldquo;고급\u0026rdquo;의 두 가지 네트워킹 옵션 중에서 선택할 수 있습니다.\n* Basic 네트워킹은 기본값을 사용하여 클러스터에 새로운 VNet을 생성합니다.\n* Advanced 네트워킹을 통해 클러스터는 사용자 정의 가능한 주소로 새 VNet 또는 기존 VNet을 사용할 수 있습니다.\n응용 프로그램 포드는 VNet에 직접 연결되어있어 VNet 기능과의 기본 통합이 가능합니다.\nLearn more about networking in Azure Kubernetes Service\n HTTP application routing :\nHTTP application routing 솔루션을 사용하면 application endpoints 대해 공개적으로 액세스 할 수있는 DNS 이름을 만들어 클러스터에 배포 된 응용 프로그램에 쉽게 액세스 할 수 있습니다. Learn more about HTTP application routing\n이 사용 설명서에서는 NO 를 선택 합니다.\n Network configuration :\n기본 VNet과 함께 kubenet을 사용하는 Basic 네트워크 구성을 선택하거나\nAzure CNI를 사용하여 VNet을 사용자 지정하는 옵션과 함께 Advanced 구성을 선택하십시오.\n이 사용 설명서에서는 Basic 을 선택 합니다.\n  4. Monitoring - 다음 옵션을 구성 합니다. Azure Kubernetes Service를 사용하면 각 노드에 대한 CPU 및 메모리 사용량 메트릭을 얻을 수 있습니다.\n또한 컨테이너 모니터링 기능을 활성화하고 전체 Kubernetes 클러스터의 성능 및 상태에 대한 통찰력을 얻을 수 있습니다.\n처리 된 데이터의 양과 데이터 보존 설정에 따라 요금이 청구됩니다.\nLearn more about container performance and health monitoring\nLearn more about pricing\n AZURE MONITOR\n Enabe container monitoring :\n이 사용 설명서에서는 Yes를 선택 합니다.\n Log Analytics workspace :\n모니터링 데이터를 저장할 로그 분석 워크 스페이스 선택.\n이 사용 설명서에서는 Create new 를 선택해서 Japan East Region을 선택하고 생성 합니다.\n   5. Tags - 다음 옵션을 구성 합니다. 태그는 이름 / 값 쌍으로, 리소스를 분류하고 동일한 태그를 여러 자원 및 자원 그룹에 적용하여 통합 청구서를 볼 수 있습니다. Learn more\n태그를 만들고 다른 탭에서 리소스 설정을 변경하면 태그가 자동으로 업데이트됩니다.\n이 사용 설명서에서는 만들지 않습니다.\n6. Review + create - 검토 + 만들기를 구성 합니다.  검토 + 만들기를 선택하고, 완료되면 만들기를 선택 합니다.\n   AKS 클러스터를 만들고 사용 준비를 마칠 때까지 몇 분 정도 걸립니다.\n  7. 생성된 Resource groups 을 학인 완료되면 리소스 그룹에서 생성한 이름으로 리소스 그룹이 생성 됩니다. 생성된 클러스터를 확인 할 수 있습니다.\n 왼쪽 Resource groups 를 선택 후. 생성한 클러스트의 리소스 그룹을 선택 합니다.\n   생성한 클러스터의 Vitual machine 을 선택 합니다.  참고 사항 : Tags 를 변경 할 수 있습니다.    생성한 클러스터의 Vitual machine의 모니터링을 확인 할 수 있습니다.  클러스터에 연결 1. Kubernetes 클러스터를 관리하려면 - Cloud Shell 사용  Azure Portal 오른쪽 상단에 있는 \u0026gt;_ Cloud Shell을 엽니다.\n kubectl Kubernetes 명령줄 클라이언트를 사용합니다. kubectl 클라이언트가 Azure Cloud Shell에 사전 설치됩니다.\n  2. Kubernetes 클러스터에 연결하도록 kubectl을 구성하려면 - Cloud Shell 사용  az aks get-credentials 명령을 사용하여 kubectl 을 구성 합니다.\n이 명령은 자격 증명을 다운로드하고 Kubernetes CLI가 해당 자격 증명을 사용하도록 구성 합니다.\n다음 예제는 cocktail 이라는 리소스 그룹에서 cocktail-test-aks 클러스터의 자격 증명을 가져옵니다.\naz aks get-credentials --resource-group cocktail --name cocktail-test-aks  클러스터에 대한 연결을 확인하려면 kubectl get 명령을 사용하여 클러스터 노드의 목록(Computer name) 을 반환합니다.\nkubectl get nodes   AZURE Storage Accounts 만들기 Storage accounts 1. 왼쪽 메뉴 또는 Home 화면에서 Storage accounts 를 선택 합니다. 2. Storage accounts +Add 를 선택 합니다. Create storage account - 만들기 1. Basics - 다음 옵션을 구성 합니다. Azure 저장소는 가용성, 보안 성, 내구성, 확장 성 및 중복성이 뛰어난 클라우드 저장소를 제공하는 Microsoft 관리 서비스입니다.\nAzure 저장소에는 Azure Blob (개체), Azure Data Lake 저장소 Gen2, Azure 파일, Azure 대기열 및 Azure 테이블이 포함됩니다.\n스토리지 계정 비용은 아래에서 선택한 사용법 및 옵션에 따라 다릅니다. Learn more\n PROJECT DETAILS\n배포 된 리소스 및 비용을 관리하려면 Microsoft Azure 를 선택하십시오.\n폴더와 같은 리소스 그룹을 사용하여 모든 리소스를 구성하고 관리하십시오.\n Subscription :\nMicrosoft Azure 를 선택.\n Resource group :\n생성한 클러스터의 INFRASTRUCTURE RESOURCE GROUP 을 선택 합니다.\n  INSTANCE DETAILS\n기본 배포 모델은 최신 Azure 기능을 지원하는 Resource Manager입니다.\n대신 클래식 배포 모델을 사용하여 배포하도록 선택할 수 있습니다.\n Storage account name :\n이 이름은 Azure의 모든 기존 스토리지 계정 이름에서 고유해야합니다. 3 ~ 24 자 여야하며 소문자와 숫자 만 사용할 수 있습니다.\n Location :\n연결 할 클러스터와 같은 위치를 선택 합니다(권장).\n Performance :\nStandard(표준 스토리지) 계정은 마그네틱 드라이브로 백업되며 GB 당 최저 비용을 제공합니다. 대용량 저장 장치가 필요한 응용 프로그램이나 데이터 액세스 빈도가 낮은 응용 프로그램에 가장 적합합니다.\nPremium(프리미엄 스토리지) 계정은 솔리드 스테이트 드라이브를 기반으로하며 일관되고 낮은 대기 시간의 성능을 제공합니다. Azure 가상 머신 디스크에서만 사용할 수 있으며 데이터베이스와 같은 I / O 집약적 인 애플리케이션에 가장 적합합니다. 또한 모든 디스크에 Premium 스토리지를 사용하는 가상 시스템은 가용성 세트를 벗어나 실행하는 경우에도 99.9 % SLA를 사용할 수 있습니다. Learn more\n이 설정은 스토리지 계정을 만든 후에는 변경할 수 없습니다.\n Account kind :\n범용 V1, 범용V2(권장) 스토리지 계정은 통합 계정에서 blobs, 파일, 테이블 및 대기열에 대한 스토리지를 제공합니다.\nBlob 저장소 계정은 BLOB 데이터를 저장하고 액세스 계층을 선택하는 데 특화되어 있어 계정의 엑세스 빈도를 지정 할 수 있습니다.\n스토리지 요구사항과 일치하고 비용을 최저화 하는 계층을 선택하십시오. learn more\n Replication :\nAzure 스토리지 계정의 데이터는 항상 내구성과 고 가용성을 보장하기 위해 복제됩니다.\n내구성 요구 사항에 맞는 복제 전략을 선택하십시오.\n스토리지 계정을 만든 후에는 일부 설정을 별경 할 수 없습니다. learn more\n Access tier (default) :\n계정 액세스 계층은 명시 적으로 설정된 계층이없는 모든 BLOB에서 유추되는 기본 계층입니다.\nHot Access Tier는 자주 액세스하는 데이터에 이상적이며\nCool Access Tier는 자주 액세스하지 않는 데이터에 이상적입니다.\n아카이브 액세스 계층은 계정이 아닌 BLOB 수준에서만 설정할 수 있습니다. learn more\n   2. Advanced - 다음 옵션을 구성 합니다.  SECURITY\n Secure transfer required :\n보안 전송 옵션은 보안 연결을 통해 저장소 계정에 대한 요청 만 허용하여 저장소 계정의 보안을 강화합니다.\n예를 들어 저장소 계정에 액세스하기 위해 REST API를 호출 할 때는 HTTP를 사용하여 연결해야합니다.\nEnabled(보안 전송 필요)가 사용 설정된 경우 HTTP를 사용하는 모든 요청이 거부됩니다.\nAzure 파일 서비스를 사용하는 경우 SMB 2.1, 암호화없는 SMB 3.0 및 Linux SMB 클라이언트의 일부 버전을 비롯한 암호화없는 연결은 실패합니다.\nAzure 저장소는 사용자 정의 도메인 이름에 대한 HTTP를 지원하지 않으므로 사용자 정의 도메인 이름을 사용할 때는이 옵션이 적용되지 않습니다.\n이 사용 설명서에서는 Enabled 선택 합니다.  VIRTUAL NETWORKS\n Allow access from :\n모든 네트워크에서이 저장소 계정에 액세스 할 수 있습니다. Learn more\n이 사용 설명서에서는 All networks 선택 합니다.   DATA LAKE STORAGE GEN2 (PREVIEW)\n Hierarchical namespace :\nADLS Gen2 계층 적 네임 스페이스는 대규모 데이터 분석 작업을 가속화하고 파일 수준 ACL (액세스 제어 목록)을 활성화합니다. Learn more\n이 사용 설명서에서는 Disabled 선택 합니다.   3. Tags - 다음 옵션을 구성 합니다. 태그는 이름 / 값 쌍으로, 리소스를 분류하고 동일한 태그를 여러 자원 및 자원 그룹에 적용하여 통합 청구서를 볼 수 있습니다. Learn more\n태그를 만들고 다른 탭에서 리소스 설정을 변경하면 태그가 자동으로 업데이트됩니다.\n4. Review + create - 검토 + 만들기를 구성 합니다.  Review + create를 선택하고, 완료되면 Create를 선택 합니다.  4. 생성된 Storage Account 확인  완료된 화면 에서 Go to resource를 선택 하거나 , Resource groups 에서 추가되 Storage account를 확인 할 수 있습니다.  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.3-cluster-kass-eks/",
	"title": "8.1.1.3 EKS 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " 8.1.1.3 EKS 클러스터 만들기 Quick Start EKS 클러스터를 배포하는 방법. Kubernetes 용 Amazon Elastic Container Service (Amazon EKS)를 사용하면 AWS의 Kubernetes를 사용하여 컨테이너 화 된 응용 프로그램을 쉽게 배포, 관리 및 확장 할 수 있다.\n참조 : Amazon Elastic Container Service for Kubernetes(EKS) 클러스터 배포\nAmazon Eks에 로그인  Amazon Eks에 로그인.  Amazon EKS 필수 조건  Amazon EKS 클러스터를 생성하기 전에 Kubernetes가 AWS 리소스 생성 시 수임할 수 있는 IAM 역할을 생성해야 한다.\n AWS 관리자 권한이 없다면 필요한 역할 / 정책 / 권한 을 요청 해야 합니다.\n참조 : EKS 관리 권한 / 정책\n  AWS EKS 클러스터 만들기 Kubernetes 용 Amazon Elastic Container Service (Amazon EKS)는 AWS에서 Kubernetes를 쉽게 실행할 수 있도록 해주는 관리 형 서비스입니다.\n1. AWS 리소스 생성 시 수임할 수 있는 IAM 역할을 생성 합니다. 관리자 권한 또는 해당 IAM 권한이 필요 합니다.\n [Roles(역할)]를 선택한 다음 [Create role]을 선택합니다.\n 서비스 목록에서 EKS를 선택합니다.\n 사용 사례에 대해 Allows Amazon EKS to manage your clusters on your behalf(EKS에서 사용자를 대신하여 클러스터를 관리하도록 허용)를 선택합니다.\n  [Next: Permissions(다음: 권한)]를 선택합니다.\n AmazonEKSClusterPolicy :\n이 정책은 Kubernetes에게 사용자를 대신하여 리소스를 관리하는 데 필요한 권한을 제공합니다.\nKubernetes는 인스턴스, 보안 그룹 및 탄력적인 네트워크 인터페이스를 포함하되 이에 국한되지 않는 EC2 리소스에 대한 식별 정보를 배치하는 데 Ec2 : CreateTags 권한이 필요합니다.\n AmazonEKSServicePolicy :\n이 정책에 따라 Kubernetes 용 Amazon Elastic Container Service는 EKS 클러스터를 운영하는 데 필요한 리소스를 만들고 관리 할 수 있습니다.\n  [Next: Tags(다음: 태그)]를 선택합니다.\nIAM 태그는 사용자 역할에 추가할 수 있는 키-값 페어입니다.\n태그는 이메일 주소와 같은 사용자 정보를 포함하거나 직책과 같은 내용일 수 있습니다. 태그를 사용하여 이 역할에 대한 액세스를 구성, 추적 또는 제어할 수 있습니다. Learn more\n [Next: Review(다음: 검토)]를 선택합니다.\n생성하기 전에 아래에 필요한 정보를 입력하고 이 역할을 검토하십시오.\n Role name(역할 이름)에서 역할에 대한 고유 이름(예: eksServiceRole)을 입력합니다.\n  Create role(역할 생성 만들기)을 선택합니다.\n 만들어진 역할을 확인 합니다.\n [Roles(역할)]를 선택 :\n목록에서 새로 생성한 Role(역할)을 확인할 수 있습니다.   2. Amazon EKS 클러스터 VPC 생성 - CloudFormation 콘솔 사용해서 생성 참조 : AWS 사용 설명서를 참조.\n https://console.aws.amazon.com/cloudformation에서 AWS CloudFormation 콘솔에서 제공 되는 템플릿을 사용해서 생성.\n 상단 리전 모음에서 Amazon EKS를 지원하는 리전을 선택.\n참고\n현재 다음 리전에서 Amazon EKS를 사용할 수 있습니다. (한국어 사용 설명서는 아직 반영이 안되어 있다.)\n US West (Oregon) (us-west-2)\n US East (N. Virginia) (us-east-1)\n US East (Ohio) (us-east-2)\n EU (Frankfurt) (eu-central-1)\n EU (Stockholm) (eu-north-1)\n EU (Ireland) (eu-west-1)\n Asia Pacific (Tokyo) (ap-northeast-1)\n Asia Pacific (Seoul) (ap-northeast-2)\n Asia Pacific (Singapore) (ap-southeast-1)\n Asia Pacific (Sydney) (ap-southeast-2)\n  [Create stack]을 선택.\n 템플릿 선택에서 Amazon S3 템플릿 URL 지정을 선택.\nhttps://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml  주의 : 2019-01-09 이전 날짜의 템플릿파일을 서울리전에서 사용시 오류가 있다. (AZ가 없는 관계로)\n 스택 이름: AWS CloudFormation 스택에 대한 스택 이름을 선택합니다. 예를 들어 eks-vpc로 사용할 수 있다.\n VpcBlock: VPC에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet01Block: 서브넷 1에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet02Block: 서브넷 2에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet03Block: 서브넷 3에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n  (선택 사항) 옵션 페이지에서 스택 리소스에 태그를 지정합니다. [Next]를 선택.\n [Review] 페이지에서 [Create]을 선택합니다.\n 스택이 생성된 후 콘솔에서 이를 선택하고 출력을 선택합니다.\n 생성된 보안 그룹에 대한 SecurityGroups 값을 기록합니다. 이 값은 EKS 클러스터를 생성할 때 필요합니다. 이 보안 그룹은 서브넷에 생성된 계정 간 탄력적 네트워크 인터페이스에 적용되고, 이를 통해 Amazon EKS 제어 플레인이 작업자 노드와 통신할 수 있습니다.\n 생성된 VPC의 VpcId를 기록합니다. 작업자 노드 그룹 템플릿을 시작할 때 필요합니다.\n 생성된 서브넷에 대한 SubnetIds를 기록합니다. EKS 클러스터를 생성할 때 필요합니다. 작업자 노드가 시작되는 서브넷입니다.\n  3. Amazon EKS에 대한 kubectl을 설치합니다. Amazon EKS는 IAM을 사용하여 Kubernetes용 AWS IAM Authenticator를 통해 Kubernetes 클러스터에 인증을 제공합니다.\nKubernetes 버전 1.10부터 Kubernetes용 AWS IAM Authenticator를 설치하고 인증에 사용할 kubectl 구성 파일을 사용하여 Amazon EKS를 사용할 stock kubectl 클라이언트를 구성할 수 있습니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\n참조: kubectl 설치 : https://kubernetes.io/docs/tasks/tools/install-kubectl/\n Kubernetes 용 AWS IAM Authenticator는 AWS에서 Kubernetes 설치 프로그램을 작성하는 경우 부트 스트랩 프로세스를 간소화 할 수 있습니다.\n 새로 설치 한 클러스터에서 초기 admin 자격 증명을 어떻게 든 안전하게 훔쳐 낼 필요가 없습니다.\n 대신 클러스터 구축시 전용 KubernetesAdmin 역할을 만들고 Authenticator를 설정하여 클러스터 관리자 로그인을 허용 할 수 있습니다.\n  3-1. Amazon EKS용 aws-iam-authenticator를 설치합니다. Kubernetes 클러스터에 인증하기 위해 AWS IAM 자격 증명을 사용하는 도구 입니다. Kubernetes 용 AWS IAM Authenticator를 사용하면 Kubernetes 액세스에 대한 별도의 자격 증명을 관리하지 않아도됩니다.\n aws-iam-authenticator 바이너리를 다운로드 및 설치합니다.\nAmazon EKS는 사용할 수 있는 aws-iam-authenticator 바이너리를 판매합니다.\n또는 go get 을 사용하여 GitHub의 Kubernetes용 AWS IAM Authenticator 프로젝트에서 다른 운영 체제용 바이너리를 가져올 수 있습니다.\n 1. aws-iam-authenticator 바이너리를 다운로드 및 설치하려면 다음과 같이 합니다.\n Amazon S3에서 aws-iam-authenticator 바이너리를 다운로드합니다.\n Linux: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/linux/amd64/aws-iam-authenticator\n MacOS: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/darwin/amd64/aws-iam-authenticator\n Windows: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/windows/amd64/aws-iam-authenticator.exe\n아래 명령을 사용하여 바이너리를 다운로드하고 플랫폼에 맞는 올바른 URL로 교체합니다. 아래 예는 MacOS 클라이언트에 해당됩니다.\ncurl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/darwin/amd64/aws-iam-authenticator  바이너리에 실행 권한을 적용합니다.\nchmod +x ./aws-iam-authenticator  바이너리를 $PATH의 폴더에 복사합니다. $HOME/bin/aws-iam-authenticator를 생성하고 $PATH가 $HOME/bin로 시작하는 것이 좋습니다.\ncp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH  PATH 환경 변수에 $HOME/bin을 추가합니다.\n MacOS의 Bash 셸의 경우:\necho 'export PATH=$HOME/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile  Linux의 Bash 셸의 경우:\necho 'export PATH=$HOME/bin:$PATH' \u0026gt;\u0026gt; ~/.bashrc   aws-iam-authenticator 바이너리가 작동하는지 테스트합니다.\naws-iam-authenticator help  \u0026lt;!\u0026ndash;\n  2. 또는 go get을 사용하여 GitHub에서 aws-iam-authenticator 바이너리를 설치합니다.\n go가 이미 설치되지 않은 경우 운영 체제에 대해 Go 프로그래밍 언어를 설치합니다. 자세한 내용은 Go 문서의 Install the Go tools를 참조하십시오.\n go get을 사용하여 aws-iam-authenticator 바이너리를 설치합니다.\ngo get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator  PATH 환경 변수에 $HOME/go/bin을 추가합니다.\n MacOS의 Bash 셸의 경우:\nexport PATH=$HOME/go/bin:$PATH \u0026amp;\u0026amp; echo 'export PATH=$HOME/go/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile  Linux의 Bash 셸의 경우:\nexport PATH=$HOME/go/bin:$PATH \u0026amp;\u0026amp; echo 'export PATH=$HOME/go/bin:$PATH' \u0026gt;\u0026gt; ~/.bashrc   aws-iam-authenticator 바이너리가 작동하는지 테스트합니다.\naws-iam-authenticator help  \u0026ndash;\u0026gt;\n   3-2. (선택 사항) 최신 AWS CLI 다운로드 및 설치 합니다. AWS CLI가 Amazon EKS 사용을 명시적으로 요구하지는 않지만, update-kubeconfig 명령을 사용하면 kubeconfig 생성 프로세스가 크게 간소화됩니다. AWS CLI에서 Amazon EKS를 사용하려면 1.16.18 버전 이상의 AWS CLI가 설치되어 있어야 합니다.\nAWS CLI를 설치 또는 업그레이드하려면 AWS Command Line Interface 사용 설명서의 AWS 명령줄 인터페이스 설치를 참조하십시오.\n AWS CLI 버전은 다음 명령을 통해 확인할 수 있습니다.\naws --version   4. 1단계: Amazon EKS 클러스터 생성 이제 Amazon EKS 클러스터를 생성할 수 있습니다.\n참조 : Amazon EKS 클러스터 생성 사용 설명서를 참고 합니다.\n 사전 요구 사항은 다음과 같습니다.\n Amazon EKS 클러스터의 요건을 충족하는 VPC 및 전용 보안 그룹을 생성했습니다.\n자세한 내용은 클러스터 VPC 고려 사항 및 클러스터 보안 그룹 고려 사항 단원을 참조하십시오\n Amazon EKS 서비스 역할을 생성하여 클러스터에 적용했습니다. Amazon EKS 서비스 IAM 역할 가이드를 참조 하시기 바랍니다.\n    AWS EKS 콘솔을 사용하여 클러스터를 생성 합니다.\n 1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n 2. [Create cluster]를 선택합니다.\n 참고\nIAM 사용자에게 관리 권한이 없는 경우 해당 사용자가 Amazon EKS API 작업을 호출하는 권한을 명시적으로 추가해야 합니다.\n자세한 내용은 Amazon EKS IAM 정책 만들기 단원을 참조하십시오.  3. 클러스터 생성 페이지에서 다음 필드를 입력한 다음 생성을 선택합니다.\n Create cluster\n Cluster name :\nAmazon EKS 클러스터에 대해 고유한 이름입니다.\n Kubernetes version :\n클러스터에 대해 사용할 Kubernetes 버전. 기본적으로 최신 버전이 선택됩니다.\n Role name :\nAmazon EKS 서비스 역할 생성 사용을 통해 생성한 IAM 역할을 선택합니다.\n  Networking\n VPC :\nAmazon EKS 클러스터 VPC 생성 사용을 통해 생성한 VPC를 선택합니다.\n드롭다운 목록에서 VPC의 이름을 찾을 수 있습니다.\n Subnets :\n기본적으로 위 VPC에서 사용 가능한 서브넷이 사전 선택됩니다.\n Security groups :\nAmazon EKS 클러스터 VPC 생성 사용을 통해 생성된 보안 그룹(SecurityGroups) 값을 선택 합니다.\n 중요\n작업자 노드 AWS CloudFormation 템플릿이 여기서 사용자가 지정하는 보안 그룹을 수정하므로 클러스터 제어 플레인에 대한 전용 보안 그룹을 사용하는 것이 바람직합니다. 다른 리소스와 공유할 경우 이러한 리소스를 방해하거나 막을 수 있습니다.   정보 입력이 완료 되면 생성(Create)을 선택합니다.\n  4. Clusters 화면에서 새로 생성된 클러스터를 선택하고 클러스터 정보를 확인할 수 있습니다.\n 5. Status(상태) 필드는 클러스터 프로비저닝 프로세스가 완료될 때까지 CREATING(생성 중)으로 표시됩니다.\n클러스터 프로비저닝이 완료될 때(보통 10분 이내) API server endpoint(API 서버 엔드포인트) 및 인증 기관 값을 기록합니다. 이 값은 kubectl 구성에 사용됩니다.\n 6. 클러스터를 생성했으므로 이제 Amazon EKS용 kubectl 구성 및 Amazon EKS에 대한 kubeconfig 생성의 절차를 따라 새 클러스터와의 통신을 활성화합니다.\n   4-1. Amazon EKS용 kubectl 구성 AWS CLI로 kubeconfig를 생성하려면 Amazon EKS에 대한 kubeconfig 생성을 참조 하세요.\n 기본 AWS 자격 증명 공급자 체인을 사용 하려면 :\nAWS IAM 권한이 있는 관리자에게 사용자 보안 자격 증명(액세스 키)을 요청 합니다.\n AWS 자격 증명 공급자 체인 만들기\n AWS IAM service 콘솔에서 사용자(Users)를 선택합니다.\n 보안 자격 증명(Security credentials) 섹션 에서 액세스 키를 생성합니다.\n   ~/.aws/credentials 파일 안에 해당 키 값을 입력 합니다.\n 윈도우 경우 사용자 디렉토리 안에 .aws\\redentials 파일이 있습니다.  [default] aws_access_key_id= aws_secret_access_key=   또는 AWS CLI를 사용해서 클러스터를 생성한 계정의 IAM유저의 보안자격증명을 설정합니다.  # aws configure AWS Access Key ID [None]: AKIAIHE#########FELA AWS Secret Access Key [None]: uqm8MSDK######################Ddz29PcAz5 Default region name [ap-northeast-2]: Default output format [None]: # aws sts get-caller-identity { “Account”: “4952894#####”, “UserId”: “AIDAJU5LO##########LQ”, “Arn”: “arn:aws:iam::4952894#####:user/[username]” }   AWS CLI로 kubeconfig를 생성하려면 :\n# aws eks --region region update-kubeconfig --name cluster_name Updated context arn:aws:eks:ap-northeast-2:235896307296:cluster/cocktail-test-eks-cluster in C:\\Users\\shapj\\.kube\\config # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 1m   5. 2단계: Amazon EKS Work node 시작 및 구성 Amazon EKS work node는 AWS 계정에서 실행되고, 클러스터 API 서버 엔드포인트를 통해 Cluster Control Plane에 연결합니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/clusters.html\n 중요\nAmazon EKS Work node는 표준 Amazon EC2 인스턴스이고, 일반 Amazon EC2 온디맨드 인스턴스 가격을 기반으로 비용이 청구됩니다.\n자세한 내용은 Amazon EC2 요금을 참조하십시오.\n 이 주제의 사전 요구 사항은 다음과 같습니다.\n Amazon EKS 클러스터의 요건을 충족하는 VPC 및 보안 그룹을 생성했습니다.\n Amazon EKS 클러스터를 생성하고 위 VPC 및 보안 그룹에 사용한다고 지정했습니다.\n 시작 이후 SSH를 사용하여 Work node에 연결하는 데 사용할 수 있는 Amazon EC2 SSH 키 페어가 생성되어 있어야 합니다.\n참조 : Amazon EC2를 사용해 키 페어 만들기\n   5-1. 템플릿을 사용해서 Work node를 시작하려면  클러스터 상태가 ACTIVE가 되기를 기다립니다.\n※ 클러스터가 활성화되기 전에 Work node를 시작하면 Work node가 클러스터에 등록되지 않고 Work node를 다시 시작해야 한다.\n https://console.aws.amazon.com/cloudformation 에서 AWS CloudFormation 콘솔.\n Amazon EKS를 생성할 리전을 선택한다.\n이 사용 설명서에서는 서울 리전을 사용한다. \u0026lt;!\u0026ndash; 참고\n현재 다음 리전에서 Amazon EKS를 사용할 수 있습니다. (한국어 사용 설명서는 아직 반영이 안되어 있습니다.)\n US West (Oregon) (us-west-2)\n US East (N. Virginia) (us-east-1)\n US East (Ohio) (us-east-2)\n EU (Frankfurt) (eu-central-1)\n EU (Stockholm) (eu-north-1)\n EU (Ireland) (eu-west-1)\n Asia Pacific (Tokyo) (ap-northeast-1)\n Asia Pacific (Seoul) (ap-northeast-2)\n Asia Pacific (Singapore) (ap-southeast-1)\n Asia Pacific (Sydney) (ap-southeast-2) \u0026ndash;\u0026gt;\n  [Create stack]을 선택한다.\n 템플릿 선택에서 Amazon S3 템플릿 URL 지정을 선택한다.\n 텍스트 영역에 다음 URL을 붙여넣고 다음을 선택한다.\nhttps://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml  Specify Details(세부 정보 지정) 페이지에서 다음 파라미터를 입력하고 다음을 선택한다.\n 스택 이름 :\nAWS CloudFormation 스택에 대한 스택 이름을 선택합니다. 예를 들어 -worker-nodes로 사용할 수 있습니다.\n ClusterName :\nAmazon EKS 클러스터 생성 시 사용할 이름을 입력합니다.\n중요 : 이 이름은 Amazon EKS 클러스터 이름과 정확하게 일치해야 합니다. 그렇지 않은 경우 Work node가 조인할 수 없습니다.\n ClusterControlPlaneSecurityGroup :\nAmazon EKS 클러스터를 생성할 때 사용한 보안 그룹 또는 그룹을 입력합니다. 이 AWS CloudFormation 템플릿은 지정된 클러스터 제어 플레인 보안 그룹의 트래픽을 허용하는 Work node 보안 그룹을 생성합니다.\n NodeGroupName :\n노드 그룹의 이름을 입력합니다. 이 이름은 나중에 Work node에 대해 생성된 Auto Scaling 노드 그룹을 식별하는 데 사용할 수 있습니다.\n NodeAutoScalingGroupMinSize :\nAuto Scaling 그룹이 축소할 수 있는 Work node의 최소 노드 수를 입력합니다.\n NodeAutoScalingGroupDesiredCapacity : 스택을 생성할 때 조정할 원하는 노드 수를 입력합니다.\n NodeAutoScalingGroupMaxSize :\nAuto Scaling 그룹이 확장할 수 있는 Work node의 최대 노드 수를 입력합니다.\n NodeInstanceType :\nWork node에 대한 인스턴스 유형을 선택합니다.\n NodeImageId :\n리전에 대한 현재 Amazon EKS Work node AMI ID를 입력합니다.\n최신 Amazon EKS 최적화 AMI(GPU 지원 유무는 상이)의 AMI ID는 다음 표에 나와 있습니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html\nKubernetes version 1.12.7\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     Asia Pacific (Tokyo) (ap-northeast-1) ami-0bfedee6a7845c26d ami-08e41cc84f4b3f27f   Asia Pacific (Seoul) (ap-northeast-2) ami-0a904348b703e620c ami-0c43b885e33fdc29e   Asia Pacific (Sydney) (ap-southeast-2) ami-0f0121e9e64ebd3dc ami-07079cd9ff1b312da    Kubernetes version 1.11\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     US West (Oregon) (us-west-2) ami-0a2abab4107669c1b ami-0c9e5e2d8caa9fb5e   US East (N. Virginia) (us-east-1) ami-0c24db5df6badc35a ami-0ff0241c02b279f50   US East (Ohio) (us-east-2) ami-0c2e8d28b1f854c68 ami-006a12f54eaafc2b1   EU (Frankfurt) (eu-central-1) ami-010caa98bae9a09e2 ami-0d6f0554fd4743a9d   EU (Stockholm) (eu-north-1) ami-06ee67302ab7cf838 ami-0b159b75   EU (Ireland) (eu-west-1) ami-01e08d22b9439c15a ami-097978e7acde1fd7c   Asia Pacific (Tokyo) (ap-northeast-1) ami-0f0e8066383e7a2cb ami-036b3969c5eb8d3cf   Asia Pacific (Seoul) (ap-northeast-2) ami-0b7baa90de70f683f ami-0b7f163f7194396f7   Asia Pacific (Singapore) (ap-southeast-1) ami-019966ed970c18502 ami-093f742654a955ee6   Asia Pacific (Sydney) (ap-southeast-2) ami-06ade0abbd8eca425 ami-05e09575123ff498b    Kubernetes version 1.10\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     US West (Oregon) (us-west-2) ami-09e1df3bad220af0b ami-0ebf0561e61a2be02   US East (N. Virginia) (us-east-1) ami-04358410d28eaab63 ami-0131c0ca222183def   US East (Ohio) (us-east-2) ami-0b779e8ab57655b4b ami-0abfb3be33c196cbf   EU (Frankfurt) (eu-central-1) ami-08eb700778f03ea94 ami-000622b1016d2a5bf   EU (Stockholm) (eu-north-1) ami-068b8a1efffd30eda ami-cc149ab2   EU (Ireland) (eu-west-1) ami-0de10c614955da932 ami-0dafd3a1dc43781f7   Asia Pacific (Tokyo) (ap-northeast-1) ami-06398bdd37d76571d ami-0afc9d14b2fe11ad9   Asia Pacific (Seoul) (ap-northeast-2) ami-08a87e0a7c32fa649 ami-0d75b9ab57bfc8c9a   Asia Pacific (Singapore) (ap-southeast-1) ami-0ac3510e44b5bf8ef ami-0ecce0670cb66d17b   Asia Pacific (Sydney) (ap-southeast-2) ami-0d2c929ace88cfebe ami-03b048bd9d3861ce9    KeyName :\n시작 이후 SSH를 사용하여 Work node에 연결하는 데 사용할 수 있는 Amazon EC2 SSH 키 페어 이름을 입력합니다.\nAmazon EC2 키 페어가 아직 없는 경우 AWS Management 콘솔에서 새로 생성할 수 있습니다.\n자세한 내용은 Linux 인스턴스용 Amazon EC2 사용 설명서의 Amazon EC2 키 페어를 참조하십시오.\n참고 : 여기에 키 페어를 입력하지 않으면 AWS CloudFormation 스택이 생성되지 않습니다.\n BootstrapArguments :\n별도의 kubelet 인수와 같이 Work node 부트스트랩 스크립트에 전달할 선택적 인수를 지정합니다.\n자세한 내용은 https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh에서 부트스트랩 스크립트 사용 정보를 참조하십시오.\n VpcId :\nWork node에서 시작해야 하는 VPC의 ID를 입력합니다.\n Subnets :\nWork node에서 시작해야 하는 위 VPC 내 서브넷을 선택합니다.\n 옵션 페이지에서 스택 리소스에 태그를 지정할 수 있습니다. [Next]를 선택합니다.\n 검토 페이지에서 정보를 검토하고, 스택이 IAM 리소스를 생성할 수 있음을 승인합니다. 항목을 체크 한 다음 생성을 선택합니다.\n 스택이 생성된 후 콘솔에서 이를 선택하고 출력을 선택합니다.\n 생성된 노드 그룹에 대해 NodeInstanceRole을 기록합니다. Amazon EKS Work node를 구성할 때 필요합니다.\n    5-2. Work node가 클러스터에 조인하도록 하려면  AWS IAM Authenticator 구성 맵을 다운로드, 편집 및 적용합니다.\n ConfigMap 다운로드:\ncurl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml  텍스트 편집기에서 파일을 엽니다. 인스턴스 역할의 \u0026lt;ARN(비인스턴스 프로파일)\u0026gt; 조각을 이전 절차에서 기록한 NodeInstanceRole 값으로 교체하고 파일을 저장합니다.\n중요:\n이 파일에서 어떠한 행도 수정하지 마십시오.\napiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: \u0026lt;ARN of instance role (not instance profile)\u0026gt; username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes  구성을 적용합니다. 이 명령을 완료하는 데 몇 분이 걸릴 수 있습니다.\nkubectl apply -f aws-auth-cm.yaml   노드의 상태를 확인하고 Ready 상태가 될 때까지 대기합니다.\nkubectl get nodes --watch  Kubernetes Metric Server 설치 한다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n Cocktail-addon을 설치 한다.\n  Amazon Elastic File System 만들기 Amazon Elastic File System (Amazon EFS)은 Amazon EC2에서 사용하기 위한 간단하고 확장 가능한 파일 스토리지를 제공합니다.\n참조 : https://docs.aws.amazon.com/ko_kr/efs/latest/ug/whatisefs.html\n1. 보안그룹 생성  EFS 파일 시스템이 사용할 보안 그룹을 생성 한다.  2. Amazon EFS 파일 시스템 생성  Amazon EFS 파일 시스템 생성\n Amazon EFS 관리 콘솔을 엽니다.\n Create File System(파일 시스템 생성)을 선택합니다.\n VPC와 연결된 시큐리티그룹을 선택(NFS 포트가 열려 있어야한다./ port : 2049)\n 목록에서 탑재 대상을 생성하기 원하는 서브넷을 선택한다.\n IP 주소를 자동으로 선택.\n [Next Step]을 선택합니다.\n 파일 시스템의 이름을 지정하고, 기본 성능 및 처리량 모드로 선택한 범용 및 버스팅 모드를 유지한 후 다음 단계를 선택합니다.\n Create File System(파일 시스템 생성)을 선택합니다.\n 목록에서 파일 시스템을 선택하고 File system ID(파일 시스템 ID)에 대한 값을 기록합니다. 다음 단계에 이 값이 필요합니다.\n   4. EFS Persistent Volumes on Kubernetes on AWS 참조 : Amazon EFS를 선택해야하는 경우\n Deploy the efs-provisioner\n참조 : https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs\nefs-provisioner를 사용하면 EFS 저장소를 kubernetes의 PersistentVolume으로 마운트 할 수 있습니다.\nAWS EFS 리소스에 액세스 할 수있는 컨테이너로 구성됩니다.\n컨테이너는 EFS 파일 시스템 ID, AWS 영역 및 efs-provisioner에 사용할 이름을 포함하는 configmap을 읽습니다.\n 아래 스크립트를 사용 합니다.\n kubectl apply -f 1-eks-cocktail-efs-rbac.yaml\n클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.\n\u0026ldquo;default\u0026rdquo;가 아닌 네임 스페이스 / 프로젝트에 있다면 rbac.yaml을 편집하십시오.\n  --- # ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: efs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: efs-provisioner-runner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-efs-provisioner subjects: - kind: ServiceAccount name: efs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: efs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-efs-provisioner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-efs-provisioner subjects: - kind: ServiceAccount name: efs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-efs-provisioner apiGroup: rbac.authorization.k8s.io   kubectl apply -f 2-eks-cocktail-efs-provisioner.yaml\nDeployment efs-provisione  --- apiVersion: v1 kind: ConfigMap metadata: name: efs-provisioner data: # your file.system.id , aws.region file.system.id: fs-b4c21dd5 aws.region: ap-northeast-2 provisioner.name: acornsoft.io/aws-efs dns.name: \u0026quot;\u0026quot; --- --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: efs-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: efs-provisioner spec: serviceAccount: efs-provisioner containers: - name: efs-provisioner image: quay.io/external_storage/efs-provisioner:latest env: - name: FILE_SYSTEM_ID valueFrom: configMapKeyRef: name: efs-provisioner key: file.system.id - name: AWS_REGION valueFrom: configMapKeyRef: name: efs-provisioner key: aws.region - name: DNS_NAME valueFrom: configMapKeyRef: name: efs-provisioner key: dns.name optional: true - name: PROVISIONER_NAME valueFrom: configMapKeyRef: name: efs-provisioner key: provisioner.name volumeMounts: - name: pv-volume mountPath: /persistentvolumes volumes: - name: pv-volume nfs: # your file-system DNS NAME server: fs-b4c21dd5.efs.ap-northeast-2.amazonaws.com path: / ---   kubetl apply -f 3-eks-cocktail-efs-storage-class.yaml\nCocktail 퍼시스턴트 볼륨 Storage Class 생성  # single-storage : EBS-PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: single-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: AWSEBS acornsoft.io/type: SINGLE k8s-addon: storage-aws.addons.k8s.io provisioner: kubernetes.io/aws-ebs reclaimPolicy: Delete volumeBindingMode: Immediate parameters: type: gp2 --- # shared-storage : EFS-PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: shared-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: AWSEFS acornsoft.io/type: SHARED k8s-addon: storage-aws.addons.k8s.io # your provisioner.name provisioner: efs-test-pvc/aws-efs ---   4-eks-cocktail-efs-sample-pvc.yaml 샘플 PVC  # sample pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: efs-test-pvc annotations: volume.beta.kubernetes.io/storage-class: \u0026quot;shared-storage\u0026quot; spec: storageClassName: shared-storage accessModes: - ReadWriteMany resources: requests: storage: 1Mi ---   아래 명령으로 퍼시스턴트 볼륨을 확인 합니다.   kubectl get pv    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.4-cluster-kass-eks-policy/",
	"title": "8.1.1.3 EKS 클러스터 정책 / 권한",
	"tags": [],
	"description": "",
	"content": " 사용자 권한 / 정책 만들기 AWS 관리자 권한이 없다면 필요한 역할 / 정책 / 권한 을 요청 해야 합니다.\n1. 사용자 권한 / 정책 사용자가 클러스터를 생성 / 관리할 수 있는 권한 / 정책이 필요 합니다.\n CloudFormation - 전체: 목록, 쓰기 제한: 읽기 / 이에 사용되는 리소스 권한이 있어야 합니다.\nAWS CloudFormation은 개발자와 시스템 관리자에게 관련 AWS 리소스 모음을 손쉽게 생성 및 관리하고 순서 있고 예측 가능한 방식으로 프로비저닝하고 업데이트 할 수있는 방법을 제공합니다.\n EC2 - AmazonEC2FullAccess 권한이 있어야 합니다.\nAmazon Elastic Compute Cloud (Amazon EC2)는 클라우드에서 안전하고 크기 조정이 가능한 컴퓨팅 용량을 제공하는 웹 서비스입니다.\n EKS - 모든 액세스 / 모든 리소스 권한이 있어야 합니다.\nAmazon Elastic Container Service for Kubernetes (EKS)는 자체 Kubernetes control plane을 설치, 작동 및 유지할 필요없이 Kubernetes를 AWS에서 쉽게 실행할 수 있도록 관리되는 Kubernetes 서비스입니다.\n IAM - 목록, 읽기, 쓰기, 권한 관리 / 이에 사용되는 리소스 권한이 있어야 합니다.\nAWS Identity and Access Management (IAM)를 사용하면 AWS 서비스 및 리소스에 대한 액세스를 안전하게 관리합니다.\n SNS - 목록 / 모든 리소스 권한이 있어야 합니다.\nAmazon Simple Notification Service(SNS)는 마이크로서비스, 분산 시스템 및 서버리스 애플리케이션을 쉽게 분리할 수 있게 해 주는 내구적이고 안전한 고가용성의 완전 관리형 게시/구독 메시징 서비스입니다\n VPC - AWS Management Console을 통해 Amazon VPC에 대한 완벽한 액세스를 제공합니다.\nAmazon Virtual Private Cloud(Amazon VPC)에서는 사용자가 정의한 가상 네트워크로 AWS 리소스를 시작할 수 있습니다.\n  2. 사용자 권한 / 정책 만들기  IAM(Identity and Access Management) 에서 정책(Policies)을 선택합니다.\n 정책 생성(Create policy) / 기존의 정책 편집(Edit policy)을 선택합니다.\n 권한 추가(Add additional permissions)를 선택합니다.\n 서비스 항목에서 필요한 서비스를 검색 / 선택합니다.\n CloudFormation - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAWS CloudFormation은 Amazon Web Services 리소스를 모델링하고 설정하는 데 도움이되는 서비스로, 리소스 관리 시간을 줄이고 AWS에서 실행되는 애플리케이션에 집중할 수 있습니다.\n원하는 모든 AWS 리소스 (예 : Amazon EC2 인스턴스 또는 Amazon RDS DB 인스턴스)를 설명하는 템플릿을 만들고 AWS CloudFormation은 이러한 리소스를 프로비저닝하고 구성하는 작업을 담당합니다.\nAWS 리소스를 개별적으로 생성하고 구성 할 필요가 없으며 무엇에 의존하는지 파악할 수 있습니다. Learn more\n EKS - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nKubernetes 용 Amazon Elastic Container Service (Amazon EKS)는 자체 Kubernetes control plane을 세우거나 유지할 필요없이 AWS에서 Kubernetes를 쉽게 실행할 수 있도록 해주는 관리 형 서비스입니다.\n IAM - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAWS ID 및 액세스 관리 (IAM)는 AWS 리소스에 대한 액세스를 안전하게 제어 할 수있는 웹 서비스입니다.\nIAM을 사용하여 자원을 사용하도록 인증 (로그인)되고 권한이 부여 된 (사용 권한이있는) 사용자를 제어합니다.\n SNS - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAmazon SNS (Amazon Simple Notification Service)는 가입 엔드 포인트 또는 클라이언트로 메시지를 전달하거나 메시지 서비스를 조정 및 관리하는 웹 서비스입니다.\n게시자는 논리 액세스 포인트 및 통신 채널 인 주제에 메시지를 생성하고 전송하여 구독자와 비동기 적으로 통신합니다.\n  정책 검토(Review policy)를 선택합니다.\n 이름(Name) :\nAmazonEKSAdminPolicy와 같은 사용자 정책 이름을 입력 합니다.\n 설명(Description) :\n사용자 정책의 설명을 입력 합니다.\n  정책 생성을 선택합니다.\n  IAM(Identity and Access Management) 에서 사용자(Users)를 선택합니다.\n 생성한 또는 편집한 정책을 사용자에게 추가 합니다.\n 권한 추가(Add permission)를 선택합니다.\n 기존 정책 직접 연결(Attach existing policies directly )을 선택합니다.\n 생성 및 편집 한 정책을 검색 해서 사용자에게 권한 추가 합니다.\n 이 사용 설명서 에서는 다음 과 같은 정책으로 사용 하였습니다.\n AmazonEKSAdminPolicy :\nCloudFormation, EKS, IAM, SNS\n AmazonVPCFullAccess :\nProvides full access to Amazon VPC via the AWS Management Console.\nAmazon EC2를 사용하여 필요한만큼 가상 서버를 시작하고 보안 및 네트워킹을 구성하며 스토리지를 관리 할 수 있습니다.\n AmazonEC2FullAccess :\nProvides full access to Amazon EC2 via the AWS Management Console.\nAmazon Elastic Compute Cloud (Amazon EC2)는 Amazon Web Services (AWS) 클라우드에서 확장 가능한 컴퓨팅 용량을 제공합니다.\nAmazon EC2를 사용하여 필요한만큼 가상 서버를 시작하고 보안 및 네트워킹을 구성하며 스토리지를 관리 할 수 있습니다.\n     이 사용 설명서에서는 아래와 같이 권한을 사용합니다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.5-cluster-kass-eks-quick-start/",
	"title": "8.1.1.5 EKS 클러스터 만들기 Quick Start",
	"tags": [],
	"description": "",
	"content": "1. 필수 Role(역할) / Policy(정책) 만들기 a. EKS 관리 역할 만들기. b. IAM(services) \u0026gt; Roles \u0026gt; Create role 화면에서 i. EKS 선택 : AmazonEKSClusterPolicy , AmazonEKSServicePolicy (default로 구성되어 있다) ii. Next: Permissions \u0026gt; Next: Tags \u0026gt; Next: Revies iii. Review 화면에서 1) Role name : EKSClusterRole 2) Create role 클릭 c. EKS 관리 정책 만들기. d. IAM(services) \u0026gt; Policies \u0026gt; Create policy 화면에서 i. Choose a service \u0026gt; Service : EKS 검색 및 선택 ii. Action : All EKS actions 선택 iii. Resources : All Resources 선택 \u0026gt; Add additional permissions 클릭\u000biv. Choose a service \u0026gt; Service : SNS 검색 및 선택 v. Action : List \u0026gt; ListTopics 선택 \u0026gt; Review policy 클릭 vi. Review Policy 화면에서 1) Name* : AmazonEKSAdminPolicy 2) Create policy 클릭 2. EKS생성 하는 User 권한 / 정책 부여 하기 a. IAM(Services) \u0026gt; Users \u0026gt; EKS 생성 하는 user 클릭 i. Summary 화면에서 \u0026gt; Add permissions 클릭 ii. Grant permissions 에서 Attach existing policies directly 선택. 1) AmazonEKSAdminPolicy 2) AmazonEC2FullAccess 3) IAMFullAccess 4) AmazonEKSClusterPolicy 5) AmazonVPCFullAccess 6) AmazonEKSServicePolicy 7) AmazonElasticFileSystemFullAccess 8) AWSDeepRacerCloudFormationAccessPolicy iii. 위 목록을 검색 / 선택 해서 추가 한다. 3. 보안을 위한 아무 권한이 없는 칵테일 클러스터 등록 유져 만들기 a. IAM(services) \u0026gt; Users \u0026gt; Add user 화면에서 i. User name : acloud-client ii. Access type* : Programmatic access 선택 b. : \u0026gt; Next: Permissions \u0026gt; Next: Tags \u0026gt; Next: Review \u0026gt; Create user c. 완료되면 Download.csv를 받아서 저장한다. 4. VPC 만들기 - cloudformation template 사용 i. AWS console에서 CloudFormation 메뉴에서 \u0026quot;Create Stack\u0026quot;버튼 클릭 ii. Choose a template에서 \u0026quot;Specify an Amazon S3 template URL\u0026quot; 선택하고 아래 값을 입력 https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml iii. Stack name: eks-cocktail로 입력 iv. 2~3분정도 걸림. 5. EKS Cluster 생성 (Master 생성) a. AWS console에서 EKS 메뉴로 이동 i. Cluster name: cocktail-eks-cluster ii. Kubernetes Version : 1.12 iii. Role name : EKSClusterRole 선택 (1번에서 생성함) iv. VPC : eks-vpc-stack-VPC 선택 (4번에서 생성한 vpc) v. Subnet : default(모두 선택) vi. Security groups : eks-vpc-stack-ControlPlaneSecurityGroup 선택 (4번에서 생성된 security group) vii. Private access : Disabled (default) 선택 viii. Public access : Enabled (default) 선택 ix. Create -\u0026gt; 5분 정도 소요됨. 리플레쉬 버튼으로 확인 6. Aws-iam-authenticator, aws cli, kubectl 설치. (5번 진행되는 동안 설치 진행) a. Aws-iam-authenticator 설치: https://acornapps.github.io/cocktail-manual-3.1.1/docs/ko/8.1.1.3/ 의 3.1 참고 b. Aws cli 설치: https://acornapps.github.io/cocktail-manual-3.1.1/docs/ko/8.1.1.3/의 3.2 참고 c. Kubectl 설치: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl 참고 d. aws eks --region {region_code} update-kubeconfig --name {cluster_name} --kubeconfig {path_to_kubeconfig} e. export KUBECONFIG={path_to_kubeconfig} f. Kubectl get svc 로 접속확인 7. Node 생성 i. Cloudformation에서 Choose a template에서 \u0026quot;Specify an Amazon S3 template URL\u0026quot; 선택하고 아래 값을 입력 https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml ii. Specify Details 화면에서 i. Stack name: eks-nodes-stack ii. ClusterName : 생성한 EKS Cluster name iii. ClusterControlPlaneSecurityGroup : eks-vpc-stack-ControlPlaneSecurityGroup 선택 iv. NodeGroupName: cocktail-node-group v. NodeAutoScalingGroupMinSize : 1 (최소 1개) vi. NodeAutoScalingGroupDesiredCapacity : 2 (생성 할 노드 개수) vii. NodeAutoScalingGroupMaxSize : 2 (AutoScaling 최대 개수) viii. NodeInstanceType : t3.xlarge 선택 (설명 별첨 1) ix. NodeImageId : ami-0bfedee6a7845c26d (최신 ami 목록 별첨 2) x. NodeVolumeSize : 100 (노드 총 사이즈) xi. KeyName : 생성 한 키 페어 선택 xii. BootstrapArguments : 공백 (default) xiii. VpcId : 위에서 생성한 eks-vpc-stack-VPC 선택 xiv. Subnets : 위에서 생성한 eks-vpc-stack-Subnet01, eks-vpc-stack-Subnet02, eks-vpc-stack-Subnet03 xv. Next xvi. Options 화면은 default \u0026gt; Next xvii. Review 화면에서 I acknowledge that AWS CloudFormation might create IAM resources. 체크 xviii. Create iii. Node 생성 (5분내외 소요) 8. Node join 하기 i. curl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml ii. vi aws-auth-cm.yaml ``` apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: ( CloudFormation \u0026gt; 노드 생성 stack 에서 Outputs \u0026gt; NodeInstanceRole 의 value ) username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes mapUsers: | - userarn: ( IAM \u0026gt; Users \u0026gt; 위에서 생성한 acloud-client 선택 \u0026gt; User ARN 값 입력 ) username: admin groups: - system:masters ``` iii. kubectl apply -f aws-auth-cm.yaml iv. kubectl get cm --all-namespaces ``` NAMESPACE NAME DATA AGE kube-system aws-auth 2 20s ``` v. kubectl get nodes 9. EFS(Elastic File System) 생성 a. EFS 연동을 위한 Security Group 생성 i. EC2 \u0026gt; Security Groups 화면 에서 \u0026quot;Create Security Group\u0026quot; 버튼 클릭 -- Create Security Group -- Security group name : efs-sg Description : sfs-sg VPC : 4번에서 생성한 vpc 선택 ii. Add rule 버튼 클릭 Type : Custom TCP Protocol : TCP Port Range : 2049 Source : Custom, eks-cocktail-nodes-NodeSecurityGroup 을 선택하고 Create 버튼 클릭 b. Node에 생성된 Security Group 적용 i. EC2 \u0026gt; Instances 화면 에서 개별 노드 선택 ii. Action 클릭 \u0026gt; Networking 클릭 \u0026gt; Change Security Groups 선택 iii. Change Security Groups 화면 에서 생성된 efs-sg를 포함하여 \u0026quot;Assign Security Groups\u0026quot; 버튼을 클릭. c. EFS 생성 i. EFS \u0026gt; Create File System 버튼 클릭 ii. VPC : 4번에서 생성한 vpc 선택 iii. 하단의 Create mount targets의 security groups에 EFS 연동을 위해 생성한 security group만 선택 iv. Next Step 버튼 클릭 v. Configure optional settings 화면에서 수정없이 \u0026quot;Next Step\u0026quot; 버튼클릭. vi. Review and create 화면에서 \u0026quot;Create File System\u0026quot;버튼 클릭하여 생성함. 10. Cocktail Addon i. 제공되스 스크립트를 사용 하여 일관 apply 한다. i. ./addon.sh \u0026lt;your kubeconfig-path\u0026gt; \u0026lt;your efs-file-system-id\u0026gt; \u0026lt;your region\u0026gt; \u0026lt;your cocktail cluster-id\u0026gt; 11. Cocktail Cluster 등록 i. Cluster 등록 -- Provider -- i. Provider : Amazon Web Service ii. Type : EKS iii. Region : Tokyo iv. Cluster ID : \u0026lt;your EKS Cluster Name\u0026gt; -- Basic Infomation -- i. name : \u0026lt;your Cocktail Cluster Name\u0026gt; ii. Kubernetes Version : 1.12.6 (EKS Kubernetes Version) iii. ID : \u0026lt;your Cocktail Cluster Id\u0026gt; -- Endpoint -- i. Master address : \u0026lt;your EKS API server endpoint\u0026gt; ii. Ingress host address : \u0026lt;your Load Balancer DNS name\u0026gt;(선택사항) 1) LB 없을시 Work node의 public ip를 사용 해서 확인할 수 있다. iii. Node Port Host Address : \u0026lt;your Load Balancer DNS name\u0026gt;(선택사항) 1) LB 없을시 Work node의 public ip를 사용 해서 확인할 수 있다. iv. Node Port Range : 30000-32767 -- User Account Authentication Information -- i. Cluster CA Certification : \u0026lt;your EKS화면에서 Certificate authority 값\u0026gt; ii. AWS Access Key ID : \u0026lt;your acloud-client Access Key ID\u0026gt;(보안을 위해 생성한 더미 유저) iii. AWS Secret Access Key : \u0026lt;your acloud-client Secret Access Key\u0026gt;(보안을 위해 생성한 더미 유저) ii. Workspace Create i. Workspace Name : \u0026lt;your workspaces name\u0026gt; ii. Image Registry : \u0026lt;your workspaces name\u0026gt; iii. Cluster Add 클릭 1) 등록한 Cocktail Cluster를 선택 후 생성 / 업데이트 한다. 참조 : EKS cluster 에서 elb 생성 없이 nodepott / ingress 확인 하기 i. Work Node의 public ip로 확인 할 수 있다. 1) Cocktail cluster 등록 / 수정 화면에서 인그레스 호스트 주소 , 노트 포트 호스트 주소 를 수정 한다. ## 별첨 목록 1. AWS instance별 pod 제한 개수: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt 2. 최신 ami 목록 참조 : https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/8.operation/8.1.cluster-install/8.1.2.onpremise/8.1.2-cluster-on-premise/",
	"title": "8.1.2.1 클러스터 직접 구성",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/0.5/",
	"title": "Catalog Layer",
	"tags": [],
	"description": "",
	"content": "Containers implement deployments by creating images from configurations. Therefore, repetitive tasks can be minimized and frequently-used DBs and middleware runtime can be easily deployed whenever needed by creating templates of container images and deployment information.\nThe catalog layer is responsible for creating and managing these templates. The following diagram shows how the catalog layer is structured:\nThe catalog layer is divided into two sections according to purpose.\n Service Catalog : The service catalog stores and manages application snapshots that the user has configured for a service. A snapshot of the service catalog can be used if migration is required due to infrastructure or cloud provider replacement. A snapshot stores the service map configuration as-is. Therefore, the snapshot configuration must be modified as needed when deploying to a different environment.\n Platform Catalog : The Platform catalog refers to the runtime template provided via Cocktail Hub in Cocktail. The user can deploy the template as-is or by changing the configuration to suit a specific environment. The Platform catalog currently provides widely-used open-source templates (DB, middleware, development/operations tool, blog, machine learning, etc.) and is continually being updated.\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/0.2/",
	"title": "Cluster Management Layer",
	"tags": [],
	"description": "",
	"content": "The cluster management layer is divided into the infrastructure, which consists of clusters, and the container orchestration engine. The detailed configuration is as follows:\nCocktail automatically provisions the cluster infrastructure and orchestration engine according to user configuration. The tool responsible for this is Cube. Cube constitutes high-availability (HA) Kubernetes clusters. Cube-provisioned cluster configurations provide high reliability, security, and scalability. (This is referred to as a Cube cluster.)\nCube clusters currently support the cloud platforms and providers listed below. Note, however, that Cube can be configured independently on any infrastructure. The following is based on automatic infrastructure provisioning.\n Physical infrastructure (Baremetal)\n Cloud platform : Openstack, Cloudstack, VMWare\n Cloud service : AWS, GCP, Azure\n  Cube also provides cluster management features such as Kubernetes version upgrading, node (physical/virtual machine) adding/deleting, and backups.\nCocktail provides integrated-management of multiple clusters. That is, multiple clusters can be assigned to the desired service, and all clusters can be managed and monitored.\nClusters can be added as needed. Additional scaling components are provided via Cube clusters.\n Monitoring : Infrastructure, container, service (workload) status, configuration, resource monitoring. Alerting : Alerts via email/messenger when specific conditions are met. Metering : View usage cost of public cloud clusters. Inspecting : Change tracking and event management of containers and Kubernetes objects  Information and management functions provided via scaling component are available through Cocktail\u0026rsquo;s Custer Management view.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/0.1/",
	"title": "Cocktail Structure Overview",
	"tags": [],
	"description": "",
	"content": "Cocktail is comprised of five layers that correspond to its various roles.\n Cluster Management Layer: Responsible for infrastructure (cluster), where containers are deployed/executed, and orchestration. Kubernetes (https://kubernetes.io) handles orchestration and provides scaling management functions such as infrastructure management and monitoring.\n Service Management Layer: Responsible for container configuration and management based on a service (workload) packages containers and associated objects that constitute a service and manages lifecycles and monitoring.\n Pipeline: Automates the coding and container build/deployment process and continuously integrates/deploys. Users can configure the desired pipeline via settings and scripting.\n Catalog: Provides common runtime (DB, middleware, etc.) templates. When needed, templates can be deployed and used without any additional configuration. In addition, templates can also store and manage snapshots of user applications.\n Dashboard: Provides cluster, service status, and monitoring views.\n  Let\u0026rsquo;s look at each layer in detail\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/",
	"title": "Cocktial Cloud Manual",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/0.6/",
	"title": "Dashboard Layer",
	"tags": [],
	"description": "",
	"content": "The dashboard layer provides service status, monitoring, and cluster status information via a web-based UI.\nThe dashboard provides role-specific views according to user privileges. Cocktail user privileges are as follows:\n Administrator : The admin manages clusters and has administrative privileges on all Cocktail services.\n DevOps: Have administrative privileges only for one or more assigned workspaces. It provides a dashboard for each workspace, and can not manage clusters.\n Manager : A manager can access all views and information but cannot perform management actions such as creation or modification. This user privilege is granted to users who perform analyses (e.g., status).\n  Cocktail provides the following dashboard views:\n Comprehensive dashboard : A comprehensive status view of all services and clusters.\n Service management view : Views for creating, deploying, monitoring, and operating services.\n Catalog view : View for looking up service and platform catalogs and deployment.\n Cluster view : View for integrated-managing and monitoring multiple clusters.\n Environment settings : Settings view for the Cocktail platform.\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/0.4/",
	"title": "Pipeline Layer",
	"tags": [],
	"description": "",
	"content": "The pipeline layer automates container building and deployment from code.\nThe build step performed tasks (code download, build/test) and container image building sequentially based on user settings. Tasks use containers that perform tasks internally and can be scaled using public or user-created containers in addition to the default containers.\nBuilt images are updated (rolling) to containers on a Cocktail server. Images created in the build step are assigned an automatic build number, which is used for version updates or rollbacks.\nAll work processes from build to deployment are automated via the pipeline. Thus, developers can focus on writing and modifying code once a pipeline is configured.\nThe following diagram shows how the pipeline layer is structured:\nThe pipeline layer provides a foundation for DevOps through continuous integration and deployment (CI/CD), and DevOps emphasizes continuity and organic cooperation between development and operations. As one of the components required for DevOps, CI/CD is equally important in container-based development and operations.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/1.intro/0.3/",
	"title": "Service Management Layer",
	"tags": [],
	"description": "",
	"content": "In general, a single application or service consists of multiple containers. In particular, an orchestration engine such as Kubernetes requires additional objects such as a service port, controller, and storage volume in addition to the container. Therefore, to manage container-based applications and services, an abstraction unit is required to package and manage containers and associated objects. The service management layer fulfills this role.\nCocktail service management layer has the following hierarchical structure:\n Service : Being the highest level, this group allocates and manages cluster resources, image registries, and image builds required for a service. Independent management is possible through the granting of usage privileges according to developer and administrator roles. In Cocktail, multiple services can be created as needed and assigned/managed by a team or service.\n Service Map : This unit configures/manages applications and corresponds to one workload. For example, a service can be configured via an application map that corresponds to development, validation, and operating systems, and the application map can be configured based on roles, such as data management and analysis systems. Logically, an application map can be considered a unit that manages multiple container configurations for workloads. The previously-described service consists of one or more application maps.\n Cocktail Server : This unit packages containers and associated orchestration objects. An application map consists of one or more Cocktail servers. For example, a server can be configured based on components such as web and DB servers, and small-scale microservices can also be configured via a Cocktail server. Cocktail servers are described in detail in the following section.\n  The following diagram shows how the service management layer is structured:\nCocktail Server\nA Cocktail server has the most important role in the service management layer. As previously described, a Cocktail server is a unit for packaging containers and associated orchestration objects. A Cocktail server manages the lifecycle (creation, update, etc.) of a packaged object and monitors and manages the status and resources in package units.\nThe following diagram describes this relationship:\nThe management features provided via a Cocktail server are as follows:\n Server creation, modification (update), termination, restart, deletion, auto-scaling\n Rolling update\n Service port management\n Volume management\n Monitoring\n Inspection : Orchestration object status and deployment info lookup\n Web terminal and log lookup : Container shell access; Container log lookup\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]