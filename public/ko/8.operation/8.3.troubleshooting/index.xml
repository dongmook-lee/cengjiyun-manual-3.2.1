<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>8.3 TroubleShooting on 칵테일 클라우드 메뉴얼</title>
    <link>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/</link>
    <description>Recent content in 8.3 TroubleShooting on 칵테일 클라우드 메뉴얼</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr-ko</language>
    <managingEditor>cloud.jung@acornsoft.io (cloud.jung)</managingEditor>
    <webMaster>cloud.jung@acornsoft.io (cloud.jung)</webMaster>
    
	<atom:link href="http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>8.3.1 k8s cluster의 health check</title>
      <link>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.1-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>cloud.jung@acornsoft.io (cloud.jung)</author>
      <guid>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.1-cluster/</guid>
      <description>Cube로 설치한 장비에서 Kubernetes와 칵테일의 주요 구성 요소에 대해 상태 체크를 하기 위한 방법은 아래와 같다.
이는 cube로 설치후 k8s 주요 구성 요소의 정상 동작 여부를 확인하는데 사용할 수 있다.
 cube status 명령을 통한 etcd, k8s pod, monitoring pod, 칵테일 pod 상태를 체크할 수 있다.  etcd daemon은 kubernetes에서 생성되는 각종 object의 데이터 저장소로서 HA환경일 경우 etcd cluster로 구성(최소 3대)된다. 아래 명령으로 etcd cluster leader 및 상태를 조회할 수 있다.</description>
    </item>
    
    <item>
      <title>8.3.2 사용자의 micro service가 서로 연결이 안될때.</title>
      <link>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.2-dns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>cloud.jung@acornsoft.io (cloud.jung)</author>
      <guid>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.2-dns/</guid>
      <description>일반적으로 k8s에서 각 Container들은 연관된 Container와 통신하기 이해 Kubernetes DNS를 사용한다. 즉 POD는 생성과 삭제가 일어나면 POD가 가지고 있던 내부 cluster ip는 변동되게 됨으로 일반적으로 micro service들을 k8s dns를 통해 필요한 container를 찾는다.
먼저 k8s의 DNS 서비스가 정상 동작 중인지 확인하는 방법은 busybox를 생성하고 컨테이너에 접속해서 nslookup명령으로 k8s의 DNS에 접속되는지 확인한다.
 busybox container 생성  아래는 busybox의 yaml 파일로 이를 busybox.yaml로 저장한다.
apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - image: busybox command: - sleep - &amp;quot;3600&amp;quot; imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always   마스터 서버에 ssh로 접속하여 kubectl 명령으로 busybox container를 생성한다.</description>
    </item>
    
    <item>
      <title>8.3.3 NFS 서버의 접속 확인 방법.</title>
      <link>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.2-nfs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>cloud.jung@acornsoft.io (cloud.jung)</author>
      <guid>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.2-nfs/</guid>
      <description>칵테일 서비스는 기본적으로 Storage class를 이용하여 persistence volume을 사용한다. local 및 baremetal 환경에서는 NFS 서버를 구축하여 Storage class로 활용하며 public cloud에서는 해당 서비스 제공자가 기본적으로 제공해 주는 storage class를 사용하게 된다.
본 문서는 local 및 Baremetal 환경에서 NFS 서버의 상태 및 칵테일 service와의 연결이 정상적인지 확인하는 방법을 기술한다.
 NFS 서버의 설치 및 기동 ( nfs server 주소가 192.168.0.175로 가정하고, cluster를 구성하는 node들이 192.168.0.0/24 대역에 위치한다고 가정함)  # yum install nfs-utils # mkdir -p /cocktail-nfs # vi /etc/exports /cocktail-nfs 192.</description>
    </item>
    
    <item>
      <title>8.3.4 Cube로 설치한 node의 ip 정보가 변경될 경우.</title>
      <link>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.4-nodeip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>cloud.jung@acornsoft.io (cloud.jung)</author>
      <guid>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.4-nodeip/</guid>
      <description>cube로 설치한 master node, worker node에서 ip 정보가 변경되면 현재로서는 cube를 이용하여 재설치하는 방식을 권장한다.
이는 k8s의 component중 api server, scheduler, control manager가 ip 인증서 기반으로 통신하며, etcd의 flannel subnetwork정보도 ip정보가 저장되기 때문이다.</description>
    </item>
    
    <item>
      <title>8.3.5 Centos yum 자동 upgrade 방지</title>
      <link>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.5-yum/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>cloud.jung@acornsoft.io (cloud.jung)</author>
      <guid>http://acornapps.github.io/cengjiyun-manual-3.2.1-public/ko/8.operation/8.3.troubleshooting/8.3.5-yum/</guid>
      <description>칵테일은 kubernetes package의 버전에 따라 설정파일의 차이가 있으므로, 시스템에서 yum upgrade가 자동으로 설정되어 있으면 시스템 업그레이드시에 칵테일 관련 설정이 override 될 수 있으므로 반드시 security 부분만 자동으로 업그레이드가 되도록 수정해야 한다.
만약 그렇지 않으면, 시스템의 재기동이 필요한 업그레이드나 칵테일 관련 설정파일이 override 되면서 서비스가 중단될 수 있다.
 yum-cron 설정 파일 수정.
# vi /etc/yum/yum-cron.conf ... update_cmd = security 로 수정 # vi /etc/yum/yum-cron-hourly.conf ... update_cmd = security 로 수정   참고) https://www.</description>
    </item>
    
  </channel>
</rss>