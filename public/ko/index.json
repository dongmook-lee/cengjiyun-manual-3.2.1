[
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/0.1/",
	"title": "Cocktail Cloud 구성 개요",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud는 그 역할에 따라 5개의 레이어로 구성된다.\n Cluster Management Layer: 컨테이너가 배포/실행되는 인프라(클러스터)와 오케스트레이션(Orchestration)을 담당하는 레이어다. 오케스트레이션은 Kubernetes(https://kubernetes.io)가 담당하고 인프라 관리, 모니터링 등 확장 관리 기능을 제공한다.\n Service Management Layer: 서비스(Workload)를 기준으로 컨테이너 구성과 관리를 담당하는 레이어다. 서비스를 구성하는 다수의 컨테이너와 관련 객체를 패키징하고 그 수명주기와 모니터링을 관리한다.\n Pipeline: 코드에서 컨테이너 빌드, 배포까지의 과정을 자동화하여 지속적인 통합/배포를 수행한다. 사용자는 원하는 파이프라인 구성을 설정과 스크립트를 통해 할 수 있다.\n Catalog: 공통으로 사용되는 런타임(DB, 미들웨어 등)템플릿을 제공하는 레이어다. 템플릿은 필요할 때 별도의 구성 작업 없이 바로 배포하여 사용 할 수 있다. 또한 사용자 어플리케이션의 스냅샷도 저장, 관리 할 수 있다.\n Dashboard: 클러스터, 서비스 현황과 모니터링 뷰를 제공 한다.\n  계속해서 각 레이어를 구체적으로 살펴 보자\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/",
	"title": "칵테일 클라우드 이해",
	"tags": [],
	"description": "",
	"content": " Cocktail Cloud란? Cocktail Cloud는 올인원 컨테이너 관리 플랫폼(All-in-one Container Management Platform)이다.\n클라우드 사용이 보편화 되면서, 인프라 뿐 아니라 어플리케이션, 서비스 관리에 대한 요구가 높아지고 있다. 과거와 같은 개발, 운영 방식으로는 클라우드의 장점을 활용하기에 한계가 있기 때문이다. 특히 어플리케이션 영역에서는 지속적인 통합 및 배포(Continuous Integration/Deploy, CI/CD), 마이그레이션(Migration), 멀티/하이브리드 클라우드 구축 등 자동화, 효율화, 통합 관리에 대한 요구가 증가 하고 있다.\n컨테이너 기술의 확산은 이러한 맥락에서 당연하다고 할 수 있다. 현재 많은 기업들이 컨테이너 기술을 도입하였고 그 추세는 계속 증가하고 있다. (참조: http://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100)\n컨테이너는 어플리케이션 또는 서비스를 독립되고 실행 가능한 단위로 패키지 화 하는 기술로, 인프라 환경에 관계 없이 동일한 개발, 운영 경험을 제공 한다. 따라서 인프라에서 서비스까지 클라우드 관리를 표준화하고, 개발 및 운영 노력을 절감 할 수 있다. 특히 일관된 환경하에 멀티/하이브리드 클라우드를 관리 할수 있다는 장점을 제공한다.\nCocktail Cloud는 컨테이너의 장점을 클라우드 관리에 적용하여, 개발 및 운영 업무를 효율화 하고 단일 또는 멀티/하이브리드 클라우드 전략 구현을 위한 플랫폼을 제공한다.\nCocktail Cloud의 주요 기능은 다음과 같다\n 코드로 부터 빌드, 배포, 업데이트까지의 파이프라인 자동화 워크로드(서비스)중심의 컨테이너 관리: 패키징, 수명주기, 자원 등 풀 스택 모니터링: 인프라에서 컨테이너까지의 상태 및 자원 모니터링. Alert 관리 멀티/하이브리드 클러스터 프로비져닝 및 관리: Baremetal, 프라이빗/퍼블릭 클라우드  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/",
	"title": "2.1 시스템",
	"tags": [],
	"description": "List the child pages of a page",
	"content": "시스템 이용에 필요한 시스템 정보, 계정 정보, 클라우드 미터링 계정, 시스템 사용자를 관리 할 수 있다.\n 좌측 메인메뉴 → 시스템     시스템 정보 설명     시스템 명 칵테일 클라우드의 시스템 이름   시스템 접속 계정 칵테일 클라우드 접속을 위한 시스템 계정   시스템 유형 칵테일 클라우드 시스템 사용 유형   기본 언어 칵테일 클라우드 기본 언어   시스템 로고 칵테일 클라우드 시스템 로고   시스템 설명 칵테일 클라우드 시스템 설명       계정 정보 설명     조직명 칵테일 클라우드 시스템 이용 조직명   계정 담당자 명 칵테일 클라우드 계정 관리 담당자 명   계정 담당자 이메일 칵테일 클라우드 계정 관리 담당자 이메일   주소 칵테일 클라우드 시스템 이용 조직 주소   라이센스 코드 칵테일 클라우드 라이센스 코드       클라우드 미터링 계정 설명     계정 이름 클라우드 미터링 계정 이름   프로바이더 클라우드 프로바이더 유형   설명 클라우드 미터링 계정 설명       시스템 사용자 설명     이름 시스템 사용자 이름   아이디 칵테일 클라우드 접속 사용자 아이디   권한 시스템 사용자 권한 (관리자, 사용자)   마지막 접속일 시스템 사용자 마지막 접속 시간   설명 시스템 사용자 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.1.kass/",
	"title": "8.1.1 클러스터 서비스 사용",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.1-cluster-startup/",
	"title": "8.2.1 Cube cluster startup &amp; shutdown",
	"tags": [],
	"description": "",
	"content": " Cube cluster의 startup과 shutdown은 일련의 순서대로 진행해야 한다.\nStartup Startup시에는 master node를 먼저 기동하고 worker node를 기동하는 순서로 진행한다.\n1) Master Node\n# systemctl start etcd # systemctl start docker # systemctl start kubelet  2) Worker Node\n# systemctl start docker # systemctl start kubelet  Shutdown Shutdown시에는 startup시와는 반대로 worker node를 먼저 종료하고 master node를 종료하는 순서로 진행한다.\n1) Worker Node\n# systemctl stop kubelet # systemctl stop docker # systemctl stop etcd  2) Master Node\n# systemctl stop kubelet # systemctl stop docker  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.1-init-command/",
	"title": "`init` command",
	"tags": [],
	"description": "",
	"content": " init는 Cube Cluster를 설치하려는 플랫폼에 맞는 cube.toml 을 다운로드한다.\nUsage cube init [flags]\nFlags provider  --provider, -p\n Cube Cluster를 설치할 플랫폼의 이름을 지정한다.\n 필수: 필수\n 현재 버전에서 지원하는 provider 목록은 아래와 같다.\n     flag 값 Provider     onpremise On-premise   aws Amazon Web Service   azure Microsoft Azure   gcp Google Compute Engine   aliyun AlibabaCloud    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/0.2/",
	"title": "Cluster Management Layer (클러스터 관리 레이어)",
	"tags": [],
	"description": "",
	"content": "클러스터 관리 레이어는 클러스터를 구성하는 인프라 스트럭처와 컨테이너 오케스트레이션 엔진 부분으로 구분된다. 상세 구성은 아래 그림과 같다.\nCocktail Cloud는 클러스터 인프라와 오케스트레이션 엔진을 사용자의 설정에 따라 자동 프로비져닝(Provisioning)하는데 이를 담당하는 도구가CUBE이다. CUBE는 고가용(High Availability, HA)의 Kubernetes클러스터를 구성한다. CUBE가 프로비져닝하는 클러스터 구성은 높은 안정성과 보안, 확장성을 제공한다.(이를 CUBE클러스터라 한다.)\nCUBE클러스터는 현재 다음과 같은 클라우드 플랫폼 및 공급자를 지원한다. 하지만 CUBE는 인프라에 독립적인 구성이 가능함으로 어떠한 인프라에서도 구성이 가능하다. 아래는 인프라 자동 프로비져닝을 기준으로 한 것이다.\n 물리 인프라(Baremetal)\n 클라우드 플랫폼 : Openstack, Cloudstack, VMWare\n 클라우드 서비스 : AWS, GCP, Azure\n  또한 CUBE도구는 클러스터의 Kubernetes 버전 업그레이드, 노드(물리/가상 머신)의 추가/삭제, 백업등의 클러스터 관리를 위한 기능을 제공한다.\nCocktail Cloud는 멀티 클러스터를 통합관리 한다. 즉, 하나 이상의 클러스터를 필요한 서비스에 할당하고 전체 클러스터의 관리 및 모니터링을 수행한다. 클러스터는 필요한 경우 추가 될 수 있다.\nCUBE클러스터에는 관리를 위한 확장 컴포넌트가 추가 제공된다.\n Monitoring : 인프라, 컨테이너, 서비스(Workload)의 상태, 구성, 자원 모니터링 Alerting : 특정 조건이 되었을 때 이메일, 메신져를 통한 경고 전파 및 관리 Metering : 퍼블릭 클라우드 클러스터인 경우 사용 비용 조회 Inspecting : 컨테이너 및 Kubernetes 오브젝트의 변경 추적 및 이벤트 관리  확장 컴포넌트가 제공하는 정보와 관리 기능은 Cocktail Cloud “클러스터 관리”뷰(View)를 통해 제공된다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.2.manual-install/",
	"title": "8.1.2 클러스터 직접 구성",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.2-cluster-certification/",
	"title": "8.2.2 Update cube cluster certificate",
	"tags": [],
	"description": "",
	"content": "Kubernetes를 구성하는 각 컴포넌트들도 내부 인증서를 통해 통신한다. 필요에 의해 이 인증서를 update해야 하는 경우 아래의 절차대로 수행한다.\n1.먼저 각 master node와 worker node의 기존 인증서를 삭제한다.\n# ssh to master node # rm -rf /opt/kubernetes/pki # rm -rf /etc/kubernetes/pki  2.인증서를 재 생성한다.\n이전에 설치를 진행한 디렉토리에서 cert.yaml을 실행하여 인증서를 update한다.\n먼저, openssl.conf에서 추가할 ip나 dns를 IP.N에 추가 기입한다.\n# vi cubescripts/roles/sslcert/openssl.conf.j2 [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.{{ cluster_name }} DNS.5 = {{ domain_name }} DNS.6 = *.{{ domain_name }} DNS.7 = localhost {% if cloud_provider == 'azure' or cloud_provider == 'aws' -%} DNS.8 = {{ lb_ip }} {% endif -%} IP.1 = 127.0.0.1 IP.2 = {{ kubernetes_service_ip }} {% if cloud_provider == 'baremetal' or cloud_provider == 'rovius' or cloud_provider == 'virtualbox' -%} IP.3 = {{ lb_ip }} IP.4 = 14.52.93.202 {% endif -%} # ansible-playbook -i inventories/inventory -u {userId} cert.yaml  3.k8s에 생성된 default token를 모두 삭제한다.\n# kubectl get secrets --all-namespaces | grep default-token | awk '{print $1, $2}' | xargs -n 2 kubectl delete secrets -n  4. Worker node -\u0026gt; Master Node 순으로 rebooting한다.\n# reboot  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.2-create-command/",
	"title": "`create` command",
	"tags": [],
	"description": "",
	"content": " create는 Cube Cluster를 init으로 준비한 플랫폼에 설치한다.\nPrerequisite init command\nUsage  cube create Cube Cluster를 설치한다.  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/",
	"title": "시스템 관리",
	"tags": [],
	"description": "",
	"content": "빌드 및 배포, 운영할 수 있는 워크스페이스를 할당한다. 워크스페이스 별로 사용자, 클러스터, 라이브러리 등록이 가능하다.\n 좌측 메인메뉴 → 워크스페이스    서비스 설정 설명     워크스페이스 명 워크스페이스 이름 (사용자지정)   클러스터 수 워크스페이스에 할당된 클러스터 수   구성원 수 워크스페이스를 사용하는 구성원 수   시스템 명 시스템 명   설명 워크스페이스 설명     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/0.3/",
	"title": "Service Management Layer (서비스 관리 레이어)",
	"tags": [],
	"description": "",
	"content": "일반적으로 하나의 어플리케이션 및 서비스는 다수의 컨테이너로 구성된다. 특히 Kubernetes와 같은 오케스트레이션 엔진의 경우 컨테이너 외에도 서비스 포트, 컨트롤러, 볼륨등의 추가적인 객체를 필요로 한다. 따라서 컨테이너 기반의 어플리케이션 및 서비스를 관리하기 위해서는 컨테이너와 관련 객체를 패키징하고 관리하는 추상화 단위가 필요하다. 서비스 관리 레이어는 이를 담당하는 레이어다.\nCocktail Cloud의 서비스 관리는 다음과 같은 계층 구조를 가진다.\n Service : 최상위 계층으로 서비스에 필요한 클러스터 자원, 이미지 레지스트리, 이미지 빌드를 할당 관리하는 그룹이다. 개발, 운영자의 역할에 따라 권한을 부여하여 독립적인 관리를 할 수 있다. Cocktail Cloud에서는 필요에 따라 다수의 서비스를 생성하고 팀 또는 서비스 별로 할당하여 관리 할 수 있다. Service Map : 어플리케이션의 구성과 관리를 하는 단위로 하나의 워크로드(Workload)에 대응 된다. 예를 들면 하나의 서비스가 개발계, 검증계, 운영계에 해당하는 어플리케이션 맵으로 구성될 수 있으며, 데이터 관리, 분석 시스템과 같이 역할을 기준으로 어플리케이션 맵을 구성 할 수도 있다. 논리적으로 어플리케이션 맵은 워크로드를 위한 다수의 컨테이너 구성을 관리하는 단위라 할 수 있다. 앞서 설명한 서비스는 하나 이상의 어플리케이션 맵으로 구성 된다. Cocktail Server : 컨테이너와 연관 오케스트레이션 객체를 패키징하는 단위이다. 어플리케이션 맵은 하나 이상의 청지 서버로 구성된다. 예를 들면 웹서버, DB서버등과 같은 컴포넌트를 기준으로 서버를 구성 할 수도 있고, 작은규모의 마이크로서비스도 Cocktail Cloud 서버로 구성할 수 있다. 층적운 서버는 다음에서 좀 더 자세히 살펴 보도록 하겠다.  아래 그림은 서비스 관리 레이어의 구성을 도식화 한 것이다.\nCocktail Server\n서비스 관리 레이어에서 가장 핵심적인 역할을 담당하는 것이 Cocktail Cloud 서버이다. 앞서 언급했듯이 Cocktail Cloud 서버는 컨테이너와 관련 오케스트레이션 객체를 패키징 하는 단위이다. Cocktail Cloud는 패키지 된 객체의 생성, 업데이트 등 라이프사이클을 관리하며 상태와 자원을 패키지 단위로 모니터링하고 관리한다.\n아래는 이를 도식화 한 그림이다.\nCocktail Cloud가 제공하는 관리기능에는 다음과 같은 것들이 있다.\n 워크로드의 생성, 수정(업데이트), 정지, 재기동, 삭제 오토 스케일링\n 롤링 업데이트\n 서비스 포트 관리\n 볼륨 관리\n 모니터링\n 인스펙팅 : 오케스트레이션 객체의 상태 및 배포 정보 조회\n 웹터미널, 로그 조회 : 컨테이너 쉘 접속. 컨테이너 로그 조회\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/",
	"title": "8.1.3 Cube Reference",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.3-cluster-etcd/",
	"title": "8.2.3 Add etcd member to existing etcd cluster",
	"tags": [],
	"description": "",
	"content": "Etcd cluster가 가동중인 상태에서 etcd member를 추가하는 방법에 대해 기술한다.\n etcd 인증서 디렉토리: /etc/kubernets/pki/etcd etcd 설정파일: /etc/etcd/etcd.conf  1.etcd ca 파일을 기반으로 추가하고자 하는 etcd node에서 etcd 인증서를 생성한다.\n# cube.toml에 추가할 etcd node 정보를 기입한다. etcd cluster는 반드시 홀수개의 node로 구성되어야 하며, 최대 5개로 구성하는 것을 권고한다. $ vi cube.toml ... [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, \u0026quot;xxx.xxx.xxx.xxx\u0026quot;, \u0026quot;yyy.yyy.yyy.yyy\u0026quot;, ] ... # cube create 명령을 사용하여 추가한 etcd node에 설치를 진행한다. $ cube create  2. 추가 후 etcd cluster member list 및 상태 확인\n$ cube status  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.3-status-command/",
	"title": "`status` command",
	"tags": [],
	"description": "",
	"content": " status는 Cube Cluster의 현재 상태를 출력한다.\nUsage cube status\nOutput  etcd 상태 node 상태 Kubernetes pod 상태 addon pod 상태  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/3.dashboard/",
	"title": "대시보드",
	"tags": [],
	"description": "",
	"content": "대시보드에서는 Cocktail Cloud에 서비스된 정보를 한 눈에 확인 할 수 있다.\n 요약정보  현재 등록 관리되고 있는 서비스 개수, 워크로드 개수, 등록된 클러스터 수, 현재 발생한 알람의 개수를 볼 수 있다.\n 자원현황  CPU, Memory의 전체량, 요청량, 사용량과 Storage(Node, Pv) 전체, 사용량을 확인 할 수 있다.\n 클러스터 현황  등록된 클러스터 별로 서비스 개수, 워크로드 개수를 확인 할 수 있다.\n 클러스터 비용 현황  사용 중인 클러스터의 비용을 확인 할 수 있다.\n 클러스터 자원 할당률  사용 중인 클러스터의 클러스터 자원 할당률(request)을 확인 할 수 있다.\n 서비스 자원 할당량  사용 중인 서비스 별로 자원(CPU/Memory) 할당량을 확인 할 수 있다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/0.4/",
	"title": "Pipeline Layer (파이프라인 레이어)",
	"tags": [],
	"description": "",
	"content": "파이프라인 레이어는 코드로부터 컨테이너 빌드와 배포를 자동화 한다.\nBuild는 코드 다운로드, 코드 빌드 및 테스트등 작업(Task), 컨테이너 이미지 빌드를 사용자 설정을 기준으로 순차적으로 수행한다. 작업(Task)은 내부에서 작업을 수행하는 컨테이너를 사용하는데, 기본 제공되는 컨테이너 외에도 공개 또는 사용자가 만든 컨테이너를 사용하여 확장 할 수 있다.\n빌드 된 이미지는 Cocktail Cloud 서버의 컨테이너로 롤링 업데이트 한다. 빌드를 통해 만들어진 이미지는 자동 빌드 번호가 부여 되는데 이를 통해 최신 버전 또는 롤백을 수행한다.\n빌드부터 배포까지의 전 작업 과정은 모두 파이프라인을 통해 자동화되어 있다. 따라서 한번의 빌드와 파이프라인이 구성되면 개발자는 코드의 작성 및 수정에만 집중 할 수 있다.\n아래 그림은 파이프라인 레이어의 구성을 도식화 한 것이다.\n파이프라인 레이어는 지속적인 통합과 배포(CI/CD)를 통해 DevOps를 위한 기반을 제공한다. DevOps는 개발과 운영간 연속성과 유기적인 협력을 강조한다. CI/CD는 DevOps에 필요한 구성 요소중 하나로 컨테이너 기반의 개발, 운영에서도 마찬가지로 매우 중요하다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.4-cluster-backup/",
	"title": "8.2.4 Cube cluster backup &amp; restore",
	"tags": [],
	"description": "",
	"content": "Cube cluster가 어떤 이유로 인해 재설치할 경우를 대비하여 kubernetes workload의 데이터베이스 역할을 수행하는 etcd 데이터의 백업은 반드시 주기적으로 수행되어야 한다. 또한, 칵테일이 설치되어 있는 경우에는 칵테일 데이터베이스도 주기적으로 백업하여 cluster장애로 인한 재설치시에 사용하도록 한다.\netcd 인증서는 반드시 기존 설치시에 사용된 인증서를 사용해야 하며, 그렇지 않을 경우 복원할 수 없다.\n etcd 인증서 디렉토리: /etc/kubernets/pki/etcd  ca.crt ca.key ca.srl healthcheck-client.crt healthcheck-client.key peer.crt peer.key server.crt server.key  etcd 설정파일: /etc/etcd/etcd.conf  1.etcd, 칵테일 database backup\n아래와 같이 etcd snapshot 생성, 칵테일 database backup을 위한 shell script를 자신의 환경에 맞게 수정한 후 backup을 주기적으로 실시한다.\n$ vi cocktail_backup.sh #!/bin/sh # usage: cocktail_backup.sh save_path days # ./cocktail-backup.sh /nas/BACKUP/ 10 export KUBECONFIG=/etc/kubernetes/admin.conf export ETCDCTL_API=3 ETCD_CERT=\u0026quot;/etc/kubernetes/pki/etcd/peer.crt\u0026quot; ETCD_KEY=\u0026quot;/etc/kubernetes/pki/etcd/peer.key\u0026quot; ETCD_CACERT=\u0026quot;/etc/kubernetes/pki/etcd/ca.crt\u0026quot; ETCD_EP=\u0026quot;https://192.168.0.221:2379,https://192.168.0.222:2379,https://192.168.0.223:2379\u0026quot; CURRENT_DATE=`date '+%Y%m%d'` CURRENT_TIME=`date '+%Y%m%d_%H%M%S'` ETCD_BACKDIR=\u0026quot;$1/etcd\u0026quot; COCKTAIL_BACKDIR=\u0026quot;$1/db\u0026quot; SOMAC_NS=\u0026quot;acloud-system\u0026quot; error_exit() { echo \u0026quot;error: ${1:-\u0026quot;unknown error\u0026quot;}\u0026quot; 1\u0026gt;\u0026amp;2 exit 1 } verify_prereqs() { echo \u0026quot;Verifying Prerequisites\u0026quot; if [ ! -d $ETCD_BACKDIR ]; then error_exit \u0026quot;Can't access etcd backup directory $ETCD_BACKDIR\u0026quot; fi if [ ! -d $COCKTAIL_BACKDIR ]; then error_exit \u0026quot;Can't access cmdb backup directory $ETCD_BACKDIR\u0026quot; fi acloud_cmdb_pod=`kubectl get pods -n $SOMAC_NS | grep api-cmdb | awk '{print $1}'` if [ -z $acloud_cmdb_pod ]; then echo \u0026quot;Can't get acloud cmdb pod name. exit.\u0026quot; exit 1; fi acloud_cmdb_pvc=`kubectl get pvc -n $SOMAC_NS | grep api-cmdb | awk '{print \u0026quot;acloud-system-\u0026quot;$1\u0026quot;-\u0026quot;$3}'` if [ -z $acloud_cmdb_pvc ]; then echo \u0026quot;Can't get acloud cmdb pvc name. exit.\u0026quot; exit 4; fi } main() { if [ \u0026quot;$#\u0026quot; -ne 2 ]; then echo \u0026quot;./cocktail-backup.sh /nas/BACKUP/ 10\u0026quot; error_exit \u0026quot;Illegal number of parameters. You must pass backup directory path and number of days to keep backups\u0026quot; fi verify_prereqs echo \u0026quot;Getting ready to backup to etcd($ETCD_BACKDIR), cmdb($COCKTAIL_BACKDIR)\u0026quot; kubectl exec \u0026quot;$acloud_cmdb_pod\u0026quot; -n $SOMAC_NS -- sh -c \u0026quot;cd /var/lib/mysql; /usr/bin/mysqldump --single-transaction --databases cocktail builder -u root -p [password] \u0026gt; acloud_cmdb_dump.sql\u0026quot; echo \u0026quot;acloud cmdb dump succeeded.\u0026quot; # etcd backup /bin/etcdctl --cert \u0026quot;$ETCD_CERT\u0026quot; --key \u0026quot;$ETCD_KEY\u0026quot; --cacert \u0026quot;$ETCD_CACERT\u0026quot; --endpoints=\u0026quot;$ETCD_EP\u0026quot; snapshot save \u0026quot;$ETCD_BACKDIR/etcd_$CURRENT_TIME\u0026quot; # mv cmdb dumpfiles to backup directory echo \u0026quot;mv /nas/shared/$acloud_cmdb_pvc/acloud_cmdb_dump.sql $COCKTAIL_BACKDIR/acloud_cmdb_dump.$CURRENT_TIME.sql\u0026quot; mv /nas/shared/\u0026quot;$acloud_cmdb_pvc\u0026quot;/acloud_cmdb_dump.sql \u0026quot;$COCKTAIL_BACKDIR\u0026quot;/acloud_cmdb_dump.\u0026quot;$CURRENT_TIME\u0026quot;.sql echo \u0026quot;find $ETCD_BACKDIR -name 'etcd*' -mtime +$2 | xargs rm -rf\u0026quot; find $ETCD_BACKDIR -name \u0026quot;etcd*\u0026quot; -mtime +$2 | xargs rm -rf echo \u0026quot;find $COCKTAIL_BACKDIR -name '*cmdb-pvc*' -mtime +$2 | xargs rm -rf\u0026quot; find $COCKTAIL_BACKDIR -name \u0026quot;*cmdb_dump*\u0026quot; -mtime +$2 | xargs rm -rf find /home/data/log -name \u0026quot;*\u0026quot; -mtime +30 | xargs rm -rf echo \u0026quot;Backup completed.\u0026quot; } main \u0026quot;${@:-}\u0026quot;  2.k8s cluster 재 설치\n기존 k8s cluster를 삭제하고 재설치 한다. 이때, etcd 인증서는 따로 백업받아서 설치시 해당 위치에 copy 한후 진행한다.\n$ cp -R /etc/kubernets/pki/etcd /tmp $ cube destroy Do you really want to destroy? Only 'yes' will be accepted to confirm: yes $ cp -R /tmp/etcd /etc/kubernets/pki/ $ cube create  3.etcd database restore.\n# etcdctl --cert /etc/kubernetes/pki/etcd-peer.crt --key /etc/kubernetes/pki/etcd-peer.key \\ --cacert /etc/kubernetes/pki/etcd-ca.crt --endpoints=https://xxx.xxx.xxx.xxx:2379 --name=master \\ --initial-advertise-peer-urls=\u0026quot;https://xxx.xxx.xxx.xxx:2380\u0026quot; --initial-cluster=\u0026quot;master=https://xxx.xxx.xxx.xxx:2380\u0026quot; \\ --initial-cluster-token=\u0026quot;etcd-k8-cluster\u0026quot; --data-dir=“/data/etcd” snapshot restore /root/backup/etcd_20180322  4. k8s resource 기동 확인\n# kubectl get pods --all-namespaces  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.4-login-command/",
	"title": "`login` command",
	"tags": [],
	"description": "",
	"content": " login는 설치된 Cluster의 Cocktail DashBoard 또는 Cocktail Online에 로그인 한다.\nPrerequisite Cube Cluster 설치 상태.create command\nUsage  cube login [flags]  Flags url  --url, -u\n 접속할 Cocktail DashBoard의 Url을 입력 한다.\n 필수: 필수\n  system  --system, -s\n System 정보를 입력 한다.\n 필수: 필수\n  id  --id, -i\n ID 정보를 입력 한다.\n 필수: 필수\n  password  --password, -p\n password 정보를 입력 한다.\n 필수: 필수\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/",
	"title": "서비스",
	"tags": [],
	"description": "",
	"content": "서비스 맵과 워크로드를 생성, 관리 하는 기능이다.    목록 설명      서비스 맵 그룹 액션메뉴    서비스 맵 생성 팝업 호출    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/0.5/",
	"title": "Catalog Layer (카탈로그 레이어)",
	"tags": [],
	"description": "",
	"content": "컨테이너는 그 구성을 이미지화하여 배포 실행한다. 따라서 자주 사용하는 DB나 미들웨어 등의 런타임은 컨테이너 이미지와 배포 정보를 템플릿화 하면 반복적인 작업을 줄이고 언제든 필요할 때 쉽게 배포하여 사용 할 수 있다.\n이러한 템플릿을 생성하고 관리를 담당하는 레이어가 카탈로그다. 카탈로그 레이어의 구성을 도식화하면 아래와 같다.\n카탈로그는 목적에 따라 두 가지 형태로 구분 된다.\n Service Catalog : 서비스 카탈로그는 사용자가 서비스를 위해 구성한 어플리케이션의 스냅샷을 저장 관리한다. 인프라 또는 클라우드 공급자의 교체 등으로 이전이 필요할 경우 서 비스 카탈로그의 스냅샷을 사용 할 수 있다. 스냅샷은 서비스 맵의 구성을 그대로 저장한다. 따라서 다른 환경으로 배포 할 경우 필요에 따라 스냅샷의 설정을 수정하여 배포하여야 한다. Platform Catalog : Cocktail Cloud에서 “Cocktail Cloud 허브”를 통해 제공되는 런타임 템플릿을 말한다. 사용자는 템플릿을 그대로 또는 자신의 환경에 맞게 설정을 변경하여 배포 할 수 있다. 현재 플렛폼 카탈로그는 DB, 미들웨어, 개발/운영 도구, 블로그, 머신러닝 등 널리 사용되는 오픈소스를 템플릿으로 제공하고 있으며, 지속적으로 업데이트하고 있다.  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.5-cluster-docker/",
	"title": "8.2.5 Docker container disk 관리",
	"tags": [],
	"description": "",
	"content": "Container 내에서 stdout, stderr로 출력을 보내면 docker overlay storage driver를 사용하는 경우 $DOCKERROOT/containers/{containerid}/containerid-json.log 파일에 저장되며 별다른 조치를 하지 않을 경우 계속 증가하게 된다. 이를 방지하기 위해 아래와 해당 디렉토리에 logrotation 설정파일을 추가한다. 아래 /data/docker 디렉토리가 $DOCKER_ROOT임.\n1./etc/logrotate.d/docker-container\n# Added by cube /data/docker/containers/*/*.log { daily rotate 4 compress size=10M missingok delaycompress copytruncate }   로테이트주기로 daily, weekly, monthly, yearly 를 선택할 수 있다. rotate: rotate되는 파일을 최대 4개까지만 유지하고 이전 파일을 삭제한다. compress : 백업파일을 gzip 압축한다. 압축하고 싶지 않다면 nocompress를 설정하면 된다. copytruncate : 대부분의 잘 만든 애플리케이션들은 로그파일을 새로 만들기 위한 시그널 처리 코드를 가지고 있다. 하지만 그렇지 않은 애플리케이션들도 있다. copytruncate를 이용하면 원본파일을 지우지 않고 truncate(파일 크기를 0으로 만든다) 한다. 애플리케이션은 파일을 잃어버리지 않으므로 계속 해서 로그를 쓸 수 있다. 다만 파일을 복사하고 truncate 하는 순간 로그를 잃어버릴 수도 있다. size : size 보다 로그파일이 클 경우 로테이션을 수행한다. M(메가), K(키로)단위로 설정 할 수 있다.  2. Docker Storage driver 구조.\nhttps://www.joinc.co.kr/w/man/12/docker/storage 참조.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.5-logout-command/",
	"title": "`logout` command",
	"tags": [],
	"description": "",
	"content": " logout는 설치된 Cluster의 Cocktail DashBoard 또는 Cocktail Online 서비스에서 로그아웃 한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nUsage  cube logout  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/",
	"title": "빌드 관리",
	"tags": [],
	"description": "",
	"content": "도커(Docker) 이미지에 대한 빌드 작업을 관리한다.\n어플리케이션을 다운로드, 소스 파일을 빌드, 이미지 파일을 생성하여 Harbor 레지스트리에 업로드한다.\n 좌측 메인 메뉴 → 빌드 를 클릭하여 해당 페이지로 이동한다.      목록 설명     + 빌드 생성 버튼   Filter 검색을 통해 필요한 빌드 조회   빌드명 빌드의 이름   이미지명 저장된 도커 이미지 이름   이미지 사이즈 저장된 도커 이미지의 크기. MB단위로 표기   최근 액션 빌드의 상태를 표시   상태 빌드의 실행 상태를 표시   날짜 빌드가 실행된 날짜를 표시    빌드 액션 메뉴(작업 설정 편집, 히스토리, 빌드 실행, 빌드 취소, 로그 보기, 작업 삭제)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/1.intro/0.6/",
	"title": "Dashboard Layer (대시보드 레이어)",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud의 대시보드 레이어는 서비스 현황, 모니터링, 클러스터 현황 등을 웹UI형태로 제공한다.\n대시보드는 사용자의 역할에 맞는 뷰를 권한에 따라 제공한다. Cocktail Cloud의 사용자 권한은 다음과 같다.\n Administrator(Admin) : 클러스터 관리를 수행하고, Cocktail Cloud의 모든 서비스에 대해 관리 권한을 가진다.\n DevOps : 할당된 하나 이상의 워크스페이스에 대해서만 관리 권한을 가진다. 워크스페이스 별 대시보드를 제공하며, 클러스터 관리는 할 수 없다.\n  Cocktail Cloud에서 제공하는 대시보드 뷰는 다음과 같다.\n 종합 대시보드 : 전체 서비스와 클러스터 현황을 종합하여 보여 주는 뷰\n 서비스 관리 뷰 : 서비스를 생성하고 배포와 모니터링, 운영을 위한 뷰\n 카탈로그 뷰 : 서비스 카탈로그와 플랫폼 카탈로그를 조회 및 배포 할 수 있는 뷰\n 클러스터 뷰 : 하나 이상의 클러스터를 통합하여 관리하고 모니터링 할 수 있는 뷰\n 환경 설정 : Cocktail Cloud 플랫폼의 설정 뷰\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.6-cluster-password/",
	"title": "8.2.6 칵테일 암호 관리",
	"tags": [],
	"description": "",
	"content": "1.칵테일 로그인 암호 관리.\n1) 칵테일 암호 관리 대상은 칵테일이 설치된 모든 cluster를 대상으로 한다.\n2) 암호는 \u0026ldquo;영문대소문자 + 특수기호(!@#$%^\u0026amp;*=+-_) + 숫자\u0026rdquo; 조합으로 최소 8자 이상 최대 24자리까지 생성 가능하다.\n3) 암호 변경 주기는 90일이며, 90일이 지나서 칵테일에 로그인할 경우 암호 변경 화면이 표시된다. 암호 변경화면에서 \u0026ldquo;다음에 변경하기\u0026rdquo;를 선택하면 기존 암호로 칵테일을 사용할 수 있지만, 재 로그인시마다 암호 변경화면이 표시된다.\n2.Image registry 로그인 암호\n1) 칵테일 설치 시에 칵테일 Private Hub를 설치한 경우에 admin의 암호는 외부 노출이 되지 않도록 운영자가 잘 관리해야 한다. 왜냐하면, 해당 admin의 암호는 칵테일 api server에서 환경변수로 등록하여 registry 정상 접속여부 확인, project 등록, 사용자 등록, project에 사용자 mapping, 이미지 정보 조회 기능 등의 기능 을 수행함으로 이를 변경하면 api server를 재 기동해야 한다.\n2) 칵테일 Private Hub의 암호 변경시에는 각 master/worker node에서 docker login을 변경된 암호로 다시 로그인 해야 한다.\n즉, 아래 그림에서와 같이 cluster node에서 칵테일 Hub에 Harbor Registry로 docker login을 실행하면 해당 로그인 정보가 각 노드의 /root/.docker/config.json파일로 저장하게 됨으로 이를 갱신하기 위해서는 변경된 암호로 docker login을 다시 실행해야 한다. 3) 이에 따라, 부득이 암호를 변경할 경우에는 사전에 메일을 통해 담당 서비스팀으로 요청하여 안내를 받도록 해야 한다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.6-cluster-register-command/",
	"title": "`cluster register` command",
	"tags": [],
	"description": "",
	"content": " cluster register는 설치된 Cube Cluster를 Cocktail DashBoard 또는 Cocktail Online에 등록 할 때 사용한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nUsage  cube cluster register  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/",
	"title": "카탈로그",
	"tags": [],
	"description": "",
	"content": "서비스 구성을 템플릿화하여 쉽게 워크로드 생성 및 배포, 관리가 가능한 기능이다.\n여러 워크로드들을 한 번에 묶어 사용하고자 하는 경우 카탈로그 기능을 사용한다.\n카탈로그는 서비스 카탈로그와 플랫폼 카탈로그로 구분된다.\n   카탈로그 메뉴 설명     서비스 카탈로그 사용자가 생성한 카탈로그   플렛폼 카탈로그 Cocktail Cloud 기본 제공 공식 카탈로그   Filter Catalog 검색을 통해 필요한 템플릿 조회   템플릿 서비스를 구성하고 있는 워크로드 모음   배포 카탈로그를 통해 워크로드를 배포   편집 카탈로그 내용을 편집    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.7-cluster-cronjob/",
	"title": "8.2.7 캐시 메모리 증가에 따른 클러스터 노드 메모리 최적화 방법",
	"tags": [],
	"description": "",
	"content": "클러스터를 구성하는 각 node는 cache memory가 점차적으로 증가하게 되어 있다.\n주기적으로 각 node에서 아래 명령을 수행함으로써 cache memory를 초기화 할 수 있다.\n# sync; echo 3 \u0026gt; /proc/sys/vm/drop_caches 또는 # sync; sysctl -w vm.drop_caches=3  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.7-cluster-list-command/",
	"title": "`cluster list` command",
	"tags": [],
	"description": "",
	"content": " cluster list는 Cocktail DashBoard 또는 Cocktail Online에 등록된 Cube Cluster를 정보를 표시한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nUsage  cube cluster list  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/",
	"title": "클러스터",
	"tags": [],
	"description": "",
	"content": "클러스터의 등록관리와 노드와 서비스 단위의 모니터링, 볼륨, 알람, 미터링 서비스가 제공된다.   클러스터 메뉴 설명     노드 전체 노드의 CPU, Memory, Network 사용량 및 개별 노드의 모니터링 서비스   서비스 전체 서비스의 CPU, Memory, Network 사용량 및 개별 서비스의 모니터링 서비스   볼륨 스토리지 및 볼륨 현황   알람 클러스터의 이벤트 정보 관련 알람 서비스   미터링 클러스터 비용과 리소스별 비용 서비스   등록관리 클러스터 정보 조회    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/8.2.8-storage-volume-scaleup/",
	"title": "8.2.8 칵테일 클라우드에서 스토리지 볼륨 스케일업 방법",
	"tags": [],
	"description": "",
	"content": "워크로드가 스토리지 볼륨(Persistent Volume)을 사용 중, 용량이 부족 할 경우 스토리지의 용량을 확장 할 경우가 있다. 이 경우 칵테일에서는 아래와 같이 조치하면 된다.\n쿠버네티스에서는 몇몇 클라우드 프로바이더의 스토리지 클래스를 통해 PVC의 요청량을 늘리면 PV 및 해 당 스토리지의 용량이 자동 확장 된다. 이 때 PV는 Capacity 량이 조정 되며, 연결된 스토리지는 프로비져 닝을 통해 스케일 업 된다. 단, 이렇게 조정된 요청량이 Pod에 반영되기 위해서는 마운트 한 Pod를 재 시작 해주어야 한다.\n칵테일 클라우드는 서비스 맵의 스토리지 볼륨 화면에서 해당 볼륨의 요청량을 수정하면 쿠버네티스와 같은 방식으로 볼륨을 확장 할 수 있다. 단, 해당 볼륨을 마운트 한 워크로드는 재 시작 해주어야 한다.\n용량 확장이 가능하게 스토리지 클래스가 지원되는 프로바이터 스토리지 이외에는 스토리지 자체를 확장하고 위와 같은 작업을 수행해 주어야 한다. 대표적인 예가 NFS 스토리지 이다.\n칵테일 클라우드에서 NFS 플러그인 타입(NFS, NFS Named)의 스토리지는 스토리지 클래스를 통해 해당 스토리지가 확장될 수 없기 때문에, 스토리지 확장 부터 해주어야 한다.\nNFS 스토리지의 확장은 다음 명령어를 통해 수행 한다.\n# NFS 스토리지가 운영되는 호스트에 로그인 한 후 # 스토리지 디바이스 이름을 조회 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0020G0disk └─sda1 8:1 0 20G 0 part / sdb 8:16 0 100G 0 disk /storage # sda 디바이스의 용량을 확장 $ xfs_growfs /dev/sdb -D 40G  위 명령어로 스토리지 디바이스의 용량을 확장한 후에는 칵테일 클라우드의 클러스터 메뉴의 스토리지 화면에서 아래와 같이 스토리지 정보 수정을 해주어야 한다 * NFS 플러그인 타입의 스토리지가 확장된 경우, 스토리지의 총 용량 값 수정 * NFS Named 타입인 경우, 용량 값 수정\n다음 서비스 맵에서 볼륨의 요청량 정보를 수정 하면 워크로드를 확장된 스토리지 용량을 사용 할 수 있다. NFS인 경우는 워크로드를 재 시작 할 필요가 없다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.8-cluster-update-command/",
	"title": "`cluster update` command",
	"tags": [],
	"description": "",
	"content": " cluster update는 노드의 개수를 늘리거나 줄일 때 사용한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nCocktail DashBoard 또는 Cocktail Online에 Cluster가 등록된 상태. register command\nUsage  cube cluster update  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/",
	"title": "시스템 운영",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.9-cluster-upgrade-command/",
	"title": "`cluster upgrade` command",
	"tags": [],
	"description": "",
	"content": " cluster upgrade는 설치된 Cube Cluster를 upgrade한다. **현재 지원가능한 버전은 1.13.1 버전에서 1.13.2,1.13.3 버전으로의 upgrade를 지원한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nCocktail DashBoard 또는 Cocktail Online에 Cluster가 등록된 상태. register command\nUsage  cube cluster upgrade\n 현재 버전에서 지원하는 version 목록은 아래와 같다.\n     version     1.13.1   1.13.2   1.13.3    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/9.member/",
	"title": "구성원",
	"tags": [],
	"description": "",
	"content": "현재 워크스페이스에 속한 구성원들 정보를 보여준다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.10-cluster-getconfig-command/",
	"title": "`cluster getconfig` command",
	"tags": [],
	"description": "",
	"content": " cluster getconfig는 Cocktail DashBoard 또는 Cocktail Online에 등록된 Cube Cluster의 설치 정보 파일을 다운로드한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nCocktail DashBoard 또는 Cocktail Online에 Cluster가 등록된 상태. register command\nUsage  cube cluster getconfig [flags]  Flags clusterId  --clusterId, -c\n 설치 정보 파일을 가져올 cluster id 를 입력한다.\n 필수: 필수\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/10.terminology/",
	"title": "용어설명",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud 활용의 이해를 돕기 위한 Kubernetes 및 청지원 용어 정리이다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.11-cluster-delete-command/",
	"title": "`cluster delete` command",
	"tags": [],
	"description": "",
	"content": " cluster delete는 Cocktail DashBoard 또는 Cocktail Online에 등록된 Cube Cluster를 삭제한다.\nPrerequisite Cube Cluster 설치 상태.create command\nCocktail DashBoard 또는 Cocktail Online에 로그인된 상태. login command\nCocktail DashBoard 또는 Cocktail Online에 Cluster가 등록된 상태. register command\nUsage  cube cluster delete [flags]  Flags clusterId  --clusterId, -c\n Cocktail DashBoard 또는 Cocktail Online에서 삭제할 cluster id 를 입력한다.\n 필수: 필수\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/11.support/",
	"title": "고객지원 서비스",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud을 이용해 주셔서 감사합니다.\n제품 구매 기능상담 등의 기술 지원 서비스 및 각종 혜택을 받을 수 있습니다. 제공하는 각종 서비스는 다음과 같습니다.\n 방문 서비스를 통하여 Q\u0026amp;A, 제품에 관한 정보 및 기타 문의를 할 수 있습니다. 고객 등록을 마친 제품과 상담이 가능한 제품에 한하여 전화나 팩스를 통하여 전문 기술상담 서비스를 받을 수 있습니다. 각종 행사 초청 및 정보를 제공합니다.  서비스 지원\n이메일 : byoungoh.lee@namutech.co.kr\n제품 개발사\n아콘소프트(주)\n 주소 : 서울특별시 강남구 역삼로 239 화광센터 4층 TEL : 02-554-0301 FAX : 02-554-0302  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.3.cubecli/8.1.3.12-destroy-command/",
	"title": "`destroy` command",
	"tags": [],
	"description": "",
	"content": " destroy는 서버 인스턴스에서 Cube Cluster를 제거 한다.\nPrerequisite Cube Cluster 설치 상태.create command\nUsage  cube destory  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/",
	"title": "칵테일 클라우드 메뉴얼",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/10.terminology/10.1-kubenetes/",
	"title": "10.1 Kubernetes(k8s)",
	"tags": [],
	"description": "",
	"content": " Cluster\n   용어 설명     Namespace k8s의 가상 클러스터로 사용자들이 여러팀 또는 프로젝트로 분산되어 작업 할 수 있는 별도의 환경   Nodes k8s의 클러스터링되어 있는 어플리케이션을 구동하는 물리서버 또는 VM   Persistent Volumes(PV) 외부 저장소. NFS, iSCSI, 클라우드에서 제공하는 스토리지 시스템을 지원   Roles 사용의 따라 권한을 부여.   Storage Classes PV를 동적으로 Provisioning 할 경우, NFS 서버를 식별하기 위한 식별자    Workloads\n   용어 설명     Cron Jobs \u0026lsquo;특정 시점에서 한 번\u0026rsquo; 또는 \u0026lsquo;특정 시점에서 반복\u0026rsquo;과 같이 시간 기반의 관리 작업   Deployments Pod 생성을 위한 설정값의 등록 정보   Jobs Job은 일괄처리를 수행하는 Pod의 Supervisor. 즉, 특정한 계산 또는 백업과 같이 특정 시간 동안만 실행되는 프로세스.   Pods 1개 이상의 컨테이너로 구성되는 k8s에서 deploy를 위한 최소단위. Pod는 어플리케이션 스택에서 서로 다른 Docker image를 혼합하여 구성   Replica Sets 지정된 Pod 복제본이 항상 실행(유지)되도록 한다.   Replication Controllers 지정된 수의 Pod 복제본이 실행되고 있는지 체크   Stateful Sets Pod의 배포 및 확장을 관리하고, Pod의 순서와 특성에 대한 설정을 제공    Discovery and load balancing\n   용어 설명     Ingresses 외부접근에 대한 요청을 name-based로 가상 Hosting을 제공   Services 여러 컨테이너를 하나의 논리적인 단위로 그룹화하여, 외부에서 접속할 수 있는 단일 EndPoint를 제공    config and storage\n   용어 설명     Config Maps 데이터를 저장하여 사용할 수 있게 지원   Persistent Volume Claime(PVC) 사용자가 PV에 대한 요청. Pod는 PVC를 통하여 리소스(CPU 및 메모리)를 요청할 수 있고, 저장공간의 사이즈 및 접근 권한(rw, readOnly \u0026hellip;)을 요청할 수 있다.   Secrets Password 또는 OAuth 토큰 및 ssh key 등의 저장소.    settings\nK8s의Global Settings값을 조정 할 수 있습니다.\n   옵션 설명     Cluster name Kubernetes Cluster의 이름값을 조정할 수 있다.   Items per page 1페이지에 나타낼 수 있는 Items의 값을 조정할 수 있다.   Auto-refresh time interval Log를 자동으로 새로고침하는 시간을 조정할 수 있다.     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/10.terminology/10.2-system/",
	"title": "10.2 Cocktail Cloud",
	"tags": [],
	"description": "",
	"content": " 환경설정\nCocktail Cloud을 이용하기 위해서 사전에 정보를 등록하는 서비스.\n   용어 설명     사용자 Cocktail Cloud을 이용하기 위한 사용자 관리를 지원   계정 k8s Cluster에 대한 사용자 인증 정보에 대한 관리를 지원   서비스 업무 또는 프로젝트 단위로 작업을 분산하여 관리. 관리자는 일반사용자에 대하여 서비스 단위로 접근권한을 부여할 수 있다.    C.클러스터 : k8s를 사용하기 위한 k8s정보를 관리하는 페이지\n 서비스\n서비스 단위로 작업을 세분화가 가능하며, 빌드를 통한 Docker 이미지를 생성 할 수 있다. 빌드된 이미지를 통하여 생성, 조회, 수정, 삭제등 관리가 이루어지는 서비스.\n   용어 설명     서비스 맵 k8s의 Namespace   빌드 Docker image를 만들기 위한 작업을 지원하는 서비스   모니터링 서비스 안에서 워크로드를 Pod, Container 단위의 자원 모니터링을 제공   파이프라인 빌드부터 배포까지 한 번에 편리한 실행이 가능한 서비스   설정 서비스 단위로 변수값 또는 파일들을 저장하여 사용할 수 있는 서비스    카탈로그\n워크로드 구성을 템플릿화 하여 쉽게 워크로드생성 및 배포, 편집 관리가 가능한 서비스.\n 클러스터\n클러스터, 노드, 워크로드, 볼륨/스토리지, 알람, 미터링 등을 등록관리 및 현황 조회가 가능한 서비스\n   용어 설명     클러스터 k8s 오케스트레이션을 구성하는 서버의 패키지   노드 k8s 클러스터의 서버의 개별 서버. Master와 Worker로 구분   워크로드 1개 이상의 컨테이너로 구성되는 서비스   볼륨/스토리지 외부 스토리지 정보   알람 클러스터에 대한 이벤트 알람   미터링 클러스터에 대한 요금정보     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/10.terminology/10.3-error-code/",
	"title": "10.3 Cocktail Cloud System Error Code",
	"tags": [],
	"description": "",
	"content": " 1. 에러 코드    코드 메시지     CCCM001 구체적 오류 번호를 밝힐 수 없음.   CCCM002 알수 없는 오류 입니다   CCCM004 잘못된 함수 호출 인자입니다   CCCM005 인증되지 않은 요청입니다   CCCM006 전달된 정보가 올바르지 않습니다   CCCM007 서버, 작업 등이 올바르지 않거나 요청을 처리할 수 없습니다   CCCM019 내부 서버 오류   CCCM020 외부 호출 API가 오류를 반환 또는 호출이 실패 했습니다   CCCM023 수행 할 작업이 없습니다   CCCM026 오류가 발생하였습니다.   CCCM027 등록 중 오류가 발생하였습니다.   CCCM028 수정 중 오류가 발생하였습니다.   CCCM029 삭제 중 오류가 발생하였습니다.   CCCM030 조회 중 오류가 발생하였습니다.   CDCM031 데이터 베이스 작업 중 오류가 발생했습니다. 잠시 후 다시 시도 해 주세요. 문제가 지속될 경우 관리자에게 문의 해 주세요.   CDCM032 데이터 베이스 연결 중 오류가 발생했습니다. 잠시 후 다시 시도 해 주세요. 가능한 경우 데이터 베이스 연결 상태를 점검하거나 관리자에게 문의 해 주세요.   CKSY001 Cube Cluster API 오류가 발생하였습니다.   CKSY002 Cube Cluster의 버전이 Cocktail Cloud에서 지원하지 않습니다.   CKSY003 암호화 중 오류가 발생하였습니다.   CCUS001 사용자 ID가 등록되어 있지 않습니다   CCUS002 사용자 암호가 맞지 않습니다   CCUS003 지정한 사용자 권한이 사용자에게 할당되어 있지 않습니다   CCUS004 사용자가 이미 등록되어 있습니다   CCUS005 사용 중지된 계정입니다   CCUS006 Root Admin 사용자는 조작할 수 없습니다   CCAC001 프로바이더 credential의 형식이 올바르지 않습니다   CCAC002 프로바이더 credential이 없습니다   CCAC003 프로바이더 access key 또는 secret key가 올바르지 않습니다   CCAC004 새로 입력하려는 프로바이더 키 값이 이전과 같지 않습니다   CCAC005 프로바이더를 찾을 수 없습니다   CCAC006 해당 프로바이더를 사용하는 클러스터가 존재합니다   CCCT001 클러스터 접속 계정이 올바르지 않습니다   CCCT002 해당 마스터 URL로 등록된 클러스터가 이미 존재합니다   CCCT003 클러스터 인증서가 올바르지 않습니다   CCCT005 클러스터가 서버를 포함하고 있어 클러스터를 수정 또는 삭제할 수 없습니다   CCCT006 클러스터 현황 조회 중 오류가 발생하였습니다.   CCCT007 어플리케이션 현황 조회 중 오류가 발생하였습니다.   CCCT008 볼륨 현황 조회 중 오류가 발생하였습니다.   CCGP001 그룹이 서버를 포함하고 있어 클러스터를 삭제할 수 없습니다   CCGP002 그룹명이 중복되었습니다   CCSV001 서비스에 클러스터가 등록되어 있습니다   CCSV002 어플리케이션 맵이 클러스터를 사용 중 입니다   CCBD001 빌드 Job 생성 오류   CCBD002 빌드 Job 삭제 오류   CCBD003 이미 레지스트리에 등록된 이미지명입니다 다른 이미지명을 입력하시기 바랍니다   CCBD004 이미 등록된 이미지입니다   CCBD005 빌드 서버 접속이 원활하지 않습니다   CCBD006 빌드 조회 중 오류가 발생하였습니다.   CCBD007 빌드 생성 중 오류가 발생하였습니다.   CCBD008 빌드 수정 중 오류가 발생하였습니다.   CCBD009 빌드 삭제 중 오류가 발생하였습니다.   CCBD010 빌드 히스토리 조회 중 오류가 발생하였습니다.   CCCV001 클러스터 볼륨의 이름이 없거나 올바르지 않습니다   CCCV002 클러스터 볼륨의 스토리지 클래스 이름이 없거나 올바르지 않습니다   CCCV003 클러스터 볼륨의 플러그인 이름이 없습니다   CCCV004 클러스터 볼륨의 리클레임 정책 이름이 없습니다   CCCV005 클러스터 볼륨의 클러스터 일련번호가 없습니다   CCCV006 클러스터 볼륨을 찾을 수 없습니다   CCCV007 동일한 설정의 볼륨이 이미 존재합니다   CCCV008 클러스터 볼륨 유형을 찾을 수 없습니다   CCCV009 클러스터 볼륨이 사용중이어서 삭제 할 수 없습니다   CCCV010 파라미터는 필수 입니다   CCCV011 동일한 설정의 Static 볼륨이 이미 존재합니다   CCCV012 해당 클러스터 볼륨은 고정 클러스터 볼륨이 아닙니다   CKCF001 K8S ConfigMap 생성 실패   CKCF002 지정한 K8S ConfigMap을 찾을 수 없습니다   CCCF003 컨피그맵의 이름이 없거나 올바르지 않습니다   CCCF004 컨피그맵의 키가 올바르지 않습니다   CCCF005 추가하려는 컨피그맵의 이름이 이미 존재합니다   CCCF006 컨피그맵 데이터가 올바르지 않습니다   CKCF007 컨피그맵을 사용하는 서버가 존재합니다   CKST001 K8S Secret 생성 실패   CCST002 지정한 K8S Secret을 찾을 수 없습니다   CCST003 Secret의 이름이 없거나 올바르지 않습니다   CCST004 시크릿 데이터가 올바르지 않습니다   CCST005 추가하려는 시크릿의 이름이 존재합니다   CCST006 시크릿을 사용하는 서버가 존재합니다   CCCL001 작업 대상 카탈로그의 일련번호가 없습니다   CCCL002 카탈로그를 적용할 기존 어플리케이션 맵의 일련 번호가 없습니다   CCCL003 동일한 카탈로그의 이름이 이미 존재합니다   CCCL004 동일한 카탈로그의 버전이 이미 존재합니다   CCCL005 카탈로그 등록 중 오류가 발생했습니다   CCCL006 카탈로그 삭제 중 오류가 발생했습니다   CCCL007 카탈로그 수정 중 오류가 발생했습니다   CCCL008 카탈로그 배포 중 오류가 발생했습니다   CCCL010 카탈로그가 사용하는 레지스트리를 찾을 수 없습니다   CCCL011 카탈로그가 사용하는 클러스터 볼륨을 찾을 수 없습니다   CCCL012 등록할 카탈로그의 이름이 없거나 올바르지 않습니다   CCCL013 카탈로그를 적용할 새 어플리케이션 맵의 이름이 없거나 올바르지 않습니다   CCCL015 새 네임스페이스의 이름이 없거나 올바르지 않습니다   CCSR001 실행하려는 Job 타입이 올바르지 않습니다   CCSR002 task가 실행 중이라 새로운 task를 실행할 수 없습니다   CCSR003 서버가 지정한 동작을 실행할 수 없는 상태입니다   CCSR004 현재 상태에서 실행할 수 없는 액션입니다   CCSR006 실행할 업무(task)가 존재하지 않습니다   CCSR007 사용 가능한 메모리 양을 초과하였습니다   CCSR008 사용 가능한 CPU 양을 초과하였습니다   CCSR009 사용 가능한 Pod 수를 초과하였습니다   CCSR010 서버 생성 준비 중 오류가 발생했습니다   CCSR011 서버를 수정할 수 없는 상태입니다   CCSR012 서버 수정 준비 중 오류가 발생했습니다   CCSR013 서버 종료 준비 중 오류가 발생했습니다   CCSR014 서버 재시작 준비 중 오류가 발생했습니다   CCSR015 이미 사용 중인 이름입니다   CCSR016 조회하려는 리소스의 갯수가 없습니다   CCSR017 알 수 없는 리소스 타입입니다   CCSR018 지원하지 않는 서버 형식입니다   CCSR019 지원하지 않는 볼륨 플러그인 입니다   CKSR020 K8S Deployment 생성을 기다리던 중 timeout이 발생했습니다   CKSR021 K8S Pod 생성을 기다리던 중 timeout이 발생했습니다   CKSR022 K8S Deployment 삭제 실패했습니다   CKSR023 K8S Replicatset 삭제 실패했습니다   CKSR024 K8S Service 또는 Load Balancer 생성을 기다리던 중 timeout이 발생했습니다   CCSR025 K8S Pod를 찾을 수 없습니다   CKSR026 K8S Object 상태 조회 결과에 status가 없습니다   CKSR027 K8S PersistentVolume 생성 실패 했습니다   CKSR028 K8S PersistentVolume을 찾을 수 없습니다   CKSR029 K8S PersistentVolumeClaim 생성 실패 했습니다   CKSR030 K8S PersistentVolumeClaim 생성 중 timeout이 발생했습니다   CKSR031 지정한 K8S Namespace를 찾을 수 없습니다   CKSR032 K8S Deployment 생성 실패 했습니다   CKSR033 K8S Service 생성 실패 했습니다   CKSR034 K8S Ingress 생성 실패 했습니다   CKSR035 K8S HorizontalPodAutoscaler 생성 실패 했습니다   CCSR036 같은 이름을 가진 볼륨이 이미 존재합니다   CCSR037 지정한 서버를 찾을 수 없습니다   CCSR038 서버가 Cube 타입이 아닙니다   CKSR039 서버 설정 수정에 실패 했습니다   CCSR040 서버 생성/수정 전 k8s 자원 검사에 실패 했습니다   CKSR042 K8S Deployment 정보를 찾을 수 없습니다   CCSR043 서버타입 변경 불가   CCSR044 서버 컨테이너가 없습니다   CCSR045 호스트 포트 중복입니다   CCSR046 인그레스 경로가 이미 사용중입니다   CCSR047 K8S의 로그 타입이 아닙니다   CCSR048 준비상태에서 서버 삭제 실패   CCSR050 노드 포트가 지정 범위를 벗어났습니다   CCSR051 노드포트 중복   CCSR053 볼륨을 사용 할 수 없는 상태여서 재시작을 실행할수 없습니다   CCSR054 컨테이너명이 중복되었습니다   CCSR055 네임스페이스명이 중복되었습니다   CCSR056 서버 중지 가능한 상태가 아닙니다.   CCSR057 서버 시작 가능한 상태가 아닙니다.   CCSR058 서버 재시작 가능한 상태가 아닙니다.   CCSR059 서버 삭제 가능한 상태가 아닙니다.   CKSR062 Cube Cluster 정보 조회 중 오류가 발생하였습니다.   CKSR063 Cube Cluster 정보 생성 중 오류가 발생하였습니다.   CKSR064 Cube Cluster 정보 수정 중 오류가 발생하였습니다.   CKSR065 Cube Cluster 정보 삭제 중 오류가 발생하였습니다.   CCSR066 서버 포트가 올바르지 않습니다. ( 범위 : 1 ~ 65535 ).   CCSR067 서버 포트 범위가 올바르지 않습니다.   CCSR068 서버 포트 범위 형식이 올바르지 않습니다. ( e.g. 8080-8082 )   CCSR069 서버 타겟 포트와 포트의 범위가 동일하지 않습니다. ( e.g. 8080-8082, 9090-9092 )   CKSR070 Cube Cluster의 클러스터 리소스 제한량 조회 중 오류가 발생하였습니다.   CCSR071 서비스가 존재하지 않습니다.   CCSR072 서버 실행 중 오류가 발생하였습니다.   CCSR073 서버 볼륨 설정이 올바르지 않습니다. 다시 확인 해주세요.   CCSR074 Port Range는 지정 노드 포트를 설정하실 수 없습니다.   CCSR075 지정 노드포트 값이 없습니다.   CCSR076 서버 제거 준비 중 오류 발생.   CCPL001 서버가 실행중 상태가 아니어서 파이프라인을 실행할 수 없습니다   CCPL002 파이프라인이 실행중 상태여서 다른 동작을 수행할 수 없습니다   CCPL003 지정한 파이프라인을 찾을 수 없습니다   CCPL004 파이프라인 생성 실패   CCPL005 파이프라인 수정 실패   CCPL006 파이프라인 실행 중 동일한 빌드 작업이 존재하여 실행할 수 없습니다. 동일한 빌드를 포함하는 파이프라인은 각각 실행하여 주세요.   CCPL007 파이프라인 실행 중 오류가 발생하였습니다.   CCRG001 레지스트리에서 이미지 목록을 받을 수 없습니다   CCRG002 레지스트리에 로그인 할 수 없습니다   CCRG003 레지스트리에서 이미지 태그 목록을 받을 수 없습니다   CCRG004 레지스트리에 사용자를 등록 할 수 없습니다   CCRG005 레지스트리에 로그인하려는 사용자가 등록되어 있지 않습니다   CCRG006 레지스트리 사용자의 암호를 수정 할 수 없습니다   CCRG008 레지스트리에 등록된 사용자를 프로젝트에 할당 할 수 없습니다   CCRG009 레지스트리에 등록된 사용자를 프로젝트에서 제외 할 수 없습니다   CCRG010 레지스트리에 프로젝트를 등록할 수 없습니다   CCRG011 레지스트리에 프로젝트가 이미 존재 합니다   CCRG012 레지스트리에 프로젝트를 삭제할 수 없습니다    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.1-edit/",
	"title": "2.1.1 시스템 편집",
	"tags": [],
	"description": "",
	"content": "시스템 이용에 필요한 시스템 정보, 계정 정보를 수정 할 수 있다.\n 좌측 메인메뉴 → 시스템 → 우측 상단 편집 버튼 시스템 정보(기본 언어, 시스템 로고, 시스템 설명)와 계정 정보(조직명, 계정 담당자 명, 계정 담당자 이메일, 주소)를 수정 할 수 있다.\n 좌측 메인메뉴 → 시스템 → 편집 → 수정완료 후 수정버튼을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.2-meteringadd/",
	"title": "2.1.2 클라우드 미터링 계정 등록",
	"tags": [],
	"description": "",
	"content": "클라우드 미터링 계정 등록 할 수 있다.\n 좌측 메인메뉴 → 시스템 → 계정 등록    Amazon Web Service  Google Cloud Plalform   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.3-meteringedit/",
	"title": "2.1.3 클라우드 미터링 계정 편집",
	"tags": [],
	"description": "",
	"content": "클라우드 미터링 계정 편집 할 수 있다.\n 좌측 메인메뉴 → 시스템 → 계정 액션 메뉴 → 계정 편집 → 수정완료 후 수정버튼을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.4-meteringdelete/",
	"title": "2.1.4 클라우드 미터링 계정 삭제",
	"tags": [],
	"description": "",
	"content": "클라우드 미터링 계정을 삭제 할 수 있다.\n 좌측 메인메뉴 → 시스템 → 계정 액션 메뉴 → 계정 삭제   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.5-useradd/",
	"title": "2.1.5 시스템 사용자 추가",
	"tags": [],
	"description": "",
	"content": "시스템 사용자를 추가 하는 기능이다.\n 좌측 메인메뉴 → 시스템 → 사용자 추가를 클릭한다.      시스템 사용자 설명     이름 시스템 사용자 이름   아이디 칵테일 클라우드 접속 사용자 아이디   설명 시스템 사용자 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.6-useredit/",
	"title": "2.1.6 시스템 사용자 편집",
	"tags": [],
	"description": "",
	"content": "시스템 사용자 정보(이름, 설명)를 편집 하는 기능이다.\n 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 편집를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.7-userreset/",
	"title": "2.1.7 시스템 사용자 비밀번호 초기화",
	"tags": [],
	"description": "",
	"content": "Cocktail Cloud 사용자 비밀번호를 초기화 하는 기능이다. (초기화 후 제공받는 초기 비밀번호는 \u0026lsquo;Pass0000\u0026rsquo;이다.)\n 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 비밀번호 초기화를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.1.system/2.1.8-userdelete/",
	"title": "2.1.8 시스템 사용자 삭제",
	"tags": [],
	"description": "",
	"content": "시스템 사용자를 삭제하는 기능이다.\n 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 삭제를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/",
	"title": "2.2 클러스터",
	"tags": [],
	"description": "",
	"content": "클러스터의 등록관리와 노드와 어플리케이션 단위의 모니터링, 볼륨, 알람, 미터링 서비스가 제공된다.    클러스터 메뉴 설명     노드 전체 노드의 CPU, Memory, Network 사용량 및 개별 노드의 모니터링 서비스   어플리케이션 전체 어플리케이션의 CPU, Memory, Network 사용량 및 개별 어플리케이션의 모니터링 서비스   볼륨 스토리지 관리 및 볼륨 현황   알람 클러스터의 이벤트 정보 관련 알람 서비스   미터링 클러스터 비용과 리소스별 비용 서비스   등록관리 클러스터 조회 및 수정 및 삭제    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.1-add/",
	"title": "2.2.1 클러스터 등록",
	"tags": [],
	"description": "",
	"content": "클러스터를 추가 등록할 수 있는 기능이다.\n 클러스터 → 오른쪽 상단 + 클러스터 등록 버튼을 클릭한다.  기본 정보(이름, 쿠버네티스 버전, 아이디, 설명)를 입력한다.\n   기본 정보 설명     이름 등록할 클러스터의 이름   쿠버네티스 버전 클러스터에 설치된 Kubernetes의 버전정보. e.g) 1.8.13   아이디 클러스터 고유 아이디(사용자 지정), 알람 메시지 Redirect시 필요   설명 클러스터에 대한 사용자 설명    프로바이더 정보(프로바이더, 유형, 리전)를 입력한다. 계정의 프로바이더와 유형에 따라 입력란이 변경된다.\n   프로바이더 설명     프로바이더 프로바이더를 선택   유형 Kubernetes의 사용 유형으로 MANAGED, PROVIER, GKE, EKS, AKS 선택   리전 Kubernetes가 설치된 서버의 리전 선택       추가입력 설명     Billing Account 클라우드 미터링 계정 선택   프로젝트 아이디 Google Cloud Platform의 계정이 사용할 프로젝트의 아이디(GKE를 사용할 프로젝트)    엔드포인트 정보(마스터 URL, Ingress Host, 노드 포트 호스트 주소, 노드 포트 범위)를 입력한다.\n   엔드포인트 설명     마스터 주소 Kubernetes API 주소. \u0026ldquo;https://host:port\u0026rdquo; 형식을 사용한다.   인그레스 호스트 주소 인그레스 방식에 사용할 Host IP Address 서비스(Master IP or Loadbelancer IP)   노드 포트 호스트 주소 노드에 포트를 붙여 서비스 노출하는 방식에서 포트 앞에 사용할 IP서비스(Master IP or Loadbelancer IP)   노드 포트 범위 노드에 포트를 붙여 서비스 노출하는 방식에서 IP뒤에 사용할 포트의 범위.(30000~32767 권장)    클러스터 유형을 입력한다.\n   인증 정보 설명     Cluster CA Certification 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 ca.crt파일값 입력   Client Certificate Data 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 admin.crt파일 값 입력   Client Key Data 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 admin.key파일 값 입력     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.2-update/",
	"title": "2.2.2 클러스터 수정",
	"tags": [],
	"description": "",
	"content": "클러스터의 기본정보, 프로바이더, 모니터링, 클러스터 유형을 수정하는 기능이다. 프로바이더의 계정, 유형, 리전은 변경할 수 없다.\n 클러스터 → 클러스터 선택 → 등록관리 → 수정버튼 → 수정 완료 후 저장버튼을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.3-delete/",
	"title": "2.2.3 클러스터 삭제",
	"tags": [],
	"description": "",
	"content": "클러스터 정보를 삭제하는 기능이다. 해당 클러스터를 사용하고 있는 어플리케이션 맵이 있을 경우 삭제할 수 없다.\n 클러스터 → 클러스터 선택 → 등록관리 → 삭제버튼을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.4-volume/",
	"title": "2.2.4 볼륨",
	"tags": [],
	"description": "",
	"content": "Public Cloud 스토리지 및 외부 스토리지 정보를 등록하고, 볼륨 현황에 대한 모니터링 서비스를 제공한다.\n 클러스터 → 클러스터 선택 → 볼륨을 클릭한다.      스토리지 설정 정보 설명     + 버튼 스토리지 추가 페이지로 이동   이름 스토리지 이름(사용자지정)   타입 스토리지 종류(NFS/EBS/Google Persistent Disk/Azure Disk)   스토리지 클래스 이름 k8s에 등록된 Class Name   정책 스토리지 볼륨의 정책 설정(Retain,Recyle,Delete)   상태 스토리지 사용 상태       볼륨 설정 정보 설명     볼륨이름 PVC 이름   상태 PVC 마운트 상태   사용량 PV의 할당된 가용량과 사용량   접속모드 PV 접속 권한   Age PVC 생성 경과 시간    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.5-add/",
	"title": "2.2.5 스토리지 추가",
	"tags": [],
	"description": "",
	"content": "볼륨을 추가하여 사용 할 수 있다.\n 클러스터 → 클러스터 선택 → 볼륨을 클릭 → 오른쪽 상단 + 모양의 버튼을 클릭한다.  클러스터, 이름, 설명, 스토리지 플러그인, 정책, 스토리지 클래스 이름을 차례로 기입한다.    스토리지 등록 정보 설명     이름 PV 이름   설명 PV 사용자 설명   유형 스리지 유형 (Single, Shared)   스토리지 플러그인 스토리지 종류(NFS, NFS Named, EBS, Google Persistent Disk, Azure Disk)   정책 PV 사용 정책(RETAIN/DELETE)       유형 설명     Single 워크로드 하나에서만 사용 가능한 스토리   Shared 워크로드 간에 공유하여 사용 가능한 스토리지       정책 설명     Retain PersistentVolumeClaim(PVC)가 삭제되도 PersistentVolume(PV)안에 데이터가 남는다. 추후 재사용 가능하지만 재사용시 PV를 다시 등록해야 한다.   Delete PVC가 삭제되면서 해당 PV도 함께 삭제된다.    사용하는 스토리지 플러그인에 따라 스토리지 클래스와 파라미터 설정.\n NFS 스토리지 플러그인 사용 시      스토리지 플러그인 NFS     총용량 스토리지 총용량(Gb)   정책 Retain과 Delete 정책을 지원   스토리지 클래스 이름 default-storage(기본값)   스토리지 프로비저너 스토리지 프로비저닝 값 입력     NFS Named 스토리지 플러그인 사용 시      스토리지 플러그인 NFS Named     정책 NFS Named 플러그인은 Retain 단일 정책을 지원   총용량 스토리지 총용량(Gb)   server 스토리지의 IP Address   path mount 경로     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.6-edit/",
	"title": "2.2.6 스토리지 편집",
	"tags": [],
	"description": "",
	"content": "등록된 스토리지 정보를 편집하는 기능이다. 단, NFS Named 타입의 스토리지는 편집할 수 없다.\n 클러스터 → 클러스터 선택 → 볼륨 → 볼륨 액션 버튼 → 스토리지 편집을 클릭한다.  이름, 설명, 정책, 스토리지 클래스 이름, 파라미터값을 수정 할 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.2.cluster/2.2.7-delete/",
	"title": "2.2.7 스토리지 삭제",
	"tags": [],
	"description": "",
	"content": "등록된 스토리지 정보를 삭제하는 기능이다.\n 클러스터 → 클러스터 선택 → 볼륨 → 볼륨 액션버튼 → 볼륨 삭제를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.3.workspace/",
	"title": "2.3 워크스페이스",
	"tags": [],
	"description": "",
	"content": "빌드 및 배포, 운영할 수 있는 워크스페이스를 할당한다. 워크스페이스 별로 사용자, 클러스터, 라이브러리 등록이 가능하다.\n 좌측 메인메뉴 → 워크스페이스    서비스 설정 설명     워크스페이스 명 워크스페이스 이름 (사용자지정)   클러스터 수 워크스페이스에 할당된 클러스터 수   구성원 수 워크스페이스를 사용하는 구성원 수   시스템 명 시스템 명   설명 워크스페이스 설명     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.3.workspace/2.3.1-add/",
	"title": "2.3.1 워크스페이스 생성",
	"tags": [],
	"description": "",
	"content": "워크스페이스를 생성 하는 기능이다. 워크스페이스 별로 구성원 등록 및 클러스터, 레지스트리(저장소) 등록을 할 수 있다.\n 좌측 메인메뉴 → 워크스페이스 → 오른쪽 상단 워크스페이스 생성 버튼을 클릭한다.  서비스에 사용할 이름, 설명, 색상, 레지스트리 및 구성원, 클러스터 정보를 등록한다.  구성원, 클러스터 추가는 체크박스가 선택 된것만 반영이 된다.\n 클러스터 편집  구성원 편집    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.3.workspace/2.3.2-edit/",
	"title": "2.3.2 워크스페이스 편집",
	"tags": [],
	"description": "",
	"content": "워크스페이스 정보를 편집하는 기능이다. 워크스페이스 이름, 설명, 색상 변경이 가능하며 해당 서비스를 사용할 구성원 편집, 워크스페이스에 등록할 클러스터 편집이 가능하다.\n 좌측 메인메뉴 → 워크스페이스 → 워크스페이스 액션 메뉴 → 워크스페이스 편집 → 수정완료 후 수정버튼을 클릭한다.  워크스페이스 명, 설명, 색상 및 구성원, 클러스터 수정이 가능하다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/2.system/2.3.workspace/2.3.3-delete/",
	"title": "2.3.3 워크스페이스 삭제",
	"tags": [],
	"description": "",
	"content": "워크스페이스를 삭제 하는 기능이다. 삭제 시, 워크스페이스에 클러스터가 등록되어 있을 경우 삭제가 불가능하며 워크스페이스 편집에서 클러스터를 제거 후 삭제를 진행할 수 있다.\n 좌측 메인메뉴 → 워크스페이스 → 워크스페이스 액션 메뉴 → 워크스페이스 삭제를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/3.dashboard/3.1-user/",
	"title": "3.1 사용자 관리",
	"tags": [],
	"description": "",
	"content": "현재 사용자 정보를 확인 및 관리 할 수 있다.\n 대시보드 -\u0026gt; 페이지 왼쪽 하단 사용자 모양 버튼을 클릭\n   목록 설명     ID 현재 사용중인 계정의 ID를 확인할 수 있다.   사용자 언어 선택 Korean, Japanese, English, Chinese 중 언어를 선택할 수 있다.   사용자 타임존 선택 Seoul, Tokyo, NewYork, Shanghai 중 타임존을 선택할 수 있다.   테마 Light와 Dark 테마 변경할 수 있다.   비밀번호 변경 기존 비밀번호, 신규 비밀번호를 넣어 비밀번호 변경이 가능하다.   도움말 Cocktail Cloud 기능 사용을 위한 도움말 매뉴얼이 제공된다.   제품명 제품명을 보여준다.   버전 현재 칵테일 버전을 확인할 수 있다.   로그아웃 현재 사용중인 계정을 로그아웃 한다.     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/",
	"title": "4.1 서비스 관리",
	"tags": [],
	"description": "",
	"content": "서비스 맵과 해당 서비스 맵 그룹 및 워크로드를 관리한다.\n   목록 설명      논리적 단위로 구성된 그룹. 액션메뉴를 활성화하면 해당 그룹에 그룹명 변경, 위 or 아래 이동, 위 or 아래 그룹추가, 그룹 삭제가 가능. +버튼을 클릭하면 서비스 맵을 생성할 수 있음.    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/4.1.1.servicemap/",
	"title": "4.1.1 서비스 맵",
	"tags": [],
	"description": "",
	"content": "서비스 구성 단위이다.\n논리적 단위로 그룹을 구성하고 그룹 내에 워크로드를 배치한다.\n맵을 구성하고 있는 클러스터의 미터링 정보를 확인하거나, 워크로드의 모니터링 정보를 확인 할 수 있다.\n배포 작업을 생성해서 워크로드를 생성하거나 업데이트 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택하여 클릭한다.    목록 설명      논리적 단위로 구성된 그룹. 액션메뉴를 활성화하면 해당 그룹에 그룹명 변경, 컬럼수 변경, 오른쪽 or 왼쪽 이동, 오른쪽 or 왼쪽에 그룹추가, 그룹 삭제가 가능. +버튼을 클릭하면 워크로드를 생성할 수 있음.    서비스 안에 배포되고 있는 워크로드. 드래그 \u0026amp; 드롭을 이용하여 자유롭게 이동이 가능    현재 배포중인 워크로드들을 위 사진과 같이 한눈에 볼 수 있도록 간략하게 표시    서비스 배포된 워크로드 단위로 사용중인 CPU, Memory, Network에 대한 모니터링 서비스 메뉴    이미지를 빌드하고 배포작업까지 한 번에 진행 가능하고, 배포 중인 워크로드의 이미지 버전을 변경하여 재배포 할 수 있으며, 여러 워크로드를 동시에 일괄배포가 가능    퍼시스턴트 볼륨을 설정 할 수 있는 메뉴    컨피그 맵, 시크릿 여러 설정 파일들을 저장하여 사용 가능     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.1-servicemapgroup/",
	"title": "4.1.1-1 서비스 맵 그룹관리",
	"tags": [],
	"description": "",
	"content": " 서비스맵에 대하여 그룹으로 관리한다. 해당 그룹에 서비스맵을 생성할 수 있다. 그룹명, 그룹색상을 변경하거나, 그룹 추가, 이동, 삭제 가능하다.\na) 좌측 메인메뉴 → 서비스 → 서비스맵 그룹의 ▼(액션 버튼) 클릭하여 메뉴를 활성화한다.    목록 설명     그룹명 변경 그룹명, 그룹색상을 변경   위로 이동 그룹을 위로 이동   아래로 이동 그룹을 아래로 이동   위에 그룹 추가 그룹 위에 새로운 그룹을 생성   아래에 그룹 추가 그룹 아래에 새로운 그룹을 생성   그룹 삭제 그룹을 삭제(그룹에 서비스맵이 존재하면 삭제가 불가능)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.2-add/",
	"title": "4.1.1-2 서비스 맵 생성",
	"tags": [],
	"description": "",
	"content": " 서비스에 새로운 서비스 맵을 생성 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스의 우측 + 버튼을 클릭한다. b) 사용하고자 하는 클러스터를 선택하고, 맵 이름과 네임스페이스 이름을 지정하여 서비스 맵을 생성한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.3-rename/",
	"title": "4.1.1-3 서비스 맵 이름변경",
	"tags": [],
	"description": "",
	"content": " 서비스 맵의 이름을 변경한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵의 우측 ✎ 버튼 ㅡ을 클릭한다. b) 활성화된 이름 입력란에 내용을 변경 후, Enter 키를 눌러 서비스 맵의 이름을 변경한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.4-delete/",
	"title": "4.1.1-4 서비스 맵 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스 맵을 삭제하는 기능이다. 서비스 맵을 삭제하기 위해서는 배포 중인 워크로드가 없어야 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 서비스 맵 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.1.servicemanagement/4.1.2-group/",
	"title": "4.1.2 그룹 관리",
	"tags": [],
	"description": "",
	"content": "워크로드에 대하여 그룹으로 관리한다. 해당 그룹에 워크로드를 생성할 수 있다. 그룹명을 변경하거나, 그룹 추가, 이동, 삭제, 컬럼수 변경이 가능하다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 그룹의 ▼(액션 버튼) 클릭하여 메뉴를 활성화한다.    목록 설명     그룹명 변경 그룹명을 변경   컬럼수 변경 컬럼수를 변경(최소:1~최대:8)   왼쪽으로 이동 그룹을 왼쪽으로 이동   오른쪽으로 이동 그룹을 오른쪽으로 이동   왼쪽에 그룹 추가 그룹 왼쪽에 새로운 그룹을 생성   오른쪽에 그룹 추가 그룹 오른쪽에 새로운 그룹을 생성   그룹 삭제 그룹을 삭제(그룹에 워크로드가 존재하면 삭제가 불가능)     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/",
	"title": "4.2 워크로드",
	"tags": [],
	"description": "",
	"content": " 서비스를 구성하는 최소 단위이다. 1개의 워크로드가 서비스가 될 수도 있고, 여러개의 워크로드가 하나의 서비스를 구성 할 수 있다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택하면 맵안에 구성된 워크로드들을 볼 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.1-add/",
	"title": "4.2.1 워크로드 생성",
	"tags": [],
	"description": "",
	"content": "그룹관리에서 워크로드를 생성하고 실행한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 그룹 + 버튼(워크로드 생성)을 클릭한다. 워크로드 유형을 선택 한다.  기본정보, 컨테이너, 인스턴스, 업데이트 정책, 서비스 포트, 볼륨 내용을 작성하고 생성버튼을 눌러 워크로드를 생성한다.  워크로드 기본 정보 정의      항목 설명     이름 생성하는 워크로드의 이름   그룹 워크로드가 생성되는 워크로드 그룹   유형 워크로드의 인스턴스 유형(Single/Multi)   설명 워크로드의 설명   서비스 계정 명 서비스 계정 명   노드 셀렉터 키 노드 셀렉터 키   노드 셀렉터 값 노드 셀렉터 값     컨테이너 생성\n 컨테이너 우측 \u0026lsquo;+ 버튼\u0026rsquo; 클릭  컨테이너 이름을 입력하고, 컨테이너로 사용할 이미지를 빌드나 이미지에서 선택하고, 사용할 CPU/Memory 리소스값을 정의한다.  컨테이너가 생성하면서 사용할 커맨드를 입력 할 수 있다.  컨테이너의 환경변수 값을 입력하거나 시크릿을 선택하여 설정할 수 있다.  보안정책을 설정 할 수 있다.  컨테이너의 헬스체크를 설정 할 수 있다.   인스턴스\n 인스턴스 항목의 \u0026lsquo;✎ 버튼\u0026rsquo; 클릭  서버 유형이 Single인 경우 인스턴스 종료 대기시간만 조절 가능  (서버 유형이 Multi일 경우만 해당)       항목 설명     오토스케일링 유형 오토스케일링을 하기 위한 기준을 정의   CPU 사용율(백분율) 오토스케일링의 기준(사용율)   메모리 사용률(백분율) 오토스케일링의 기준(사용율)   최대개수 스케일링의 인스턴스 최댓값   인스턴스 수 기본 인스턴스의 대수     업데이트 정책(서버 유형이 Multi일 경우만 해당)\n 업데이트 정책 항목의 \u0026lsquo;✎ 버튼\u0026rsquo; 클릭       항목 설명     업데이트 정책 Rolling Update/Recreate로 정의. 롤링업데이트를 통해 중단 Pod와 확장 Pod를 정의하여 자원의 효율성을 증대한 무중단 업데이트가 가능하고, Recreate 통하여 한 번에 업데이트가 가능   서비스 제거(중단Pod비율) 롤링업데이트를 할 경우, 중단되는 Pod의 비율   확장 Pod비율 롤링업데이트를 할 경우, 늘어날 수 있는 Pod의 비율     서비스 포트      항목 설명     Internal 컨테이너 내부통신 네트워크   Cluster IP Pod 내부통신 네트워크   Node Port 외부노출 네트워크   Loadbalancer 로드밸런스 통신 네트워크   Sticky Session 특정 클라이언트 요청에 세션을 유지하는 옵션   타임아웃(초) Sticky Session 유지 시간       항목 설명     이름 포트 이름을 정의   타겟 포트 컨테이너 포트를 정의   포트 외부 포트를 정의   지정 여부 node port를 지정하여 사용할 것인지에 대한 여부   인그레스 사용 여부 인그레스를 사용할 것인지에 대한 여부. (Public Cloud로 배포할 경우 Loadbalancer 타입으로는 인그레스 사용이 불가)     볼륨      항목 설명     볼륨 워크로드에서 사용할 볼륨을 생성   볼륨 마운트 생성된 볼륨에 워크로드를 마운트   퍼시스턴트 볼륨 마운트 생성된 퍼시스턴트 볼륨에 워크로드를 마운트       볼륨 종류 설명     Empty Dir 컨테이너 안에 임시 디렉토리 볼륨   Host Path 컨테이너가 올라가 있는 클러스터 서버 볼륨   Config Map 워크로드에 등록 되어있는 Config File을 사용   Secret 워크로드에 등록 되어있는 Secret File을 사용 (권한사용 유무 설정이 가능하고, CHMOD 룰을 사용. 미사용시 Default 권한은 644)     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.2-edit/",
	"title": "4.2.2 워크로드 편집",
	"tags": [],
	"description": "",
	"content": "Running 또는 Stopped 상태의 워크로드를 편집하고 재배포 한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드를 클릭한다.  기본정보, 컨테이너, 인스턴스, 업데이트 정책, 서비스 포트, 볼륨 내용을 편집하고 수정버튼을 눌러 워크로드를 재배포 한다.\n워크로드 Running 중이라면 워크로드유형과 컨테이너의 이미지 변경이 불가능. 단, 컨테이너 이미지태그 변경은 가능. 이를 변경하기 위해서는 워크로드 중지 후 편집하면 편집이 가능 하다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.3-instance/",
	"title": "4.2.3 워크로드 인스턴스 정보 조회",
	"tags": [],
	"description": "",
	"content": "워크로드 인스턴스의 정보를 조회 할 수 있다.\n   인스턴스 정보     Instance 상세 정보, 상태 정보, 이벤트 정보, 배포 정보, 웹터미널(컨테이너), 로그(컨테이너)   Services 상세 정보, 이벤트 정보, 배포 정보   Persistent Volume Claims 상세 정보, 볼륨 상세 정보, 배포 정보   Controller 상세 정보, 이벤트 정보, 배포 정보, 오토스케일러(Deployments만 해당) 정보(옵션)   Ingresses 상세 정보, 배포 정보     좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 선택  상태에서 새로고침 클릭시 상태정보(서버상태, 사용량, 인스턴스 등등)가 재조회 된다.   인스턴스 이름 클릭  인스턴스 → 컨테이너와 웹 터미널 연결  인스턴스 → 컨테이너의 로그 인스턴스 액션메뉴  인스턴스의 액션 버튼을 클릭(활성화) → 상태 정보 클릭  인스턴스의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭  인스턴스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭  컨트롤러 이름 클릭  컨트롤러 액션메뉴  Type Replicaset  Type Deployments   컨트롤러의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭  컨트롤러의 액션 버튼을 클릭(활성화) → 배포 정보 클릭  컨트롤러의 액션 버튼을 클릭(활성화) → 오토스켈러 정보(Type: Deployments) 클릭  오토스켈러 정보 팝업 → 이름 클릭 오토스켈러 팝업 액션메뉴  오토스켈러 팝업 액션 버튼을 클릭(활성화) → 상태 정보 클릭  오토스켈러 팝업 액션 버튼을 클릭(활성화) → 배포 정보 클릭  서비스 이름 클릭 서비스 액션메뉴  서비스의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭  서비스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭  볼륨 요청 이름 클릭  볼륨 요청 볼륨 클릭 볼륨 요청 액션메뉴  볼륨 요청의 액션 버튼을 클릭(활성화) → 배포 정보 클릭  인그레스 이름 클릭 인그레스 액션메뉴  인그레스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.4-stop/",
	"title": "4.2.4 워크로드 중지",
	"tags": [],
	"description": "",
	"content": "배포된 워크로드를 중지한다. 워크로드를 중지하면 POD는 삭제된다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Running 상태의 워크로드 액션메뉴를 클릭(활성화) → 중지를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.5-start/",
	"title": "4.2.5 워크로드 시작",
	"tags": [],
	"description": "",
	"content": "중지된 워크로드를 시작한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Stopped 상태의 워크로드 액션메뉴를 클릭(활성화) → 시작을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.6-restart/",
	"title": "4.2.6 워크로드 재시작",
	"tags": [],
	"description": "",
	"content": "워크로드를 재시작한다. 컴퓨터의 Rebooting과 같은 개념이다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Running 상태의 워크로드 액션메뉴를 클릭(활성화) → 재시작을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.2.workload/4.2.7-delete/",
	"title": "4.2.7 워크로드 제거",
	"tags": [],
	"description": "",
	"content": "중지된 워크로드를 제거한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Stopped 상태의 워크로드 액션메뉴를 클릭(활성화) → 제거를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.3.monitoring/",
	"title": "4.3 모니터링",
	"tags": [],
	"description": "",
	"content": "워크로드, POD, Container 별로 사용 중인 CPU, Memory, Network 사용량에 대하여 조회 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 모니터링을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.4.pipeline/",
	"title": "4.4 파이프라인",
	"tags": [],
	"description": "",
	"content": "파이프라인 기능을 통해 이미지 빌드 작업부터 워크로드 배포작업까지 한 번에 진행 할 수 있다. 또한 배포 중인 워크로드의 이미지 버전을 변경하여 재배포 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 파이프라인을 클릭한다.    파이프라인 메뉴 설명     일괄 실행 파이프라인 작업들을 일괄적으로 실행   실행 해당 파이프라인 작업을 실행       이미지 메뉴 빌드 설명     이미지 태그 X 레지스트리에 있는 태그 입력시 해당 버전으로 워크로드를 배포   지정 ⃝ 이미지 버전 중 지정된 버전으로 워크로드를 배포   최신 ⃝ 이미지 버전 중 최신 버전으로 워크로드를 배포   빌드 \u0026amp; 배포 ⃝ 새롭게 빌드 작업 후 해당 이미지 버전으로 워크로드를 배포    파이프라인 실행\n   공용 이미지로 워크로드를 생성 했을 경우  이미지 태그 입력 후 \u0026lsquo;실행\u0026rsquo; 혹은 \u0026lsquo;일괄실행\u0026rsquo; 클릭(deploy된 버전과 입력한 버전이 다를 경우에만 실행가능. 단, latest 제외)  빌드한 이미지로 워크로드를 생성 했을 경우\n 지정한 이미지로 배포하기   파이프라인 작업 목록 우측에서 \u0026lsquo;지정\u0026rsquo; 및 이미지 선택 후 \u0026lsquo;실행\u0026rsquo; 혹은 \u0026lsquo;일괄실행\u0026rsquo; 클릭(deploy된 버전과 입력한 버전이 다를 경우에만 실행가능)  새롭게 빌드 작 후 해당 이미지로 배포하기   파이프라인 작업 목록 우측에서 \u0026lsquo;빌드\u0026amp;배포\u0026rsquo; 선택 후 \u0026lsquo;실행여부\u0026rsquo; 체크. 이후 \u0026lsquo;실행\u0026rsquo; 혹은 \u0026lsquo;일괄실행\u0026rsquo; 클릭(실행 여부가 체크되어 있는 경우에만 실행가능)  최신 이미지로 배포하기   파이프라인 작업 목록 우측에서 \u0026lsquo;최신\u0026rsquo; 선택 후 \u0026lsquo;실행\u0026rsquo; 혹은 \u0026lsquo;일괄실행\u0026rsquo; 클릭(deploy된 버전과 입력한 버전이 다를 경우에만 실행가능)  빌드 편집 화면 이동 및 로그보기   빌드 이미지명을 클릭 할 경우 빌드 편집화면으로 이동한다. 빌드 태그명을 클릭 할 경우 로그를 볼 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.5.persistencevolume/",
	"title": "4.5 퍼시스턴트 볼륨",
	"tags": [],
	"description": "",
	"content": " 퍼시스턴트 볼륨 현황 및 생성,삭제를 제공한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨    항목 설명     볼륨 이름 생성한 퍼시스턴트 볼륨 이름, 클릭 시 퍼시스턴트 볼륨 상세 화면으로 이동   볼륨 타입 볼륨 타입(Single, Shared)   상태 볼륨의 상태(Ready, Mounted)   사용량 사용량, 요청량, 총량   Age 볼륨 생성 경과 시간   Filter 검색을 통해 필요한 퍼시스턴트 볼륨 조회   + 버튼 퍼시스턴트 볼륨 생성 화면으로 이동     퍼시스턴트 볼륨 상세  퍼시스턴트 볼륨 액션메뉴   a) 볼륨 요청 정보 b) 볼륨 정보 c) 스토리지 정보 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.5.persistencevolume/4.5.1-add/",
	"title": "4.5.1 퍼시스턴트 볼륨 생성",
	"tags": [],
	"description": "",
	"content": "퍼시스턴트 볼륨 생성 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨      항목 설명     퍼시스턴트 볼륨 타입 볼륨 타입(Single, Shared)   스토리지 클러스터에 등록된 스토리지 목록 노출   엑세스 모드 퍼시스턴트 볼륨 타입이 Single일 경우 ReadWriteOnce, Shard일 경우 ReadWriteMany, ReadOnlyMany를 선택   이름 생성할 퍼시스턴트 볼륨의 이름   용량 생성할 퍼시스턴트 볼륨의 용량(GB)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.5.persistencevolume/4.5.2-delete/",
	"title": "4.5.2 퍼시스턴트 볼륨 삭제",
	"tags": [],
	"description": "",
	"content": "퍼시스턴트 볼륨을 삭제한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨 → 퍼시스턴트 액션메뉴(활성화) → 삭제 클릭   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.1.configmap/",
	"title": "4.6.1 컨피그 맵",
	"tags": [],
	"description": "",
	"content": "서버에서 Config File을 Map 형식으로 저장하여 볼륨으로 사용 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵을 클릭한다.    항목 설명      컨피그 맵 추가 페이지로 이동   이름 컨피그 맵 이름(사용자 지정)   설명 컨피그 맵 설명(사용자 지정)    컨피그 맵의 액션 메뉴     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.1.configmap/4.6.1.1-add/",
	"title": "4.6.1.1 컨피그 맵 추가",
	"tags": [],
	"description": "",
	"content": "서비스에 컨피그 맵을 추가한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 생성 버튼을 클릭하여 등록페이지로 이동한다.  이름과 데이터의 값을 입력하고 우측 상단 생성 버튼을 눌러 컨피그 맵을 생성한다.    항목 설명     이름 이름을 입력하는 란입니다. 추후 편집이 불가   설명 컨피그 맵의 설명을 입력하는 란입니다.   데이터 추가 버튼 컨피그 맵의 데이터 추가할 수 있는 버튼   KEY 컨피그 맵의 Key 값   VALUE 컨피그 맵의 Value 값     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.1.configmap/4.6.1.2-edit/",
	"title": "4.6.1.2 컨피그 맵 수정",
	"tags": [],
	"description": "",
	"content": "서비스에 추가 된 컨피그 맵의 데이터를 수정 할 수 있다.(이름 변경불가)\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 액션메뉴(활성화) → 컨피그 맵 수정을 클릭  데이터 값 변경 후, 수정 버튼을 클릭하여 컨피그 맵을 편집한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.1.configmap/4.6.1.3-delete/",
	"title": "4.6.1.3 컨피그 맵 삭제",
	"tags": [],
	"description": "",
	"content": "서비스에 컨피그 맵을 삭제한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 액션메뉴(활성화) → 컨피그 맵 삭제 클릭   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.2.secret/",
	"title": "4.6.2 시크릿",
	"tags": [],
	"description": "",
	"content": "서버에서 Secret File로 저장하여 환경변수 및 볼륨으로 사용 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿을 클릭한다.    항목 설명      시크릿 추가 페이지로 이동   이름 시크릿 이름(사용자 지정)   설명 시크릿 설명(사용자 지정)    시크릿의 액션 메뉴     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/",
	"title": "4.6.2 시크릿",
	"tags": [],
	"description": "",
	"content": "서버에서 Secret File로 저장하여 환경변수 및 볼륨으로 사용 할 수 있다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿을 클릭한다.    항목 설명      시크릿 추가 페이지로 이동   이름 시크릿 이름(사용자 지정)   설명 시크릿 설명(사용자 지정)    시크릿의 액션 메뉴     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.2.secret/4.6.2.1-add/",
	"title": "4.6.2.1 시크릿 추가",
	"tags": [],
	"description": "",
	"content": "서비스에 시크릿을 추가한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 생성 버튼을 클릭하여 등록페이지로 이동한다.  이름과 데이터의 값을 입력하고 우측 상단 생성 버튼을 눌러 시크릿을 생성한다.    항목 설명     이름 이름을 입력하는 란입니다. 추후 편집이 불가   설명 시크릿의 설정의 설명 입력 란입니다.   데이터 추가 버튼 시크릿의 데이터를 추가할 수 있는 버튼   KEY 시크릿의 Key 값   VALUE 시크릿의 Value 값     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.2.secret/4.6.2.2-edit/",
	"title": "4.6.2.2 시크릿 수정",
	"tags": [],
	"description": "",
	"content": "서비스에 추가 된 시크릿의 데이터를 수정 할 수 있다.(이름 변경불가)\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 액션메뉴(활성화) → 시크릿 수정을 클릭  데이터 값 변경 후, 수정 버튼을 클릭하여 시크릿을 편집한다.(시크릿 값은 보안상 보여주지 않음)   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/4.service/4.6.config/4.6.2.secret/4.6.2.3-delete/",
	"title": "4.6.2.3 시크릿 삭제",
	"tags": [],
	"description": "",
	"content": "서비스에 시크릿을 삭제한다.\n 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 액션메뉴(활성화) → 시크릿 삭제를 클릭   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.1-add/",
	"title": "5.1 빌드 추가",
	"tags": [],
	"description": "",
	"content": "도커 이미지에 대한 빌드 작업을 추가한다. 어플리케이션 다운로드, 빌드, 이미지 빌드로 총 3가지를 선택 할 수 있다.\n 좌측 메인 메뉴 → 빌드 화면에서 [+] 버튼을 클릭하여 해당 페이지로 이동한다. 기본 정보, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드 내용을 작성하고 [생성] 버튼을 눌러 빌드를 생성한다.     빌드 생성 단계 설명     어플리케이션 다운로드 빌드에 필요한 소스를 다운로드   어플리케이션 빌드 소스의 컴파일이 필요할 경우 사용커맨드 - 빌드하면서 실행될 작업호스트 경로 - 작업이 이루어질 컨테이너 경로와 그와 마운트되는 호스트경로Working dir -실제 컨테이너안에서 작업할 경로로 컨테이너 경로와 맞춤이미지 - 빌드시 사용될 이미지   이미지 빌드 앞서 작업된 소스로 도커파일을 만들어 이미지를 만들고 해당이미지를 레지스트리 저장소에 저장하는 단계     빌드 기본 정보     기본 정보 설명     이름 생성할 빌드의 이름   최근 액션 빌드의 최근 동작 상태   상태 빌드의 현재 상태     어플리케이션 다운로드     어플리케이션 다운로드 설명     리파지토리 종류 소스파일을 내려받을 버전관리툴의 종류. 현재 버전에서는 GIT만을 지원   프로토콜 유형 리파지토리에서 제공하는 HTTP/HTTPS 프로토콜을 지원   깃 저장소 유형 Private/Common을 지원   리파지토리 URL 다운로드 하기위한 저장소 URL   리파지토리 User ID 저장소의 권한을 갖는 ID   리파지토리 패스워드 저장소의 권한을 갖는 ID의 패스워드   타겟 브랜치(Branch) 다운로드할 소스의 브랜치 정보     어플리케이션 빌드     어플리케이션 빌드 설명     커맨드 빌드하면서 실행될 명령어   호스트경로 다운로드 된 소스가 있는 경로   컨테이너 경로 작업이 이루어질 컨테이너 경로   Working dir 실제 컨테이너안에서 작업할 경로로 컨테이너 경로와 맞춤   이미지 빌드를 실행할 컨테이너 이미지     이미지 빌드     이미지 빌드 설명     Dockerfile 이미지를 만들기 위한 Dockerfile   레지스트리명 생성한 이미지를 저장하는 레지스트리를 선택   이미지 생성한 이미지를 레지스트리에 저장 할 이미지 명   버전 생성한 이미지를 레지스트리에 저장 할 태그    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.2-edit/",
	"title": "5.2 작업 설정 편집",
	"tags": [],
	"description": "",
	"content": "빌드 작업을 편집한다.\n 좌측 메인 메뉴 → 빌드 → 빌드 액션 메뉴(활성화) → [작업 설정 편집]을 선택하여 해당 페이지로 이동한다. 기본 정보, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드 내용을 수정하고 [수정] 버튼을 눌러 작업 설정을 편집한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.3-history/",
	"title": "5.3 히스토리",
	"tags": [],
	"description": "",
	"content": "해당 빌드의 히스토리 페이지로 이동하여 빌드 실행 항목에 대하여 히스토리 정보를 볼 수 있다.\n 좌측 메인 메뉴 → 빌드 화면에서 빌드 액션 메뉴(활성화) → [히스토리]를 클릭하여 해당 페이지로 이동한다.  액션 메뉴(활성화) → [로그보기]를 클릭하여 빌드 로그를 확인할 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.4-run/",
	"title": "5.4 빌드 실행",
	"tags": [],
	"description": "",
	"content": "빌드 작업을 실행한다. 빌드 실행을 하면, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드를 한 번에 실행한다.\n 빌드 목록에서 실행할 빌드 작업의 빌드 액션 메뉴(활성화) → [빌드 실행]을 선택하면 빌드실행 창이 활성화된다.  설명을 입력 후, [승인] 버튼을 클릭하면 빌드 작업을 실행한다.  빌드 실행 시, 현재 액션이 변경되면서 빌드 로그 창이 활성화 된다.\n  빌드생성시 체크박스에서 선택된 순서대로 순차적으로 실행되며 각 단계별로 빌드 과정을 확인 할 수 있다.\n(DOWN 단계에서 오류가 발생하면 다음 단계로 진행이 되지 않는다.)\n빌드 실행 작업이 \u0026ldquo;CREATE IMAGE STEP DONE.\u0026rdquo; 이라는 로그가 보인다면 이미지 빌드는 성공적으로 마무리된 것이다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.5-cancel/",
	"title": "5.5 빌드 취소",
	"tags": [],
	"description": "",
	"content": "빌드 작업을 취소한다. 빌드작업이 실행 상태(RUNNING)인 것만 취소할 수 있다.\n 빌드 목록에서 취소할 빌드 작업의 빌드 액션 메뉴(활성화) → [빌드 취소]를 선택하여 빌드 작업을 취소한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.6-log/",
	"title": "5.6 로그 보기",
	"tags": [],
	"description": "",
	"content": "빌드작업의 최근 실행 로그를 보여준다. 빌드실행 도중이라면 실행 중인 로그를 볼 수 있다.\n 빌드 목록에서 로그 보기할 빌드 작업의 빌드 액션 메뉴(활성화) → [로그 보기]를 선택하여 해당 빌드 작업의 빌드 실행 로그를 볼 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/5.build/5.7-delete/",
	"title": "5.7 작업 삭제",
	"tags": [],
	"description": "",
	"content": "빌드 작업을 삭제한다.\n 빌드 목록에서 삭제할 빌드 작업의 빌드 액션 메뉴(활성화) → [작업 삭제]를 선택하여 해당 빌드 작업을 삭제할 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/6.1-add/",
	"title": "6.1 카탈로그 저장",
	"tags": [],
	"description": "",
	"content": "카탈로그를 생성하여 필요한 서버들을 묶어 한 번에 배포 할 수 있다. 워크로드 컨테이너 이미지가 빌드 인 경우에는 이미지만을 저장하여 공용이미지로 변환하여 카탈로그로 저장한다. 저장된 카탈로그는 서비스 카탈로그에서 확인 할 수 있다.\n카탈로그 저장시 설정의 컨피그 맵, 시크릿도 템플릿으로 저장한다. (단, 시크릿은 보안상 값은 저장하지 않는다.)\n 서비스에서 카탈로그로 저장하는 서비스 맵을 선택 화면 우측 상단 카탈로그로 저장\u0026rsquo; 버튼을 클릭 신규로 템플릿 생성 시 \u0026lsquo;신규\u0026rsquo; 카테고리를, 기존 템플릿을 업데이트 할 시 \u0026lsquo;기존\u0026rsquo; 카테고리 선택 후 버전, 요약정보, 편집 내용 작성 후 아래 \u0026lsquo;저장\u0026rsquo; 버튼을 클릭. (편집 입력필드는 Mark Down파일 형식으로 해당파일 작성 방법은 페이지 우측 \u0026lsquo;Mark Down support\u0026rsquo; 를 눌러 확인 할 수 있다.)   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/6.2-deploy/",
	"title": "6.2 카탈로그 배포",
	"tags": [],
	"description": "",
	"content": "생성된 카탈로그를 배포할 수 있다. 필요 시 불필요한 인스턴스를 제외하거나 편집하여 배포 가능하다.\n 카탈로그 → 배포하고자 하는 템플릿의 \u0026lsquo;배포\u0026rsquo; 버튼을 클릭  템플릿 카테고리에서 서비스 맵 그룹, 클러스터, 서비스 맵 선택 후 \u0026lsquo;배포\u0026rsquo; 클릭. 원하지 않는 인스턴스는 제외하고 배포할 수 있다.\n 신규 Service Map 배포 시 서비스 맵 그룹, 클러스터를 선택하고 서비스 맵, 네임스페이스 이름을 작성 후 배포할 수 있다.  기존 Serivce Map 배포 시 서비스 맵 그룹, 서비스 맵 선택 후 배포할 수 있다.\n   시크릿의 경우 보안상 값은 저장하지 않기에 배포 전 값을 입력해주어야 한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/6.3-edit/",
	"title": "6.3 카탈로그 편집",
	"tags": [],
	"description": "",
	"content": "생성한 카탈로그 내용을 편집할 수 있다. 템플릿의 버전, 요약정보, 편집 내용을 수정할 수 있으며 각 인스턴스의 배포 정보를 수정할 수 있다.\n 카탈로그 → 편집하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭  수정할 템플릿의 버전, 요약정보, 편집 내용을 수정하고, 화면 우측 하단 \u0026lsquo;편집\u0026rsquo;버튼 클릭하여 인스턴스의 배포 정보를 수정할 수 있다.  편집 화면에서 컨피그맵과 시크릿은 편집은 불가하고 내용만 확인 가능하다.  워크로드 편집시 컨테이너 환경설정과 볼륨, 볼륨마운트는 편집은 불가하고 내용만 확인 가능하다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/6.4-delete/",
	"title": "6.4 카탈로그 삭제",
	"tags": [],
	"description": "",
	"content": "불필요한 카탈로그를 삭제할 수 있다. (플랫폼 카탈로그는 삭제 불가)\n 카탈로그 → 삭제하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭  삭제할 템플릿의 버전을 선택 후, \u0026lsquo;삭제\u0026rsquo; 버튼을 클릭한다. 해당 템플릿 버전에 대하여 삭제된다. ㄴ  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/6.5-export/",
	"title": "6.5 카탈로그 내보내기",
	"tags": [],
	"description": "",
	"content": "존재하는 카탈로그를 내보내기할 수 있다. (플랫폼 카탈로그는 내보내기 불가)\n 카탈로그 → 내보내기 하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭  화면 왼쪽 상단의 \u0026lsquo;내보내기\u0026rsquo; 버튼을 클릭 한다. 해당 카탈로그가 카탈로그명-버전.zip 로 다운로드 된다.\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/6.catalog/6.6-import/",
	"title": "6.6 카탈로그 가져오기",
	"tags": [],
	"description": "",
	"content": "내보내기된 카탈로그 파일과 가져오기 기능을 통해 카탈로그를 생성할 수 있다. (플랫폼 카탈로그는 가져오기 불가)\n 카탈로그 → \u0026lsquo;가져오기\u0026rsquo; 버튼을 클릭  \u0026lsquo;Choose File\u0026rsquo; 버튼을 클릭해 가져오기 할 카탈로그 압축파일을 선택한 후 \u0026lsquo;저장\u0026rsquo; 버튼을 누른다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.1-cluster/",
	"title": "7.1 클러스터 목록",
	"tags": [],
	"description": "",
	"content": "시스템관리자에 의해 등록된 Kubernetes 클러스터 목록 정보를 조회 하는 기능이다.\n 클러스터 → 정렬 버튼을 클릭하여 원하시는 정렬기준을 선택한다.      클러스터 설정 설명     클러스터 클러스터 종류   노드 클러스터의 노드 숫자   CPU 클러스터의 전체 CPU 용량, 요청량, 사용량   메모리 클러스터의 전체 메모리 용량, 요청량, 사용량   서비스 클러스터의 서비스 맵(네임스페이스)의 숫자   워크로드 클러스터에서 배포 중인 워크로드의 숫자    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.2-node/",
	"title": "7.2 노드",
	"tags": [],
	"description": "",
	"content": "전체 노드의 총 Disk, CPU, Memory, Network 사용량과 개별 노드의 라벨, 상태정보 등을 조회할 수 있는 화면이다. 노드 명을 클릭하면 노드 상세 정보를 조회 할 수 있다.\n 클러스터 → 클러스터 선택 → 노드를 클릭한다.  노드명을 클릭하여 노드 상세 정보를 확인할 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.3-application/",
	"title": "7.3 서비스",
	"tags": [],
	"description": "",
	"content": "전체 서비스의 총 CPU, Memory, Network 사용량과 개별 서비스의 네임스페이스, 인스턴스 현황 등을 조회 할 수 있는 화면이다. 서비스 명을 클릭하면 해당 서비스 맵 페이지로 이동할 수 있다.\n 클러스터 → 클러스터 선택 → 서비스을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.4-volume/",
	"title": "7.4 볼륨",
	"tags": [],
	"description": "",
	"content": "Public Cloud 스토리지 및 볼륨 현황에 대한 정보와 모니터링 서비스를 제공한다.\n 클러스터 → 클러스터 선택 → 볼륨을 클릭한다.    스토리지 설정 정보 설명     이름 스토리지 이름(사용자지정)   타입 스토리지 종류(NFS/EBS/Google Persistent Disk/Azure Disk)   스토리지 클래스 이름 k8s에 등록된 Class Name   정책 스토리지 볼륨의 정책 설정(Retain,Recyle,Delete)   상태 스토리지 사용 상태       볼륨 설정 정보 설명     볼륨이름 PVC 이름   상태 PVC 마운트 상태   사용량 PV의 할당된 가용량과 사용량   접속모드 PV 접속 권한   Age PVC 생성 경과 시간     "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.5-alarm/",
	"title": "7.5 알람",
	"tags": [],
	"description": "",
	"content": "클러스터에 대한 이벤트를 알람으로 받을 수 있다.\n 클러스터 → 클러스터 선택 → 알람을 클릭한다.  알람 명 클릭 시 상세 내용을 확인 할 수 있다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.5.1/",
	"title": "7.5.1 알람 목록",
	"tags": [],
	"description": "",
	"content": "알람은 다음 목록에서 발생 조건이 지속 시간 만큼 계속될 경우 발생한다.\n AlertManager     알람 ID ALM-001     중요도 warning   알람 이름 AlertmanagerDown   지속 시간 5분   발생 조건 Alertmanager 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 Alertmanager의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.       알람 ID ALM-002     중요도 warning   알람 이름 AlertmanagerFailedReload   지속 시간 10분   발생 조건 Alertmanager의 설정 변경시, 설정 다시읽기 작업 실패시 발생   조치 사항 해당 Pod의 로그를 확인하여 ConfigMap의 설정 오류를 수정한다.     ETCD3     알람 ID ETC-001     중요도 critical   알람 이름 InsufficientMembers   지속 시간 3분   발생 조건 ETCD 메트릭 수집이 안 될 경우 발생   조치 사항 ETCD 클러스터의 상태를 확인한다. Prometheus의 로그 및 해당 노드의\netcd 상태를 확인한다.       알람 ID ETC-002     중요도 critical   알람 이름 NoLeader   지속 시간 1분   발생 조건 ETCD 리더가 없을 경우 발생   조치 사항 ETCD 클러스터의 상태를 확인한다. Disk Latency로 인한 문제 일 수 있으므로\n다음 명령을 ETCD 클러스터 전체 노드에서 실행한다. (ETCD Tuning)\n$ sudo ionice -c2 -n0 -p `pgrep etcd`       알람 ID ETC-003     중요도 warning   알람 이름 HighNumberOfLeaderChanges   지속 시간 즉시   발생 조건 최근 1시간 동안 3번 이상의 리더 변경이 발생할 경우   조치 사항 ETCD 클러스터의 상태를 확인한다. Disk Latency로 인한 문제 일 수 있으므로\n다음 명령을 ETCD 클러스터 전체 노드에서 실행한다. (ETCD Tuning)\n$ sudo ionice -c2 -n0 -p `pgrep etcd`       알람 ID ETC-004     중요도 warning   알람 이름 HighNumberOfFailedGRPCRequests   지속 시간 10분   발생 조건 최근 5분 이내에 gRPC 메소드 호출의 1% 이상 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-005     중요도 critical   알람 이름 HighNumberOfFailedGRPCRequests   지속 시간 5분   발생 조건 최근 5분 이내에 gRPC 메소드 호출의 5% 이상 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-006     중요도 critical   알람 이름 GRPCRequestsSlow   지속 시간 10분   발생 조건 최근 5분 동안 gRPC 메서드 요청 대기 시간 중 99 번째 백분위가 150ms보다 클 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-007     중요도 warning   알람 이름 HighNumberOfFailedHTTPRequests   지속 시간 10분   발생 조건 최근 5분 이내에 HTTP 엔드 포인트에 대한 요청의 1% 이상이 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-008     중요도 critical   알람 이름 HighNumberOfFailedHTTPRequests   지속 시간 5분   발생 조건 최근 5분 이내에 HTTP 엔드 포인트에 대한 요청의 5% 이상이 실패한 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-009     중요도 warning   알람 이름 HTTPRequestsSlow   지속 시간 10분   발생 조건 최근 5분 동안의 HTTP 요청 대기 시간 중 99번째 백분위가 150ms보다 클 경우   조치 사항 ETCD 클러스터와 Kubernetes 클러스터의 대역폭을 늘리거나\n클러스터의 Sacale-Up 필요.       알람 ID ETC-010     중요도 warning   알람 이름 EtcdMemberCommunicationSlow   지속 시간 10분   발생 조건 최근 5분 동안의 멤버간 통신 대기 시간 중 99번째 백분위가 150ms보다 클 경우   조치 사항 ETCD 클러스터의 대역폭을 늘리거나 클러스터의 Scale-Up 필요.       알람 ID ETC-011     중요도 warning   알람 이름 HighNumberOfFailedProposals   지속 시간 즉시   발생 조건 최근 1시간 동안 5개 이상의 실패한 raft protocol 요청이 있을 경우.\n(RAFT Protocol은 ETCD 동기화 Protocol)   조치 사항 ETCD 메트릭 문서에 따르면 리더 선출의 일시적인 실패 또는 멤버 부족으로 인한\nETCD 클러스터 중단 시간이 길어질 경우 발생합니다.\n리더가 있는지, 중단된 ETCD 멤버가 있는지 확인       알람 ID ETC-012     중요도 warning   알람 이름 HighFsyncDurations   지속 시간 10분   발생 조건 최근 5분 동안의 wal fsync 지속 시간의 99번째 백분위가 500ms보다 클 경우\n(wal fsync: 로그 항목을 적용하기 전에 디스크에 저장시 호출.)   조치 사항 ETCD 메트릭 문서에 따르면 디스크에 문제가 있을 경우 발생한다고 함.       알람 ID ETC-013     중요도 warning   알람 이름 HighCommitDurations   지속 시간 10분   발생 조건 최근 5분 동안의 커밋 지속 시간 중 99번째 백분위가 250ms보다 클 경우\n(backend commit: 디스크에 대한 최근 변경 사항의 증분 스냅 샷의 커밋.)   조치 사항 ETCD 메트릭 문서에 따르면 디스크에 문제가 있을 경우 발생한다고 함.     General     알람 ID GEN-001     중요도 warning   알람 이름 TargetDown   지속 시간 10분   발생 조건 메트릭 수집 작업이 안 될 경우 발생. 어떤 작업이 실패인지 표시됨.   조치 사항 Prometheus의 로그 및 해당 작업에 해당하는 Pod의 로그 및 이벤트를 확인한다.       알람 ID GEN-002     중요도 none   알람 이름 DeadMansSwitch   지속 시간 즉시   발생 조건 DeadMansSwitch 알림.   조치 사항 해당 알람은 사용자에게 통지되지 않습니다.       알람 ID GEN-003     중요도 critical   알람 이름 TooManyOpenFileDescriptors   지속 시간 10분   발생 조건 file descriptor 사용율이 95%이상 일때 발생   조치 사항 노드의 Limit값을 변경한다.(노드의 재시작 필요)       알람 ID GEN-004     중요도 warning   알람 이름 FdExhaustionClose   지속 시간 10분   발생 조건 단순회귀분석(simple linear regression)을 이용하여 4시간 이내에\nfile descriptor 고갈이 예측될 경우 발생   조치 사항 해당 Pod의 로그 및 이벤트를 확인한다.\n필요할 경우, 노드의 Limit값을 변경한다.(노드의 재시작 필요)       알람 ID GEN-005     중요도 critical   알람 이름 FdExhaustionClose   지속 시간 10분   발생 조건 단순회귀분석(simple linear regression)을 이용하여 1시간 이내에\nfile descriptor 고갈이 예측될 경우 발생   조치 사항 해당 Pod의 로그 및 이벤트를 확인한다.\n필요할 경우, 노드의 Limit값을 변경한다.(노드의 재시작 필요)     Kube-ApiServer     알람 ID KAS-001     중요도 critical   알람 이름 K8SApiserverDown   지속 시간 5분   발생 조건 kube-apiserver 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 kube-apiserver의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.       알람 ID KAS-002     중요도 warning   알람 이름 K8SApiServerLatency   지속 시간 10분   발생 조건 최근 10분 동안의 요청 대기 시간 중 99번째 백분위가 1s보다 클 경우 발생   조치 사항 계속 발생할 경우, 마스터 노드를 증설한다.     Kube-ControllerManager     알람 ID KCM-001     중요도 critical   알람 이름 K8SControllerManagerDown   지속 시간 5분   발생 조건 kube-controller-manager 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 kube-controller-manager의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.     Kube-Scheduler     알람 ID KSC-001     중요도 critical   알람 이름 K8SSchedulerDown   지속 시간 5분   발생 조건 kube-scheduler 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 kube-scheduler의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.     Kube-State-Metrics     알람 ID KSM-001     중요도 warning   알람 이름 DeploymentGenerationMismatch   지속 시간 15분   발생 조건 Deployment에 설정한 generation과 수집된 generation이 다를 경우 발생   조치 사항 Deployment의 로그 및 이벤트를 확인한다.\n필요하면 Deployment를 재배포한다.       알람 ID KSM-002     중요도 warning   알람 이름 DeploymentReplicasNotUpdated   지속 시간 15분   발생 조건 Deployment에 설정한 replica 개수와 변경되거나 available 상태의\nreplica 개수가 다를 경우 발생   조치 사항 Deployment 수정 사항이 반영이 안 된 상태이므로 Deployment 및\nPod의 로그 및 이벤트를 확인한다.       알람 ID KSM-003     중요도 warning   알람 이름 DaemonSetRolloutStuck   지속 시간 15분   발생 조건 DaemonSet에 상태가 Ready가 아닌 Pod가 있을 경우 발생   조치 사항 해당 Daemonset과 Pod의 로그 및 이벤트를 확인한다.       알람 ID KSM-004     중요도 warning   알람 이름 K8SDaemonSetsNotScheduled   지속 시간 10분   발생 조건 DaemonSet에 실행되어 할 Pod 개수 보다 실행중인\nPod 개수가 작을 경우 발생   조치 사항 해당 Daemonset과 Pod의 로그 및 이벤트를 확인한다.\n배포가 안 된 노드가 정상인지 확인한다.\n마스터 노드가 격리된 경우, Daemonset에 toleration 설정이 되어 있는지 확인한다.       알람 ID KSM-005     중요도 warning   알람 이름 DaemonSetsMissScheduled   지속 시간 10분   발생 조건 DaemonSet에 잘못 스케쥴된 Pod가 생겼을 경우 발생   조치 사항 해당 Daemonset과 Pod의 로그 및 이벤트를 확인한다.       알람 ID KSM-006     중요도 warning   알람 이름 PodFrequentlyRestarting   지속 시간 10분   발생 조건 최근 1시간 동안 Pod 재시작 횟수가 5회 이상일 경우 발생   조치 사항 해당 Pod의 로그 및 이벤트를 확인한다. 필요하면 Pod를 재시작한다.     Kubelet     알람 ID KBL-001     중요도 warning   알람 이름 K8SNodeNotReady   지속 시간 1시간   발생 조건 Node 상태가 Ready가 아닐 경우 발생   조치 사항 해당 노드의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-002     중요도 critical   알람 이름 K8SManyNodesNotReady   지속 시간 1분   발생 조건 클러스터 전체에서 Node 상태가 Ready가 아닌 비율이 20%이상일 경우 발생   조치 사항 해당 노드들의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-003     중요도 warning   알람 이름 K8SKubeletDown   지속 시간 1시간   발생 조건 클러스터 전체에서 3%이상의 kubelet 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 해당 노드의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-004     중요도 critical   알람 이름 K8SKubeletDown   지속 시간 1시간   발생 조건 클러스터 전체에서 10%이상의 kubelet 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 해당 노드들의 상태 및 이벤트를 확인한다.\nssh를 통해 노드에 접속하여 kubelet의 상태를 확인한다.       알람 ID KBL-005     중요도 warning   알람 이름 K8SKubeletTooManyPods   지속 시간 즉시   발생 조건 Node의 배치된 Pod의 수가 100개가 넘으면 발생.(제한값은 110)   조치 사항 제한값에 도달할 경우, 더 이상 Pod 생성이 안됨.\n다른 노드들의 상태도 같이 확인하여 여유가 없을 경우, 노드를 증설한다.     Node     알람 ID NOD-001     중요도 warning   알람 이름 NodeExporterDown   지속 시간 10분   발생 조건 NodeExporter 메트릭 수집이 안 될 경우 발생   조치 사항 Prometheus의 로그 및 NodeExporter의 로그와 이벤트를 확인한다.\n필요할 경우, Pod를 재시작한다.       알람 ID NOD-002     중요도 critical   알람 이름 K8SNodeOutOfDisk   지속 시간 즉시   발생 조건 Node 상태가 OutOfDisk일 때 발생   조치 사항 해당 노드의 디스크를 증설한다.       알람 ID NOD-003     중요도 warning   알람 이름 K8SNodeMemoryPressure   지속 시간 즉시   발생 조건 Node 상태가 MemoryPressure일 때 발생   조치 사항 해당 노드의 메모리를 증설한다.       알람 ID NOD-004     중요도 warning   알람 이름 K8SNodeDiskPressure   지속 시간 즉시   발생 조건 Node 상태가 DiskPressure일 때 발생   조치 사항 노드에서 로그, 미사용 dodkcer image, pv backup등을 삭제하여 디스크 공간을 확보한다.\n계속 발생할 경우, 해당 노드의 디스크를 증설한다.       알람 ID NOD-005     중요도 warning   알람 이름 NodeCPUUsage   지속 시간 30분   발생 조건 Node 최근 5분간 평균 CPU 사용량이 90%를 넘을 경우 발생   조치 사항 해당 노드의 CPU를 증설한다.       알람 ID NOD-006     중요도 warning   알람 이름 NodeMemoryUsage   지속 시간 30분   발생 조건 Node Memory 사용량이 90%를 넘을 경우 발생   조치 사항 해당 노드의 메모리를 증설한다.     Prometheus     알람 ID PRM-001     중요도 warning   알람 이름 PrometheusFailedReload   지속 시간 10분   발생 조건 Prometheus의 설정 변경시, 설정 다시읽기 작업 실패시 발생   조치 사항 해당 Pod의 로그를 확인하여 ConfigMap의 설정 오류를 수정한다.     System     알람 ID CKT-001     중요도 warning   알람 이름 PvLowRequestDisk   지속 시간 30분   발생 조건 PV가 요청한 디스크의 크기 대비 사용량이 80%가 넘으면 발생   조치 사항 PV의 크기를 늘린다. 단, 서버를 재배포 해야함.       알람 ID CKT-002     중요도 warning   알람 이름 PvLowTotalDisk   지속 시간 30분   발생 조건 PV가 마운트된 디스크의 크기 대비 사용량이 80%가 넘으면 발생   조치 사항 마운트된 디스크의 상태를 확인하고 미사용 PV를 제거한다.\n필요하면 디스크를 증설한다.       알람 ID CKT-003     중요도 warning   알람 이름 PodCPULimitUsage   지속 시간 30분   발생 조건 Resource Limit 설정값 대비 CPU 사용율이 90%가 넘으면 발생   조치 사항 계속 발생할 경우, Deployment의 CPU Limit 값 변경       알람 ID CKT-004     중요도 warning   알람 이름 PodMemoryLimitUsage   지속 시간 30분   발생 조건 Resource Limit 설정값 대비 Memory 사용율이 90%가 넘으면 발생   조치 사항 계속 발생할 경우, Deployment의 Memory Limit 값 변경    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.6-metering/",
	"title": "7.6 미터링",
	"tags": [],
	"description": "",
	"content": "사용 중인 클러스터별 미터링(요금)에 대한 모니터링 서비스를 제공한다.\n미터링 서비스는 Public Cloud 중 AWS와 GCP에만 적용되는 서비스이다.\n 클러스터 → 클러스터 선택 → 미터링을 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/7.cluster/7.7-registration/",
	"title": "7.7 등록정보",
	"tags": [],
	"description": "",
	"content": "현재 클러스터에 대한 등록정보를 확인할 수 있다.\nAccount 인증정보는 보안을 위해 노출되지 않는다.\n 클러스터 → 클러스터 선택 → 등록관리를 클릭한다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/",
	"title": "8.1 클러스터 설치",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.1.kass/8.1.1.1-cluster-kass-gke/",
	"title": "8.1.1.1 GKE 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " GKE 클러스터를 배포하는 방법을 알아봅니다. Google Kubernetes Engine(GKE)에 로그인 https://cloud.google.com/kubernetes-engine/ 에서 Google cloud에 로그인합니다.\n서비스 계정 만들기 1. IAM 및 관리자에서 Service Account 만들기.  서비스 계정에는 다음 역할이 필요합니다.\n project/viewer\n kubernetes-engine/admin\n service-account/user\n 역할 기반 액세스 제어(Identity and Access Management)를 사용하기 위한 필요 조건:\nrbac 생성시 사전 필수 요소 필요.\n 다음 명령어를 실행하여 Kubernetes에서 역할을 만들 수 있는 능력을 사용자에게 부여해야 합니다. [USER_ACCOUNT]는 사용자의 이메일 주소입니다.\n참조: https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control  kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin --user [USER_ACCOUNT]  or\nkubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $(gcloud config get-value account)   서비스 계졍 만들기를 선택합니다.\n참조: 서비스 계정으로 Cloud Platform 인증\n 서비스 계정 세부 정보\n 서비스 계정 이름:\n이 서비스 계정의 표시 이름입니다.\n 만들기를 선택합니다.\n  서비스 계정 권한(선택사항)\n 역할:\n이 사용 설명서에서는 편의상 소유자 권한을 부여합니다.  Private key 만들기\n CREATE KEY 선택합니다.\n CREATE 합니다.\n서비스 계정이 생성되었으면 서비스 계정의 사용자 인증 정보가 포함된 JSON 키 파일이 컴퓨터에 다운로드됩니다. 이 키 파일은 사용자 API에 인증을 수행하도록 애플리케이션을 구성하기 위해 사용됩니다.      GKE 클러스터 만들기 참조: 클러스터 아키텍처\n1. 프로젝트 만들기 참조: https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster\n 프로젝트 만들기를 선택합니다.자세히 알아보기\n Project Name:\n프로젝트 이름을 입력합니다.\n Preject ID\n프로젝트 ID는 프로젝트의 글로벌 고유 식별자입니다. 프로젝트를 만든 후에는 프로젝트 ID를 변경할 수 없습니다.\n프로젝트를 만들 때 또는 프로젝트 ID를 만들어야 하는 API를 활성화할 때 선택한 맞춤설정된 이름입니다. 삭제된 프로젝트의 프로젝트 ID는 재사용할 수 없습니다.  Location:\n상위 조직 또는 폴더를 선택 합니다.\n    1.1 Google Kubernetes Engine API가 사용 설정되었는지 확인합니다. **GOOGLE KUBERNETES ENGINEAPI 사용 설정됨** 을 확인 합니다. ![gke-create-api-lib-1](/cocktail-manual-3.3.0-public/images/assets/KR/3.1.1/GKE/gke-create-api-lib-1.png)   Cloud SDK가 설치되었는지 확인합니다.\n gcloud 명령줄 도구의 기본값을 설정합니다.\ngcloud 명령줄 도구에서 프로젝트 ID 및 Compute Engine 영역 옵션을 입력하는 시간을 절약하기 위해 기본값을 설정할 수 있습니다.\ngcloud config set project [PROJECT_ID] gcloud config set compute/zone us-central1-b  gcloud를 최신 버전으로 업데이트합니다.\ngcloud components update    2. VPC 네트워크 만들기  VPC 네트워크 만들기 선택\n 이름 :\nVPC에 고유 이름을 부여합니다\n 서브넷 생성:\n서브넷을 사용하면 Google Cloud 내에 자체 비공개 클라우드 토폴로지를 만들 수 있습니다. 각 지역에 서브넷을 만들려면 \u0026lsquo;자동\u0026rsquo;을 클릭하고, 서브넷을 직접 정의하려면 \u0026lsquo;맞춤설정\u0026rsquo;을 클릭하세요. 자세히 알아보기\n 이름:\n서브넷 고유 이름을 부여합니다\n Region:\n이 사용 설명서에서는 asia-northeast1(Tokyo)를 사용합니다.\n자세히 알아보기\n IP 주소 범위:\nCIDR 표기법으로 나타낸 이 서브넷의 주소 범위입니다. 표준 사설 VPC 네트워크 주소 범위(예: 10.0.0.0/9)를 사용하세요.\n자세히 알아보기\n 완료를 선택합니다.\n 위와 같은 방법으로 subnet을 추가 합니다.\n  만들기를 선택합니다.\n    3. 클러스터 템플릿 GCP 콘솔을 사용해서 새 클러스터를 만들 경우 사용 가능한 모든 클러스터 템플릿이 표시됩니다. 기본적으로 표준 템플릿이 선택됩니다.\n 다음 템플릿을 사용할 수 있습니다.\n 표준 클러스터:\n지속적 통합, 웹 제공, 백엔드용입니다. 추가 맞춤설정이 필요하거나 어떤 템플릿을 선택할지 확실하지 않은 경우에 선택하면 가장 적합합니다.\n 첫 번째 클러스터:\n덜 강력한 노드를 실행하고 자동 확장과 같은 일부 고급 기능을 사용하지 않는 작은 클러스터입니다.\n CPU 집중 애플리케이션:\n해당 노드가 표준 클러스터보다 더 강력한 다중 코어 CPU를 제공하는 클러스터입니다.\n 메모리 집중 애플리케이션:\n해당 노드가 일반적으로 강력한 다중 코어 CPU와 대용량 메모리를 제공하는 클러스터입니다.\n GPU 가속 컴퓨팅:\n기본 노드 풀이 덜 강력한 노드로 구성되었고 GPU 설정 노드 풀이 추가로 포함된 클러스터입니다. 자동 확장은 기본적으로 사용되지 않습니다.\n 고가용성:\n클러스터가 지역 클러스터로 구성되었고, 특정 지역의 각 영역에 클러스터 마스터가 제공됩니다. 자동 확장 및 유지관리 기간이 사용 설정됩니다.\n  Kubernetes 클러스터 만들기\n 표준 클러스터\n템플릿을 선택하거나 해당 작업 부하에 적합한 템플릿을 선택합니다.\n 필요한 경우 템플릿을 맞춤설정합니다. 다음 필드는 필수입니다.\n 이름:\n클러스터 이름을 선택합니다. 프로젝트 및 영역 내에서 고유해야 합니다.\n 위치 유형:\n클러스터의 모든 노드가 동일 영역에 있는지 아니면 지정된 지역의 모든 영역에 있을 수 있는지 여부입니다.\n 영역:\n위치 유형이 영역인 경우 클러스터를 만들 Compute Engine 컴퓨팅 영역입니다.\n이 사용 설명서에서는 asia-northeast1-a(Tokyo)를 사용합니다.\n 지역:\n기본적으로 클러스터는 개발자가 생성 시 지정하는 단일 컴퓨팅 영역에 클러스터 마스터와 해당 노드를 만듭니다. 지역 클러스터를 만들어서 클러스터의 가용성 및 복구성을 향상시킬 수 있습니다.\n자세히 알아보기\n  마스터 버전:\nkubernetes version을 선택합니다.\n 노드 풀:\n노드 풀은 클러스터에서 Kubernetes를 실행하는 별도의 인스턴스 그룹입니다.\n 노드 수:\n클러스터에 만들 노드 수입니다. 노드 및 리소스에 대해 사용 가능한 리소스 할당량이 있어야 합니다(예: 방화벽 경로).\n 머신 유형:\n인스턴스에 사용할 Compute Engine 머신 유형입니다. 각 머신 유형은 서로 다르게 청구됩니다. 기본 머신 유형은 n1-standard-1입니다. 머신 유형 가격 정보는 머신 유형 가격표를 참조하세요.\n 고급 수정:\n 이름:\n노드 풀 이름을 부여합니다.\n 부팅 디스크 크기(GB):\n이 사용 설명서에서는 10GB를 사용합니다.\n 보안\n 서비스 계정:\nCompute Engine default service account 를 선택 합니다.\nVM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 콘솔 메뉴의 권한을 사용하여 서비스 계정을 만들거나 기본 서비스 계정이 있으면 이 계정을 사용하세요. 자세히 알아보기  저장을 선택합니다.\n   고급옵션:\n 네트워킹\n VPC 네이티브:\nVPC 네이티브 사용 설정(별칭 IP 사용) 체크합니다.\n 네트워크:\n항목에서 생성한 VPC를 선택합니다.\n 노드 서브넷:\n항목에서 생성한 subnet을 선택합니다.\n    만들기를 선택합니다.\n  GCP 콘솔에서 클러스터를 만든 다음에는 해당 클러스터와 상호작용하도록 kubectl을 구성해야 합니다. 자세한 내용은 kubeconfig 항목 생성을 참조하세요.\n  4. 클러스터에 연결  Cloud Shell에서 실행:\n Kubernetes 클러스터 목록에서 생성된 클러스터 연결을 선택합니다.   kubectl 을 사용하려면:\n kubectl 구성이 선행되야 합니다.\nkubectl은 Kubernetes Engine에서 사용되는 클러스터 조정 시스템인 Kubernetes를 관리하기 위해 사용됩니다.\n gcloud를 사용하여 kubectl을 설치할 수 있습니다.\ngcloud components install kubectl   클러스터 연결 화면에서 명령줄을 복사 콘솔에서 실행합니다.\n 아래 명령어로 확인할 수 있습니다.\nkubectl get svc     NFS Sever 만들기 Google Compute Engine 영구 디스크를 사용하여 네트워크 파일 시스템 (NFS) 서버를 만들어 컨테이너에 마운트하는 것입니다.\n1. Create an instance  이름(Name):\n인스턴스 이름을 부여합니다.\n 지역(Region):\n지역은 리소스를 실행할 수 있는 특정 지리적 위치입니다.\n 영역(Zone):\n영역은 지역 내의 격리된 위치입니다. 영역은 사용할 수 있는 컴퓨팅 리소스와 데이터를 저장하고 사용할 위치를 결정합니다.\n 머신 유형(Machine type):\n맞춤설정을 클릭하여 코어, 메모리, CPU를 선택합니다.\n 부팅 디스크(Boot disk):\n각 인스턴스에는 부팅을 위한 디스크가 필요합니다. 이미지나 스냅샷을 선택하여 새 부팅 디스크를 생성하거나 기존 디스크를 인스턴스에 연결하세요.\n이 사용 설명서에서는 CentOS 7 사용합니다.\n ID 및 API 액세스(Identity and API access):\n생성한 서비스 계정을 선택합니다.\nVM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 사용할 서비스 계정과 허용할 API 액세스 수준을 선택하세요. 자세히 알아보기\n  Management, security, disks, networking, sole tenancy 확장 섹션을 활성화 해서 아래 설정을 합니다.\n 디스크(Disks) 설정\n Add new disk를 선택 합니다.\n 노드에 필요한 디스크 사양을 설정 합니다.    네트워크(Networking) 설정\n네트워크는 인스턴스에서 액세스할 수 있는 네트워크 트래픽을 결정합니다.\n Network tags(네트워크 태그):\n네트워크 태그를 할당하여 특정 VM 인스턴스에 방화벽 규칙을 적용합니다.\n Network interfaces(네트워크 인터페이스)\n Network(네트워크):\n목록에서 VPC network에서 생성한 VPC를 선택합니다.\n Subnetwork(하위 네트워크):\n목록에서 VPC network에서 생성한 subnet을 선택합니다.\n Primary internal IP(기본 내부 IP):\n임시의 경우 인스턴스를 다시 시작해도 내부 IP가 변경되지 않지만 인스턴스를 삭제하고 다시 만들면 내부 IP가 변경됩니다.\n\u0026lsquo;임시(자동)\u0026lsquo;를 선택하여 하위 네트워크 범위의 주소를 할당하거나 \u0026lsquo;임시(커스텀)\u0026lsquo;를 선택하여 직접 입력하세요.\n인스턴스를 삭제하고 다시 만들 때 IP를 유지하려면 고정 내부 IP 주소를 선택하거나 만드세요.\n자세히 알아보기\n done(완료)**를 선택합니다.\n   Create(만들기)를 선택합니다.\n  2. 생성된 인스턴스(노드) 방화벽 설정(SSH / NFS).  인스턴스 화면에서 생성한 인스턴스의 확장 메뉴에서 View network details를 선택.\n또는 VPC network \u0026gt; Firewall rules를 선택한다.   SSH 방화벽 규칙을 추가 한다.\n Name:\n예) cocktail-test-gke-demo-storage-ssh\n Network:\n인스턴스에서 설정한 VPC를 선택합니다.\n Targets(대상):\nSpecified target tags를 선택.\n가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.\n Target tags:\nNFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.\n Source filter:\nIP ranges를 선택.\n필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.\n Source IP ranges:\n접근하고자 하는 IP ranges를 CIDR 표기법으로 추가 합니다.\n예)10.0.10.0/24 192.168.100.5\u0026frasl;32\n Protocols and ports:\n허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.\n Specified protocols and ports\n tcp: 22      NFS 방화벽 규칙을 추가 한다.\n Name:\n예) cocktail-test-gke-demo-storage-nfs\n Network:\n인스턴스에서 설정한 VPC를 선택합니다.\n Targets(대상):\nSpecified target tags를 선택.\n가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.\n Target tags:\nNFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.\n Source filter:\nSubnets를 선택.\n필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.\n Subnets:\n이 소스 하위 네트워크의 트래픽만 허용됩니다.\n Protocols and ports:\n허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.\n Allow all 선택\n이 사용 설명서에서는 편의상 Alloww all을 선택합니다.      3. 생성된 인스턴스(노드)에 SSH 연결.  인스턴스 화면에서 생성된 인스턴스(노드)의 SSH항목을 선택 -\u0026gt; View gcloud command를 선택 한다(자동생성).\n Run IN CLOUD SHELL을 실행 한다.\n RSA key pair 가 자동생성 된다.\n아래명령으로 생성된 key pair 을 확인할 수 있다.\ncd ~/.ssh  RSA key pair GCP의 Compute Engine \u0026gt; Metadata \u0026gt; SSH Keys에 자동 등록 된다.\n아래 그림처럼 등록된 SSH Key를 확인 및 편집할 수 있다.\n    또는 RSA key pair 수동 생성 / 등록 방법.\n 아래 명령으로 RSA key pair를 생성한다.\nssh-keygen -t rsa -f ./rsa-gcp-key -C\u0026quot;\u0026lt;Your-email.com\u0026gt;\u0026quot;    생성된 RSA Key 내용을 복사 해서 Compute Engine \u0026gt; Metadata \u0026gt; SSH Keys 에 등록 한다.\ncat ./rsa-gcp-key.pub     4. 생성된 인스턴스(노드)에 NFS 서버 설치 / 구성  노드 OS 확인.\nsudo grep . /etc/*-release  노드 NFS 패키지 확인.\nsudo rpm -qa | grep nfs  노드 NFS 패키지 설치.\nsudo yum install nfs-utils nfs-utils-lib  클러스터(마스터)에서 NFS에 사용될 공유 디렉토리를 생성 한다.\nprovisioner Deployment에서 NFS_PATH에 지정된 디렉토리를 생성한다.\nsudo mkdir /storage/shared  공유 디렉토리 퍼미션 설정.\nsudo chmod -R 777 /storage  공유 디렉토리 NFS 권한을 부여하고, 동기화를 한다.\n인스턴스(노드)에서 설정한 Subnet 또는 Ip ranges를 부여한다.\n# sudo vi /etc/exports /storage/shared 10.0.10.0/24(rw,sync)  NFS service를 재시작 한다.\nsudo service nfs restart   5. 클러스터(마스터)에 NFS-Client Provisioner 구성. 참조: Kubernetes NFS-Client Provisioner\n Setup authorization:\n클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.\n kubectl apply -f 1-rbac.yaml\nNamespace, ServiceAccount, Setup authorization 적용  apiVersion: v1 kind: Namespace metadata: name: cocktail-addon --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: cocktail-addon --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;] verbs: [\u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: cocktail-addon roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: cocktail-addon roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io  Configure the NFS-Client provisioner\n kubectl apply -f 2-shared-storage-provisioner-dp.yaml\nDeployment, 다음 프로비저 배포 파일을 편집하여 NFS 서버에 대한 연결 정보를 추가해야합니다.  kind: Deployment apiVersion: apps/v1 metadata: name: shared-storage-provisioner namespace: cocktail-addon spec: replicas: 1 selector: matchLabels: acornsoft.io/provisioner-type: NFSDYNAMIC app: shared-storage-provisioner strategy: type: Recreate template: metadata: labels: app: shared-storage-provisioner acornsoft.io/provisioner-type: NFSDYNAMIC spec: serviceAccount: nfs-client-provisioner containers: - name: shared-storage-provisioner image: quay.io/external_storage/nfs-client-provisioner:v2.0.1 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME # YOUR PROVISIONER_NAME value: acornsoft.io/shared-storage-provisioner - name: NFS_SERVER # YOUR NFS SERVER HOSTNAME value: 10.0.10.8 - name: NFS_PATH value: /storage/shared resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: nfs-client-root nfs: # YOUR NFS SERVER HOSTNAME server: 10.0.10.8 path: /storage/shared  Storage class 등록\n참조: https://kubernetes.io/docs/concepts/storage/storage-classes/\n kubectl apply -f 3-single-sc.yaml\nsingle-storage : Persistent Volumes with Kubernetes on GKE  --- # single-storage : Persistent Volumes with Kubernetes on GKE apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: single-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: GCE acornsoft.io/type: SINGLE addonmanager.kubernetes.io/mode: EnsureExists kubernetes.io/cluster-service: \u0026quot;true\u0026quot; parameters: type: pd-standard provisioner: kubernetes.io/gce-pd reclaimPolicy: Delete   kubectl apply -f 4-shared-sc.yaml\nshared-storage : NFS Persistent Volumes with Kubernetes on GKE  --- # shared-storage : NFS Persistent Volumes with Kubernetes on GKE apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: shared-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: NFSDYNAMIC acornsoft.io/total-capacity: \u0026quot;100\u0026quot; acornsoft.io/type: SHARED provisioner: acornsoft.io/shared-storage-provisioner reclaimPolicy: Delete allowVolumeExpansion: true   6. 샘플 PVC 등록 / 확인.  아래 명령으로 샘플을 구성할 수 있다.\n kubectl apply -f 5-test-pod.yaml\n임시 파일 생성 pod를 생성 한다.  --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: storageClassName: shared-storage accessModes: - ReadWriteMany resources: requests: storage: 1Mi --- kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - \u0026quot;/bin/sh\u0026quot; args: - \u0026quot;-c\u0026quot; - \u0026quot;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026quot; volumeMounts: - name: nfs-test-pvc mountPath: \u0026quot;/mnt\u0026quot; restartPolicy: \u0026quot;Never\u0026quot; volumes: - name: nfs-test-pvc persistentVolumeClaim: claimName: nfs-pvc   kubectl get pv\n생성된 PV를 확인 한다.\n NFS 노드에 생성된 임시 파일을 확인할 수 있다.\n   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.1.kass/8.1.1.2-cluster-kass-aks/",
	"title": "8.1.1.2 AKS 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " AKS 클러스터를 배포하는 방법을 알아봅니다. 사용 설명서 : Azure Portal을 사용하여 AKS(Azure Kubernetes Service) 클러스터 배포\nAzure에 로그인 https://portal.azure.com 에서 Azure Portal에 로그인합니다.\nAKS 클러스터 만들기 - Create a resource Azure Portal의 화면 왼쪽 상단 + Create a resource 선택 다음 \u0026gt; Kubernetes Service를 선택 합니다.\n1. Basics - 다음 옵션을 구성 합니다. Azure Kubernetes Service (AKS)는 호스팅 된 Kubernetes 환경을 관리하므로 컨테이너 오케스트레이션 전문 지식이 없어도 컨테이너 화 된 응용 프로그램을 빠르고 쉽게 배포 및 관리 할 수 있습니다.\n또한 응용 프로그램을 오프라인으로 설정하지 않고도 필요에 따라 리소스를 프로비저닝, 업그레이드 및 확장하여 진행중인 운영 및 유지 관리의 부담을 없애줍니다. Learn more about AKS\n PROJECT DETAILS\n Subscription :\nMicrosoft Azure 선택.\nAzure 서비스에 대한 엑세스 권한을 부여합니다. 자원 사용이 보고되고 서비스에 비용이 청구되는 방법이기도 합니다.\n ResourceGroup :\nmyResourceGroup 같은 Azure 리소스 그룹을 선택하거나 만듭니다.\n리소스 그룹은 동일한 수명주기, 사용 권한 및 정책을 공유하는 리소스 모음입니다.\n  CLUSTER DETAILS\n Kubernetes cluster name :\nmyAKSCluster 같은 Azure Kubernetes 서비스 클러스터 이름을 입력합니다.\n Region :\n클러스터가 배포되어야 하는 Azure 영역입니다. 이 사용 설명서에서는 Japan East를 선택 합니다..\n Kubernetes version :\n이 클러스터에 사용해야 하는 Kubernetes의 버전. 클러스터를 만든 후에는 버전을 업그레이드 할 수 있습니다.\nview the Kubernetes changelog\n DNS name prefix :\n호스팅 된 Kubernetes API 서버 FQDN과 함께 사용 할 DNS 이름 접두사.\n클러스터를 생성 한 후 컨테이너를 관리 할 때 Kubernetes API에 연결하기 위해 이를 사용 할 것입니다.\n  SCALE\n클러스터의 노드 수와 크기 입니다. Production workloads의 경우 복구를 위해 노드를 3 개 이상 권장합니다. 개발 또는 테스트 작업 부하의 경우 1개의 노드 만 필요합니다.\n클러스터를 만든 후에 노드 크기를 변경할 수는 없지만 만든 후에는 클러스터의 노드 수 를 변경할 수 있습니다.\nLearn more about scaling in Azure Kubernetes Service\n Node size :\n클러스터에서 노드를 형성 할 가상 시스템의 크기. 클러스터를 만든 후에는 변경할 수 없습니다.\n이 사용 설명서에서는 Standard DS2 v2 : 2 vcpus, 7 GB memory 를 사용 합니다.\n Node count :\n클러스터와 함께 생성되어야하는 노드의 수. 나중에 클러스터의 노드 수 를 조정할 수 있습니다\n이 사용 설명서에서는 노드 수를 1 로 설정합니다.\n Virtual nodes (preview) :\n가상 노드를 사용하면 서버가없는 Azure 컨테이너 인스턴스가 지원하는 노드에 컨테이너를 배포하거나 버스트 아웃 할 수 있습니다.\n이렇게하면 정의 된 클러스터 크기를 초과하는 빠른 버스트 확장 옵션을 제공 할 수 있습니다. Learn more\n   2. Authentication - 다음 옵션을 구성 합니다. 클러스터 인프라 서비스 원칙은 Kubernetes 클러스터에서 클러스터에 연결된 클라우드 리소스를 관리하는 데 사용됩니다. Learn more\nKubernetes 인증 및 권한 부여 는 Kubernetes 클러스터에서 클러스터에 대한 사용자 액세스를 제어하고 사용자가 인증을 받으면 수행 할 수있는 작업을 제어하는 ​​데 사용됩니다. Learn more\n CLUSTER INFRASTRUCTURE\n Service principal :\n기존 서비스 주체의 클라이언트 ID입니다.\n이 사용 설명서 에서는 (new) default service principal 를 선택하여 자동 생성 합니다.\n클러스터가 생성되 후 Properties 화면 INFRASTRUCTURE RESOURCE GROUP 에서 확인 할 수 있습니다.\n  KUBERNETES AUTHENTICATION AND AUTHORIZATION\n클러스터 리소스에 대한 세분화 된 제어를 제공하는 Kubernetes 역할 기반 액세스 제어를 사용합니다.\n Enable RBAC :\n이러한 컨트롤을 사용하면 AKS 클러스터에 배포된 Kubernetes 리소스에 대한 액세스를 정밀하게 제어할 수 있습니다.\n이 사용 설명서에서는 Kubernetes RBAC(역할 기반 액세스 제어)에 대한 옵션을 사용하도록 설정합니다.\n   3. Networking - 다음 옵션을 구성 합니다. HTTP 응용 프로그램 라우팅을 활성화하고 \u0026ldquo;기본\u0026rdquo;또는 \u0026ldquo;고급\u0026rdquo;의 두 가지 네트워킹 옵션 중에서 선택할 수 있습니다.\n* Basic 네트워킹은 기본값을 사용하여 클러스터에 새로운 VNet을 생성합니다.\n* Advanced 네트워킹을 통해 클러스터는 사용자 정의 가능한 주소로 새 VNet 또는 기존 VNet을 사용할 수 있습니다.\n응용 프로그램 포드는 VNet에 직접 연결되어있어 VNet 기능과의 기본 통합이 가능합니다.\nLearn more about networking in Azure Kubernetes Service\n HTTP application routing :\nHTTP application routing 솔루션을 사용하면 application endpoints 대해 공개적으로 액세스 할 수있는 DNS 이름을 만들어 클러스터에 배포 된 응용 프로그램에 쉽게 액세스 할 수 있습니다. Learn more about HTTP application routing\n이 사용 설명서에서는 NO 를 선택 합니다.\n Network configuration :\n기본 VNet과 함께 kubenet을 사용하는 Basic 네트워크 구성을 선택하거나\nAzure CNI를 사용하여 VNet을 사용자 지정하는 옵션과 함께 Advanced 구성을 선택하십시오.\n이 사용 설명서에서는 Basic 을 선택 합니다.\n  4. Monitoring - 다음 옵션을 구성 합니다. Azure Kubernetes Service를 사용하면 각 노드에 대한 CPU 및 메모리 사용량 메트릭을 얻을 수 있습니다.\n또한 컨테이너 모니터링 기능을 활성화하고 전체 Kubernetes 클러스터의 성능 및 상태에 대한 통찰력을 얻을 수 있습니다.\n처리 된 데이터의 양과 데이터 보존 설정에 따라 요금이 청구됩니다.\nLearn more about container performance and health monitoring\nLearn more about pricing\n AZURE MONITOR\n Enabe container monitoring :\n이 사용 설명서에서는 Yes를 선택 합니다.\n Log Analytics workspace :\n모니터링 데이터를 저장할 로그 분석 워크 스페이스 선택.\n이 사용 설명서에서는 Create new 를 선택해서 Japan East Region을 선택하고 생성 합니다.\n   5. Tags - 다음 옵션을 구성 합니다. 태그는 이름 / 값 쌍으로, 리소스를 분류하고 동일한 태그를 여러 자원 및 자원 그룹에 적용하여 통합 청구서를 볼 수 있습니다. Learn more\n태그를 만들고 다른 탭에서 리소스 설정을 변경하면 태그가 자동으로 업데이트됩니다.\n이 사용 설명서에서는 만들지 않습니다.\n6. Review + create - 검토 + 만들기를 구성 합니다.  검토 + 만들기를 선택하고, 완료되면 만들기를 선택 합니다.\n   AKS 클러스터를 만들고 사용 준비를 마칠 때까지 몇 분 정도 걸립니다.\n  7. 생성된 Resource groups 을 학인 완료되면 리소스 그룹에서 생성한 이름으로 리소스 그룹이 생성 됩니다. 생성된 클러스터를 확인 할 수 있습니다.\n 왼쪽 Resource groups 를 선택 후. 생성한 클러스트의 리소스 그룹을 선택 합니다.\n   생성한 클러스터의 Vitual machine 을 선택 합니다.  참고 사항 : Tags 를 변경 할 수 있습니다.    생성한 클러스터의 Vitual machine의 모니터링을 확인 할 수 있습니다.  클러스터에 연결 1. Kubernetes 클러스터를 관리하려면 - Cloud Shell 사용  Azure Portal 오른쪽 상단에 있는 \u0026gt;_ Cloud Shell을 엽니다.\n kubectl Kubernetes 명령줄 클라이언트를 사용합니다. kubectl 클라이언트가 Azure Cloud Shell에 사전 설치됩니다.\n  2. Kubernetes 클러스터에 연결하도록 kubectl을 구성하려면 - Cloud Shell 사용  az aks get-credentials 명령을 사용하여 kubectl 을 구성 합니다.\n이 명령은 자격 증명을 다운로드하고 Kubernetes CLI가 해당 자격 증명을 사용하도록 구성 합니다.\n다음 예제는 cocktail 이라는 리소스 그룹에서 cocktail-test-aks 클러스터의 자격 증명을 가져옵니다.\naz aks get-credentials --resource-group cocktail --name cocktail-test-aks  클러스터에 대한 연결을 확인하려면 kubectl get 명령을 사용하여 클러스터 노드의 목록(Computer name) 을 반환합니다.\nkubectl get nodes   AZURE Storage Accounts 만들기 Storage accounts 1. 왼쪽 메뉴 또는 Home 화면에서 Storage accounts 를 선택 합니다. 2. Storage accounts +Add 를 선택 합니다. Create storage account - 만들기 1. Basics - 다음 옵션을 구성 합니다. Azure 저장소는 가용성, 보안 성, 내구성, 확장 성 및 중복성이 뛰어난 클라우드 저장소를 제공하는 Microsoft 관리 서비스입니다.\nAzure 저장소에는 Azure Blob (개체), Azure Data Lake 저장소 Gen2, Azure 파일, Azure 대기열 및 Azure 테이블이 포함됩니다.\n스토리지 계정 비용은 아래에서 선택한 사용법 및 옵션에 따라 다릅니다. Learn more\n PROJECT DETAILS\n배포 된 리소스 및 비용을 관리하려면 Microsoft Azure 를 선택하십시오.\n폴더와 같은 리소스 그룹을 사용하여 모든 리소스를 구성하고 관리하십시오.\n Subscription :\nMicrosoft Azure 를 선택.\n Resource group :\n생성한 클러스터의 INFRASTRUCTURE RESOURCE GROUP 을 선택 합니다.\n  INSTANCE DETAILS\n기본 배포 모델은 최신 Azure 기능을 지원하는 Resource Manager입니다.\n대신 클래식 배포 모델을 사용하여 배포하도록 선택할 수 있습니다.\n Storage account name :\n이 이름은 Azure의 모든 기존 스토리지 계정 이름에서 고유해야합니다. 3 ~ 24 자 여야하며 소문자와 숫자 만 사용할 수 있습니다.\n Location :\n연결 할 클러스터와 같은 위치를 선택 합니다(권장).\n Performance :\nStandard(표준 스토리지) 계정은 마그네틱 드라이브로 백업되며 GB 당 최저 비용을 제공합니다. 대용량 저장 장치가 필요한 응용 프로그램이나 데이터 액세스 빈도가 낮은 응용 프로그램에 가장 적합합니다.\nPremium(프리미엄 스토리지) 계정은 솔리드 스테이트 드라이브를 기반으로하며 일관되고 낮은 대기 시간의 성능을 제공합니다. Azure 가상 머신 디스크에서만 사용할 수 있으며 데이터베이스와 같은 I / O 집약적 인 애플리케이션에 가장 적합합니다. 또한 모든 디스크에 Premium 스토리지를 사용하는 가상 시스템은 가용성 세트를 벗어나 실행하는 경우에도 99.9 % SLA를 사용할 수 있습니다. Learn more\n이 설정은 스토리지 계정을 만든 후에는 변경할 수 없습니다.\n Account kind :\n범용 V1, 범용V2(권장) 스토리지 계정은 통합 계정에서 blobs, 파일, 테이블 및 대기열에 대한 스토리지를 제공합니다.\nBlob 저장소 계정은 BLOB 데이터를 저장하고 액세스 계층을 선택하는 데 특화되어 있어 계정의 엑세스 빈도를 지정 할 수 있습니다.\n스토리지 요구사항과 일치하고 비용을 최저화 하는 계층을 선택하십시오. learn more\n Replication :\nAzure 스토리지 계정의 데이터는 항상 내구성과 고 가용성을 보장하기 위해 복제됩니다.\n내구성 요구 사항에 맞는 복제 전략을 선택하십시오.\n스토리지 계정을 만든 후에는 일부 설정을 별경 할 수 없습니다. learn more\n Access tier (default) :\n계정 액세스 계층은 명시 적으로 설정된 계층이없는 모든 BLOB에서 유추되는 기본 계층입니다.\nHot Access Tier는 자주 액세스하는 데이터에 이상적이며\nCool Access Tier는 자주 액세스하지 않는 데이터에 이상적입니다.\n아카이브 액세스 계층은 계정이 아닌 BLOB 수준에서만 설정할 수 있습니다. learn more\n   2. Advanced - 다음 옵션을 구성 합니다.  SECURITY\n Secure transfer required :\n보안 전송 옵션은 보안 연결을 통해 저장소 계정에 대한 요청 만 허용하여 저장소 계정의 보안을 강화합니다.\n예를 들어 저장소 계정에 액세스하기 위해 REST API를 호출 할 때는 HTTP를 사용하여 연결해야합니다.\nEnabled(보안 전송 필요)가 사용 설정된 경우 HTTP를 사용하는 모든 요청이 거부됩니다.\nAzure 파일 서비스를 사용하는 경우 SMB 2.1, 암호화없는 SMB 3.0 및 Linux SMB 클라이언트의 일부 버전을 비롯한 암호화없는 연결은 실패합니다.\nAzure 저장소는 사용자 정의 도메인 이름에 대한 HTTP를 지원하지 않으므로 사용자 정의 도메인 이름을 사용할 때는이 옵션이 적용되지 않습니다.\n이 사용 설명서에서는 Enabled 선택 합니다.  VIRTUAL NETWORKS\n Allow access from :\n모든 네트워크에서이 저장소 계정에 액세스 할 수 있습니다. Learn more\n이 사용 설명서에서는 All networks 선택 합니다.   DATA LAKE STORAGE GEN2 (PREVIEW)\n Hierarchical namespace :\nADLS Gen2 계층 적 네임 스페이스는 대규모 데이터 분석 작업을 가속화하고 파일 수준 ACL (액세스 제어 목록)을 활성화합니다. Learn more\n이 사용 설명서에서는 Disabled 선택 합니다.   3. Tags - 다음 옵션을 구성 합니다. 태그는 이름 / 값 쌍으로, 리소스를 분류하고 동일한 태그를 여러 자원 및 자원 그룹에 적용하여 통합 청구서를 볼 수 있습니다. Learn more\n태그를 만들고 다른 탭에서 리소스 설정을 변경하면 태그가 자동으로 업데이트됩니다.\n4. Review + create - 검토 + 만들기를 구성 합니다.  Review + create를 선택하고, 완료되면 Create를 선택 합니다.  4. 생성된 Storage Account 확인  완료된 화면 에서 Go to resource를 선택 하거나 , Resource groups 에서 추가되 Storage account를 확인 할 수 있습니다.  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.1.kass/8.1.1.3-cluster-kass-eks/",
	"title": "8.1.1.3 EKS 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " 8.1.1.3 EKS 클러스터 만들기 Quick Start EKS 클러스터를 배포하는 방법. Kubernetes 용 Amazon Elastic Container Service (Amazon EKS)를 사용하면 AWS의 Kubernetes를 사용하여 컨테이너 화 된 응용 프로그램을 쉽게 배포, 관리 및 확장 할 수 있다.\n참조 : Amazon Elastic Container Service for Kubernetes(EKS) 클러스터 배포\nAmazon Eks에 로그인  Amazon Eks에 로그인.  Amazon EKS 필수 조건  Amazon EKS 클러스터를 생성하기 전에 Kubernetes가 AWS 리소스 생성 시 수임할 수 있는 IAM 역할을 생성해야 한다.\n AWS 관리자 권한이 없다면 필요한 역할 / 정책 / 권한 을 요청 해야 합니다.\n참조 : EKS 관리 권한 / 정책\n  AWS EKS 클러스터 만들기 Kubernetes 용 Amazon Elastic Container Service (Amazon EKS)는 AWS에서 Kubernetes를 쉽게 실행할 수 있도록 해주는 관리 형 서비스입니다.\n1. AWS 리소스 생성 시 수임할 수 있는 IAM 역할을 생성 합니다. 관리자 권한 또는 해당 IAM 권한이 필요 합니다.\n [Roles(역할)]를 선택한 다음 [Create role]을 선택합니다.\n 서비스 목록에서 EKS를 선택합니다.\n 사용 사례에 대해 Allows Amazon EKS to manage your clusters on your behalf(EKS에서 사용자를 대신하여 클러스터를 관리하도록 허용)를 선택합니다.\n  [Next: Permissions(다음: 권한)]를 선택합니다.\n AmazonEKSClusterPolicy :\n이 정책은 Kubernetes에게 사용자를 대신하여 리소스를 관리하는 데 필요한 권한을 제공합니다.\nKubernetes는 인스턴스, 보안 그룹 및 탄력적인 네트워크 인터페이스를 포함하되 이에 국한되지 않는 EC2 리소스에 대한 식별 정보를 배치하는 데 Ec2 : CreateTags 권한이 필요합니다.\n AmazonEKSServicePolicy :\n이 정책에 따라 Kubernetes 용 Amazon Elastic Container Service는 EKS 클러스터를 운영하는 데 필요한 리소스를 만들고 관리 할 수 있습니다.\n  [Next: Tags(다음: 태그)]를 선택합니다.\nIAM 태그는 사용자 역할에 추가할 수 있는 키-값 페어입니다.\n태그는 이메일 주소와 같은 사용자 정보를 포함하거나 직책과 같은 내용일 수 있습니다. 태그를 사용하여 이 역할에 대한 액세스를 구성, 추적 또는 제어할 수 있습니다. Learn more\n [Next: Review(다음: 검토)]를 선택합니다.\n생성하기 전에 아래에 필요한 정보를 입력하고 이 역할을 검토하십시오.\n Role name(역할 이름)에서 역할에 대한 고유 이름(예: eksServiceRole)을 입력합니다.\n  Create role(역할 생성 만들기)을 선택합니다.\n 만들어진 역할을 확인 합니다.\n [Roles(역할)]를 선택 :\n목록에서 새로 생성한 Role(역할)을 확인할 수 있습니다.   2. Amazon EKS 클러스터 VPC 생성 - CloudFormation 콘솔 사용해서 생성 참조 : AWS 사용 설명서를 참조.\n https://console.aws.amazon.com/cloudformation에서 AWS CloudFormation 콘솔에서 제공 되는 템플릿을 사용해서 생성.\n 상단 리전 모음에서 Amazon EKS를 지원하는 리전을 선택.\n참고\n현재 다음 리전에서 Amazon EKS를 사용할 수 있습니다. (한국어 사용 설명서는 아직 반영이 안되어 있다.)\n US West (Oregon) (us-west-2)\n US East (N. Virginia) (us-east-1)\n US East (Ohio) (us-east-2)\n EU (Frankfurt) (eu-central-1)\n EU (Stockholm) (eu-north-1)\n EU (Ireland) (eu-west-1)\n Asia Pacific (Tokyo) (ap-northeast-1)\n Asia Pacific (Seoul) (ap-northeast-2)\n Asia Pacific (Singapore) (ap-southeast-1)\n Asia Pacific (Sydney) (ap-southeast-2)\n  [Create stack]을 선택.\n 템플릿 선택에서 Amazon S3 템플릿 URL 지정을 선택.\nhttps://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml  주의 : 2019-01-09 이전 날짜의 템플릿파일을 서울리전에서 사용시 오류가 있다. (AZ가 없는 관계로)\n 스택 이름: AWS CloudFormation 스택에 대한 스택 이름을 선택합니다. 예를 들어 eks-vpc로 사용할 수 있다.\n VpcBlock: VPC에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet01Block: 서브넷 1에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet02Block: 서브넷 2에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet03Block: 서브넷 3에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n  (선택 사항) 옵션 페이지에서 스택 리소스에 태그를 지정합니다. [Next]를 선택.\n [Review] 페이지에서 [Create]을 선택합니다.\n 스택이 생성된 후 콘솔에서 이를 선택하고 출력을 선택합니다.\n 생성된 보안 그룹에 대한 SecurityGroups 값을 기록합니다. 이 값은 EKS 클러스터를 생성할 때 필요합니다. 이 보안 그룹은 서브넷에 생성된 계정 간 탄력적 네트워크 인터페이스에 적용되고, 이를 통해 Amazon EKS 제어 플레인이 작업자 노드와 통신할 수 있습니다.\n 생성된 VPC의 VpcId를 기록합니다. 작업자 노드 그룹 템플릿을 시작할 때 필요합니다.\n 생성된 서브넷에 대한 SubnetIds를 기록합니다. EKS 클러스터를 생성할 때 필요합니다. 작업자 노드가 시작되는 서브넷입니다.\n  3. Amazon EKS에 대한 kubectl을 설치합니다. Amazon EKS는 IAM을 사용하여 Kubernetes용 AWS IAM Authenticator를 통해 Kubernetes 클러스터에 인증을 제공합니다.\nKubernetes 버전 1.10부터 Kubernetes용 AWS IAM Authenticator를 설치하고 인증에 사용할 kubectl 구성 파일을 사용하여 Amazon EKS를 사용할 stock kubectl 클라이언트를 구성할 수 있습니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\n참조: kubectl 설치 : https://kubernetes.io/docs/tasks/tools/install-kubectl/\n Kubernetes 용 AWS IAM Authenticator는 AWS에서 Kubernetes 설치 프로그램을 작성하는 경우 부트 스트랩 프로세스를 간소화 할 수 있습니다.\n 새로 설치 한 클러스터에서 초기 admin 자격 증명을 어떻게 든 안전하게 훔쳐 낼 필요가 없습니다.\n 대신 클러스터 구축시 전용 KubernetesAdmin 역할을 만들고 Authenticator를 설정하여 클러스터 관리자 로그인을 허용 할 수 있습니다.\n  3-1. Amazon EKS용 aws-iam-authenticator를 설치합니다. Kubernetes 클러스터에 인증하기 위해 AWS IAM 자격 증명을 사용하는 도구 입니다. Kubernetes 용 AWS IAM Authenticator를 사용하면 Kubernetes 액세스에 대한 별도의 자격 증명을 관리하지 않아도됩니다.\n aws-iam-authenticator 바이너리를 다운로드 및 설치합니다.\nAmazon EKS는 사용할 수 있는 aws-iam-authenticator 바이너리를 판매합니다.\n또는 go get 을 사용하여 GitHub의 Kubernetes용 AWS IAM Authenticator 프로젝트에서 다른 운영 체제용 바이너리를 가져올 수 있습니다.\n 1. aws-iam-authenticator 바이너리를 다운로드 및 설치하려면 다음과 같이 합니다.\n Amazon S3에서 aws-iam-authenticator 바이너리를 다운로드합니다.\n Linux: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/linux/amd64/aws-iam-authenticator\n MacOS: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/darwin/amd64/aws-iam-authenticator\n Windows: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/windows/amd64/aws-iam-authenticator.exe\n아래 명령을 사용하여 바이너리를 다운로드하고 플랫폼에 맞는 올바른 URL로 교체합니다. 아래 예는 MacOS 클라이언트에 해당됩니다.\ncurl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/darwin/amd64/aws-iam-authenticator  바이너리에 실행 권한을 적용합니다.\nchmod +x ./aws-iam-authenticator  바이너리를 $PATH의 폴더에 복사합니다. $HOME/bin/aws-iam-authenticator를 생성하고 $PATH가 $HOME/bin로 시작하는 것이 좋습니다.\ncp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH  PATH 환경 변수에 $HOME/bin을 추가합니다.\n MacOS의 Bash 셸의 경우:\necho 'export PATH=$HOME/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile  Linux의 Bash 셸의 경우:\necho 'export PATH=$HOME/bin:$PATH' \u0026gt;\u0026gt; ~/.bashrc   aws-iam-authenticator 바이너리가 작동하는지 테스트합니다.\naws-iam-authenticator help  \u0026lt;!\u0026ndash;\n  2. 또는 go get을 사용하여 GitHub에서 aws-iam-authenticator 바이너리를 설치합니다.\n go가 이미 설치되지 않은 경우 운영 체제에 대해 Go 프로그래밍 언어를 설치합니다. 자세한 내용은 Go 문서의 Install the Go tools를 참조하십시오.\n go get을 사용하여 aws-iam-authenticator 바이너리를 설치합니다.\ngo get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator  PATH 환경 변수에 $HOME/go/bin을 추가합니다.\n MacOS의 Bash 셸의 경우:\nexport PATH=$HOME/go/bin:$PATH \u0026amp;\u0026amp; echo 'export PATH=$HOME/go/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile  Linux의 Bash 셸의 경우:\nexport PATH=$HOME/go/bin:$PATH \u0026amp;\u0026amp; echo 'export PATH=$HOME/go/bin:$PATH' \u0026gt;\u0026gt; ~/.bashrc   aws-iam-authenticator 바이너리가 작동하는지 테스트합니다.\naws-iam-authenticator help  \u0026ndash;\u0026gt;\n   3-2. (선택 사항) 최신 AWS CLI 다운로드 및 설치 합니다. AWS CLI가 Amazon EKS 사용을 명시적으로 요구하지는 않지만, update-kubeconfig 명령을 사용하면 kubeconfig 생성 프로세스가 크게 간소화됩니다. AWS CLI에서 Amazon EKS를 사용하려면 1.16.18 버전 이상의 AWS CLI가 설치되어 있어야 합니다.\nAWS CLI를 설치 또는 업그레이드하려면 AWS Command Line Interface 사용 설명서의 AWS 명령줄 인터페이스 설치를 참조하십시오.\n AWS CLI 버전은 다음 명령을 통해 확인할 수 있습니다.\naws --version   4. 1단계: Amazon EKS 클러스터 생성 이제 Amazon EKS 클러스터를 생성할 수 있습니다.\n참조 : Amazon EKS 클러스터 생성 사용 설명서를 참고 합니다.\n 사전 요구 사항은 다음과 같습니다.\n Amazon EKS 클러스터의 요건을 충족하는 VPC 및 전용 보안 그룹을 생성했습니다.\n자세한 내용은 클러스터 VPC 고려 사항 및 클러스터 보안 그룹 고려 사항 단원을 참조하십시오\n Amazon EKS 서비스 역할을 생성하여 클러스터에 적용했습니다. Amazon EKS 서비스 IAM 역할 가이드를 참조 하시기 바랍니다.\n    AWS EKS 콘솔을 사용하여 클러스터를 생성 합니다.\n 1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n 2. [Create cluster]를 선택합니다.\n 참고\nIAM 사용자에게 관리 권한이 없는 경우 해당 사용자가 Amazon EKS API 작업을 호출하는 권한을 명시적으로 추가해야 합니다.\n자세한 내용은 Amazon EKS IAM 정책 만들기 단원을 참조하십시오.  3. 클러스터 생성 페이지에서 다음 필드를 입력한 다음 생성을 선택합니다.\n Create cluster\n Cluster name :\nAmazon EKS 클러스터에 대해 고유한 이름입니다.\n Kubernetes version :\n클러스터에 대해 사용할 Kubernetes 버전. 기본적으로 최신 버전이 선택됩니다.\n Role name :\nAmazon EKS 서비스 역할 생성 사용을 통해 생성한 IAM 역할을 선택합니다.\n  Networking\n VPC :\nAmazon EKS 클러스터 VPC 생성 사용을 통해 생성한 VPC를 선택합니다.\n드롭다운 목록에서 VPC의 이름을 찾을 수 있습니다.\n Subnets :\n기본적으로 위 VPC에서 사용 가능한 서브넷이 사전 선택됩니다.\n Security groups :\nAmazon EKS 클러스터 VPC 생성 사용을 통해 생성된 보안 그룹(SecurityGroups) 값을 선택 합니다.\n 중요\n작업자 노드 AWS CloudFormation 템플릿이 여기서 사용자가 지정하는 보안 그룹을 수정하므로 클러스터 제어 플레인에 대한 전용 보안 그룹을 사용하는 것이 바람직합니다. 다른 리소스와 공유할 경우 이러한 리소스를 방해하거나 막을 수 있습니다.   정보 입력이 완료 되면 생성(Create)을 선택합니다.\n  4. Clusters 화면에서 새로 생성된 클러스터를 선택하고 클러스터 정보를 확인할 수 있습니다.\n 5. Status(상태) 필드는 클러스터 프로비저닝 프로세스가 완료될 때까지 CREATING(생성 중)으로 표시됩니다.\n클러스터 프로비저닝이 완료될 때(보통 10분 이내) API server endpoint(API 서버 엔드포인트) 및 인증 기관 값을 기록합니다. 이 값은 kubectl 구성에 사용됩니다.\n 6. 클러스터를 생성했으므로 이제 Amazon EKS용 kubectl 구성 및 Amazon EKS에 대한 kubeconfig 생성의 절차를 따라 새 클러스터와의 통신을 활성화합니다.\n   4-1. Amazon EKS용 kubectl 구성 AWS CLI로 kubeconfig를 생성하려면 Amazon EKS에 대한 kubeconfig 생성을 참조 하세요.\n 기본 AWS 자격 증명 공급자 체인을 사용 하려면 :\nAWS IAM 권한이 있는 관리자에게 사용자 보안 자격 증명(액세스 키)을 요청 합니다.\n AWS 자격 증명 공급자 체인 만들기\n AWS IAM service 콘솔에서 사용자(Users)를 선택합니다.\n 보안 자격 증명(Security credentials) 섹션 에서 액세스 키를 생성합니다.\n   ~/.aws/credentials 파일 안에 해당 키 값을 입력 합니다.\n 윈도우 경우 사용자 디렉토리 안에 .aws\\redentials 파일이 있습니다.  [default] aws_access_key_id= aws_secret_access_key=   또는 AWS CLI를 사용해서 클러스터를 생성한 계정의 IAM유저의 보안자격증명을 설정합니다.  # aws configure AWS Access Key ID [None]: AKIAIHE#########FELA AWS Secret Access Key [None]: uqm8MSDK######################Ddz29PcAz5 Default region name [ap-northeast-2]: Default output format [None]: # aws sts get-caller-identity { “Account”: “4952894#####”, “UserId”: “AIDAJU5LO##########LQ”, “Arn”: “arn:aws:iam::4952894#####:user/[username]” }   AWS CLI로 kubeconfig를 생성하려면 :\n# aws eks --region region update-kubeconfig --name cluster_name Updated context arn:aws:eks:ap-northeast-2:235896307296:cluster/cocktail-test-eks-cluster in C:\\Users\\shapj\\.kube\\config # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 1m   5. 2단계: Amazon EKS Work node 시작 및 구성 Amazon EKS work node는 AWS 계정에서 실행되고, 클러스터 API 서버 엔드포인트를 통해 Cluster Control Plane에 연결합니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/clusters.html\n 중요\nAmazon EKS Work node는 표준 Amazon EC2 인스턴스이고, 일반 Amazon EC2 온디맨드 인스턴스 가격을 기반으로 비용이 청구됩니다.\n자세한 내용은 Amazon EC2 요금을 참조하십시오.\n 이 주제의 사전 요구 사항은 다음과 같습니다.\n Amazon EKS 클러스터의 요건을 충족하는 VPC 및 보안 그룹을 생성했습니다.\n Amazon EKS 클러스터를 생성하고 위 VPC 및 보안 그룹에 사용한다고 지정했습니다.\n 시작 이후 SSH를 사용하여 Work node에 연결하는 데 사용할 수 있는 Amazon EC2 SSH 키 페어가 생성되어 있어야 합니다.\n참조 : Amazon EC2를 사용해 키 페어 만들기\n   5-1. 템플릿을 사용해서 Work node를 시작하려면  클러스터 상태가 ACTIVE가 되기를 기다립니다.\n※ 클러스터가 활성화되기 전에 Work node를 시작하면 Work node가 클러스터에 등록되지 않고 Work node를 다시 시작해야 한다.\n https://console.aws.amazon.com/cloudformation 에서 AWS CloudFormation 콘솔.\n Amazon EKS를 생성할 리전을 선택한다.\n이 사용 설명서에서는 서울 리전을 사용한다. \u0026lt;!\u0026ndash; 참고\n현재 다음 리전에서 Amazon EKS를 사용할 수 있습니다. (한국어 사용 설명서는 아직 반영이 안되어 있습니다.)\n US West (Oregon) (us-west-2)\n US East (N. Virginia) (us-east-1)\n US East (Ohio) (us-east-2)\n EU (Frankfurt) (eu-central-1)\n EU (Stockholm) (eu-north-1)\n EU (Ireland) (eu-west-1)\n Asia Pacific (Tokyo) (ap-northeast-1)\n Asia Pacific (Seoul) (ap-northeast-2)\n Asia Pacific (Singapore) (ap-southeast-1)\n Asia Pacific (Sydney) (ap-southeast-2) \u0026ndash;\u0026gt;\n  [Create stack]을 선택한다.\n 템플릿 선택에서 Amazon S3 템플릿 URL 지정을 선택한다.\n 텍스트 영역에 다음 URL을 붙여넣고 다음을 선택한다.\nhttps://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml  Specify Details(세부 정보 지정) 페이지에서 다음 파라미터를 입력하고 다음을 선택한다.\n 스택 이름 :\nAWS CloudFormation 스택에 대한 스택 이름을 선택합니다. 예를 들어 -worker-nodes로 사용할 수 있습니다.\n ClusterName :\nAmazon EKS 클러스터 생성 시 사용할 이름을 입력합니다.\n중요 : 이 이름은 Amazon EKS 클러스터 이름과 정확하게 일치해야 합니다. 그렇지 않은 경우 Work node가 조인할 수 없습니다.\n ClusterControlPlaneSecurityGroup :\nAmazon EKS 클러스터를 생성할 때 사용한 보안 그룹 또는 그룹을 입력합니다. 이 AWS CloudFormation 템플릿은 지정된 클러스터 제어 플레인 보안 그룹의 트래픽을 허용하는 Work node 보안 그룹을 생성합니다.\n NodeGroupName :\n노드 그룹의 이름을 입력합니다. 이 이름은 나중에 Work node에 대해 생성된 Auto Scaling 노드 그룹을 식별하는 데 사용할 수 있습니다.\n NodeAutoScalingGroupMinSize :\nAuto Scaling 그룹이 축소할 수 있는 Work node의 최소 노드 수를 입력합니다.\n NodeAutoScalingGroupDesiredCapacity : 스택을 생성할 때 조정할 원하는 노드 수를 입력합니다.\n NodeAutoScalingGroupMaxSize :\nAuto Scaling 그룹이 확장할 수 있는 Work node의 최대 노드 수를 입력합니다.\n NodeInstanceType :\nWork node에 대한 인스턴스 유형을 선택합니다.\n NodeImageId :\n리전에 대한 현재 Amazon EKS Work node AMI ID를 입력합니다.\n최신 Amazon EKS 최적화 AMI(GPU 지원 유무는 상이)의 AMI ID는 다음 표에 나와 있습니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html\nKubernetes version 1.12.7\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     Asia Pacific (Tokyo) (ap-northeast-1) ami-0bfedee6a7845c26d ami-08e41cc84f4b3f27f   Asia Pacific (Seoul) (ap-northeast-2) ami-0a904348b703e620c ami-0c43b885e33fdc29e   Asia Pacific (Sydney) (ap-southeast-2) ami-0f0121e9e64ebd3dc ami-07079cd9ff1b312da    Kubernetes version 1.11\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     US West (Oregon) (us-west-2) ami-0a2abab4107669c1b ami-0c9e5e2d8caa9fb5e   US East (N. Virginia) (us-east-1) ami-0c24db5df6badc35a ami-0ff0241c02b279f50   US East (Ohio) (us-east-2) ami-0c2e8d28b1f854c68 ami-006a12f54eaafc2b1   EU (Frankfurt) (eu-central-1) ami-010caa98bae9a09e2 ami-0d6f0554fd4743a9d   EU (Stockholm) (eu-north-1) ami-06ee67302ab7cf838 ami-0b159b75   EU (Ireland) (eu-west-1) ami-01e08d22b9439c15a ami-097978e7acde1fd7c   Asia Pacific (Tokyo) (ap-northeast-1) ami-0f0e8066383e7a2cb ami-036b3969c5eb8d3cf   Asia Pacific (Seoul) (ap-northeast-2) ami-0b7baa90de70f683f ami-0b7f163f7194396f7   Asia Pacific (Singapore) (ap-southeast-1) ami-019966ed970c18502 ami-093f742654a955ee6   Asia Pacific (Sydney) (ap-southeast-2) ami-06ade0abbd8eca425 ami-05e09575123ff498b    Kubernetes version 1.10\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     US West (Oregon) (us-west-2) ami-09e1df3bad220af0b ami-0ebf0561e61a2be02   US East (N. Virginia) (us-east-1) ami-04358410d28eaab63 ami-0131c0ca222183def   US East (Ohio) (us-east-2) ami-0b779e8ab57655b4b ami-0abfb3be33c196cbf   EU (Frankfurt) (eu-central-1) ami-08eb700778f03ea94 ami-000622b1016d2a5bf   EU (Stockholm) (eu-north-1) ami-068b8a1efffd30eda ami-cc149ab2   EU (Ireland) (eu-west-1) ami-0de10c614955da932 ami-0dafd3a1dc43781f7   Asia Pacific (Tokyo) (ap-northeast-1) ami-06398bdd37d76571d ami-0afc9d14b2fe11ad9   Asia Pacific (Seoul) (ap-northeast-2) ami-08a87e0a7c32fa649 ami-0d75b9ab57bfc8c9a   Asia Pacific (Singapore) (ap-southeast-1) ami-0ac3510e44b5bf8ef ami-0ecce0670cb66d17b   Asia Pacific (Sydney) (ap-southeast-2) ami-0d2c929ace88cfebe ami-03b048bd9d3861ce9    KeyName :\n시작 이후 SSH를 사용하여 Work node에 연결하는 데 사용할 수 있는 Amazon EC2 SSH 키 페어 이름을 입력합니다.\nAmazon EC2 키 페어가 아직 없는 경우 AWS Management 콘솔에서 새로 생성할 수 있습니다.\n자세한 내용은 Linux 인스턴스용 Amazon EC2 사용 설명서의 Amazon EC2 키 페어를 참조하십시오.\n참고 : 여기에 키 페어를 입력하지 않으면 AWS CloudFormation 스택이 생성되지 않습니다.\n BootstrapArguments :\n별도의 kubelet 인수와 같이 Work node 부트스트랩 스크립트에 전달할 선택적 인수를 지정합니다.\n자세한 내용은 https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh에서 부트스트랩 스크립트 사용 정보를 참조하십시오.\n VpcId :\nWork node에서 시작해야 하는 VPC의 ID를 입력합니다.\n Subnets :\nWork node에서 시작해야 하는 위 VPC 내 서브넷을 선택합니다.\n 옵션 페이지에서 스택 리소스에 태그를 지정할 수 있습니다. [Next]를 선택합니다.\n 검토 페이지에서 정보를 검토하고, 스택이 IAM 리소스를 생성할 수 있음을 승인합니다. 항목을 체크 한 다음 생성을 선택합니다.\n 스택이 생성된 후 콘솔에서 이를 선택하고 출력을 선택합니다.\n 생성된 노드 그룹에 대해 NodeInstanceRole을 기록합니다. Amazon EKS Work node를 구성할 때 필요합니다.\n    5-2. Work node가 클러스터에 조인하도록 하려면  AWS IAM Authenticator 구성 맵을 다운로드, 편집 및 적용합니다.\n ConfigMap 다운로드:\ncurl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml  텍스트 편집기에서 파일을 엽니다. 인스턴스 역할의 \u0026lt;ARN(비인스턴스 프로파일)\u0026gt; 조각을 이전 절차에서 기록한 NodeInstanceRole 값으로 교체하고 파일을 저장합니다.\n중요:\n이 파일에서 어떠한 행도 수정하지 마십시오.\napiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: \u0026lt;ARN of instance role (not instance profile)\u0026gt; username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes  구성을 적용합니다. 이 명령을 완료하는 데 몇 분이 걸릴 수 있습니다.\nkubectl apply -f aws-auth-cm.yaml   노드의 상태를 확인하고 Ready 상태가 될 때까지 대기합니다.\nkubectl get nodes --watch  Kubernetes Metric Server 설치 한다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n Cocktail-addon을 설치 한다.\n  Amazon Elastic File System 만들기 Amazon Elastic File System (Amazon EFS)은 Amazon EC2에서 사용하기 위한 간단하고 확장 가능한 파일 스토리지를 제공합니다.\n참조 : https://docs.aws.amazon.com/ko_kr/efs/latest/ug/whatisefs.html\n1. 보안그룹 생성  EFS 파일 시스템이 사용할 보안 그룹을 생성 한다.  2. Amazon EFS 파일 시스템 생성  Amazon EFS 파일 시스템 생성\n Amazon EFS 관리 콘솔을 엽니다.\n Create File System(파일 시스템 생성)을 선택합니다.\n VPC와 연결된 시큐리티그룹을 선택(NFS 포트가 열려 있어야한다./ port : 2049)\n 목록에서 탑재 대상을 생성하기 원하는 서브넷을 선택한다.\n IP 주소를 자동으로 선택.\n [Next Step]을 선택합니다.\n 파일 시스템의 이름을 지정하고, 기본 성능 및 처리량 모드로 선택한 범용 및 버스팅 모드를 유지한 후 다음 단계를 선택합니다.\n Create File System(파일 시스템 생성)을 선택합니다.\n 목록에서 파일 시스템을 선택하고 File system ID(파일 시스템 ID)에 대한 값을 기록합니다. 다음 단계에 이 값이 필요합니다.\n   4. EFS Persistent Volumes on Kubernetes on AWS 참조 : Amazon EFS를 선택해야하는 경우\n Deploy the efs-provisioner\n참조 : https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs\nefs-provisioner를 사용하면 EFS 저장소를 kubernetes의 PersistentVolume으로 마운트 할 수 있습니다.\nAWS EFS 리소스에 액세스 할 수있는 컨테이너로 구성됩니다.\n컨테이너는 EFS 파일 시스템 ID, AWS 영역 및 efs-provisioner에 사용할 이름을 포함하는 configmap을 읽습니다.\n 아래 스크립트를 사용 합니다.\n kubectl apply -f 1-eks-cocktail-efs-rbac.yaml\n클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.\n\u0026ldquo;default\u0026rdquo;가 아닌 네임 스페이스 / 프로젝트에 있다면 rbac.yaml을 편집하십시오.\n  --- # ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: efs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: efs-provisioner-runner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-efs-provisioner subjects: - kind: ServiceAccount name: efs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: efs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-efs-provisioner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-efs-provisioner subjects: - kind: ServiceAccount name: efs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-efs-provisioner apiGroup: rbac.authorization.k8s.io   kubectl apply -f 2-eks-cocktail-efs-provisioner.yaml\nDeployment efs-provisione  --- apiVersion: v1 kind: ConfigMap metadata: name: efs-provisioner data: # your file.system.id , aws.region file.system.id: fs-b4c21dd5 aws.region: ap-northeast-2 provisioner.name: acornsoft.io/aws-efs dns.name: \u0026quot;\u0026quot; --- --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: efs-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: efs-provisioner spec: serviceAccount: efs-provisioner containers: - name: efs-provisioner image: quay.io/external_storage/efs-provisioner:latest env: - name: FILE_SYSTEM_ID valueFrom: configMapKeyRef: name: efs-provisioner key: file.system.id - name: AWS_REGION valueFrom: configMapKeyRef: name: efs-provisioner key: aws.region - name: DNS_NAME valueFrom: configMapKeyRef: name: efs-provisioner key: dns.name optional: true - name: PROVISIONER_NAME valueFrom: configMapKeyRef: name: efs-provisioner key: provisioner.name volumeMounts: - name: pv-volume mountPath: /persistentvolumes volumes: - name: pv-volume nfs: # your file-system DNS NAME server: fs-b4c21dd5.efs.ap-northeast-2.amazonaws.com path: / ---   kubetl apply -f 3-eks-cocktail-efs-storage-class.yaml\nCocktail 퍼시스턴트 볼륨 Storage Class 생성  # single-storage : EBS-PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: single-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: AWSEBS acornsoft.io/type: SINGLE k8s-addon: storage-aws.addons.k8s.io provisioner: kubernetes.io/aws-ebs reclaimPolicy: Delete volumeBindingMode: Immediate parameters: type: gp2 --- # shared-storage : EFS-PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: shared-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: AWSEFS acornsoft.io/type: SHARED k8s-addon: storage-aws.addons.k8s.io # your provisioner.name provisioner: efs-test-pvc/aws-efs ---   4-eks-cocktail-efs-sample-pvc.yaml 샘플 PVC  # sample pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: efs-test-pvc annotations: volume.beta.kubernetes.io/storage-class: \u0026quot;shared-storage\u0026quot; spec: storageClassName: shared-storage accessModes: - ReadWriteMany resources: requests: storage: 1Mi ---   아래 명령으로 퍼시스턴트 볼륨을 확인 합니다.   kubectl get pv    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.1.kass/8.1.1.4-cluster-kass-eks-policy/",
	"title": "8.1.1.3 EKS 클러스터 정책 / 권한",
	"tags": [],
	"description": "",
	"content": " 사용자 권한 / 정책 만들기 AWS 관리자 권한이 없다면 필요한 역할 / 정책 / 권한 을 요청 해야 합니다.\n1. 사용자 권한 / 정책 사용자가 클러스터를 생성 / 관리할 수 있는 권한 / 정책이 필요 합니다.\n CloudFormation - 전체: 목록, 쓰기 제한: 읽기 / 이에 사용되는 리소스 권한이 있어야 합니다.\nAWS CloudFormation은 개발자와 시스템 관리자에게 관련 AWS 리소스 모음을 손쉽게 생성 및 관리하고 순서 있고 예측 가능한 방식으로 프로비저닝하고 업데이트 할 수있는 방법을 제공합니다.\n EC2 - AmazonEC2FullAccess 권한이 있어야 합니다.\nAmazon Elastic Compute Cloud (Amazon EC2)는 클라우드에서 안전하고 크기 조정이 가능한 컴퓨팅 용량을 제공하는 웹 서비스입니다.\n EKS - 모든 액세스 / 모든 리소스 권한이 있어야 합니다.\nAmazon Elastic Container Service for Kubernetes (EKS)는 자체 Kubernetes control plane을 설치, 작동 및 유지할 필요없이 Kubernetes를 AWS에서 쉽게 실행할 수 있도록 관리되는 Kubernetes 서비스입니다.\n IAM - 목록, 읽기, 쓰기, 권한 관리 / 이에 사용되는 리소스 권한이 있어야 합니다.\nAWS Identity and Access Management (IAM)를 사용하면 AWS 서비스 및 리소스에 대한 액세스를 안전하게 관리합니다.\n SNS - 목록 / 모든 리소스 권한이 있어야 합니다.\nAmazon Simple Notification Service(SNS)는 마이크로서비스, 분산 시스템 및 서버리스 애플리케이션을 쉽게 분리할 수 있게 해 주는 내구적이고 안전한 고가용성의 완전 관리형 게시/구독 메시징 서비스입니다\n VPC - AWS Management Console을 통해 Amazon VPC에 대한 완벽한 액세스를 제공합니다.\nAmazon Virtual Private Cloud(Amazon VPC)에서는 사용자가 정의한 가상 네트워크로 AWS 리소스를 시작할 수 있습니다.\n  2. 사용자 권한 / 정책 만들기  IAM(Identity and Access Management) 에서 정책(Policies)을 선택합니다.\n 정책 생성(Create policy) / 기존의 정책 편집(Edit policy)을 선택합니다.\n 권한 추가(Add additional permissions)를 선택합니다.\n 서비스 항목에서 필요한 서비스를 검색 / 선택합니다.\n CloudFormation - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAWS CloudFormation은 Amazon Web Services 리소스를 모델링하고 설정하는 데 도움이되는 서비스로, 리소스 관리 시간을 줄이고 AWS에서 실행되는 애플리케이션에 집중할 수 있습니다.\n원하는 모든 AWS 리소스 (예 : Amazon EC2 인스턴스 또는 Amazon RDS DB 인스턴스)를 설명하는 템플릿을 만들고 AWS CloudFormation은 이러한 리소스를 프로비저닝하고 구성하는 작업을 담당합니다.\nAWS 리소스를 개별적으로 생성하고 구성 할 필요가 없으며 무엇에 의존하는지 파악할 수 있습니다. Learn more\n EKS - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nKubernetes 용 Amazon Elastic Container Service (Amazon EKS)는 자체 Kubernetes control plane을 세우거나 유지할 필요없이 AWS에서 Kubernetes를 쉽게 실행할 수 있도록 해주는 관리 형 서비스입니다.\n IAM - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAWS ID 및 액세스 관리 (IAM)는 AWS 리소스에 대한 액세스를 안전하게 제어 할 수있는 웹 서비스입니다.\nIAM을 사용하여 자원을 사용하도록 인증 (로그인)되고 권한이 부여 된 (사용 권한이있는) 사용자를 제어합니다.\n SNS - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAmazon SNS (Amazon Simple Notification Service)는 가입 엔드 포인트 또는 클라이언트로 메시지를 전달하거나 메시지 서비스를 조정 및 관리하는 웹 서비스입니다.\n게시자는 논리 액세스 포인트 및 통신 채널 인 주제에 메시지를 생성하고 전송하여 구독자와 비동기 적으로 통신합니다.\n  정책 검토(Review policy)를 선택합니다.\n 이름(Name) :\nAmazonEKSAdminPolicy와 같은 사용자 정책 이름을 입력 합니다.\n 설명(Description) :\n사용자 정책의 설명을 입력 합니다.\n  정책 생성을 선택합니다.\n  IAM(Identity and Access Management) 에서 사용자(Users)를 선택합니다.\n 생성한 또는 편집한 정책을 사용자에게 추가 합니다.\n 권한 추가(Add permission)를 선택합니다.\n 기존 정책 직접 연결(Attach existing policies directly )을 선택합니다.\n 생성 및 편집 한 정책을 검색 해서 사용자에게 권한 추가 합니다.\n 이 사용 설명서 에서는 다음 과 같은 정책으로 사용 하였습니다.\n AmazonEKSAdminPolicy :\nCloudFormation, EKS, IAM, SNS\n AmazonVPCFullAccess :\nProvides full access to Amazon VPC via the AWS Management Console.\nAmazon EC2를 사용하여 필요한만큼 가상 서버를 시작하고 보안 및 네트워킹을 구성하며 스토리지를 관리 할 수 있습니다.\n AmazonEC2FullAccess :\nProvides full access to Amazon EC2 via the AWS Management Console.\nAmazon Elastic Compute Cloud (Amazon EC2)는 Amazon Web Services (AWS) 클라우드에서 확장 가능한 컴퓨팅 용량을 제공합니다.\nAmazon EC2를 사용하여 필요한만큼 가상 서버를 시작하고 보안 및 네트워킹을 구성하며 스토리지를 관리 할 수 있습니다.\n     이 사용 설명서에서는 아래와 같이 권한을 사용합니다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.1.kass/8.1.1.5-cluster-kass-eks-quick-start/",
	"title": "8.1.1.5 EKS 클러스터 만들기 Quick Start",
	"tags": [],
	"description": "",
	"content": "1. 필수 Role(역할) / Policy(정책) 만들기 a. EKS 관리 역할 만들기. b. IAM(services) \u0026gt; Roles \u0026gt; Create role 화면에서 i. EKS 선택 : AmazonEKSClusterPolicy , AmazonEKSServicePolicy (default로 구성되어 있다) ii. Next: Permissions \u0026gt; Next: Tags \u0026gt; Next: Revies iii. Review 화면에서 1) Role name : EKSClusterRole 2) Create role 클릭 c. EKS 관리 정책 만들기. d. IAM(services) \u0026gt; Policies \u0026gt; Create policy 화면에서 i. Choose a service \u0026gt; Service : EKS 검색 및 선택 ii. Action : All EKS actions 선택 iii. Resources : All Resources 선택 \u0026gt; Add additional permissions 클릭\u000biv. Choose a service \u0026gt; Service : SNS 검색 및 선택 v. Action : List \u0026gt; ListTopics 선택 \u0026gt; Review policy 클릭 vi. Review Policy 화면에서 1) Name* : AmazonEKSAdminPolicy 2) Create policy 클릭 2. EKS생성 하는 User 권한 / 정책 부여 하기 a. IAM(Services) \u0026gt; Users \u0026gt; EKS 생성 하는 user 클릭 i. Summary 화면에서 \u0026gt; Add permissions 클릭 ii. Grant permissions 에서 Attach existing policies directly 선택. 1) AmazonEKSAdminPolicy 2) AmazonEC2FullAccess 3) IAMFullAccess 4) AmazonEKSClusterPolicy 5) AmazonVPCFullAccess 6) AmazonEKSServicePolicy 7) AmazonElasticFileSystemFullAccess 8) AWSDeepRacerCloudFormationAccessPolicy iii. 위 목록을 검색 / 선택 해서 추가 한다. 3. 보안을 위한 아무 권한이 없는 칵테일 클러스터 등록 유져 만들기 a. IAM(services) \u0026gt; Users \u0026gt; Add user 화면에서 i. User name : acloud-client ii. Access type* : Programmatic access 선택 b. : \u0026gt; Next: Permissions \u0026gt; Next: Tags \u0026gt; Next: Review \u0026gt; Create user c. 완료되면 Download.csv를 받아서 저장한다. 4. VPC 만들기 - cloudformation template 사용 i. AWS console에서 CloudFormation 메뉴에서 \u0026quot;Create Stack\u0026quot;버튼 클릭 ii. Choose a template에서 \u0026quot;Specify an Amazon S3 template URL\u0026quot; 선택하고 아래 값을 입력 https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml iii. Stack name: eks-cocktail로 입력 iv. 2~3분정도 걸림. 5. EKS Cluster 생성 (Master 생성) a. AWS console에서 EKS 메뉴로 이동 i. Cluster name: cocktail-eks-cluster ii. Kubernetes Version : 1.12 iii. Role name : EKSClusterRole 선택 (1번에서 생성함) iv. VPC : eks-vpc-stack-VPC 선택 (4번에서 생성한 vpc) v. Subnet : default(모두 선택) vi. Security groups : eks-vpc-stack-ControlPlaneSecurityGroup 선택 (4번에서 생성된 security group) vii. Private access : Disabled (default) 선택 viii. Public access : Enabled (default) 선택 ix. Create -\u0026gt; 5분 정도 소요됨. 리플레쉬 버튼으로 확인 6. Aws-iam-authenticator, aws cli, kubectl 설치. (5번 진행되는 동안 설치 진행) a. Aws-iam-authenticator 설치: https://acornapps.github.io/cocktail-manual-3.1.1/docs/ko/8.1.1.3/ 의 3.1 참고 b. Aws cli 설치: https://acornapps.github.io/cocktail-manual-3.1.1/docs/ko/8.1.1.3/의 3.2 참고 c. Kubectl 설치: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl 참고 d. aws eks --region {region_code} update-kubeconfig --name {cluster_name} --kubeconfig {path_to_kubeconfig} e. export KUBECONFIG={path_to_kubeconfig} f. Kubectl get svc 로 접속확인 7. Node 생성 i. Cloudformation에서 Choose a template에서 \u0026quot;Specify an Amazon S3 template URL\u0026quot; 선택하고 아래 값을 입력 https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml ii. Specify Details 화면에서 i. Stack name: eks-nodes-stack ii. ClusterName : 생성한 EKS Cluster name iii. ClusterControlPlaneSecurityGroup : eks-vpc-stack-ControlPlaneSecurityGroup 선택 iv. NodeGroupName: cocktail-node-group v. NodeAutoScalingGroupMinSize : 1 (최소 1개) vi. NodeAutoScalingGroupDesiredCapacity : 2 (생성 할 노드 개수) vii. NodeAutoScalingGroupMaxSize : 2 (AutoScaling 최대 개수) viii. NodeInstanceType : t3.xlarge 선택 (설명 별첨 1) ix. NodeImageId : ami-0bfedee6a7845c26d (최신 ami 목록 별첨 2) x. NodeVolumeSize : 100 (노드 총 사이즈) xi. KeyName : 생성 한 키 페어 선택 xii. BootstrapArguments : 공백 (default) xiii. VpcId : 위에서 생성한 eks-vpc-stack-VPC 선택 xiv. Subnets : 위에서 생성한 eks-vpc-stack-Subnet01, eks-vpc-stack-Subnet02, eks-vpc-stack-Subnet03 xv. Next xvi. Options 화면은 default \u0026gt; Next xvii. Review 화면에서 I acknowledge that AWS CloudFormation might create IAM resources. 체크 xviii. Create iii. Node 생성 (5분내외 소요) 8. Node join 하기 i. curl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml ii. vi aws-auth-cm.yaml ``` apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: ( CloudFormation \u0026gt; 노드 생성 stack 에서 Outputs \u0026gt; NodeInstanceRole 의 value ) username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes mapUsers: | - userarn: ( IAM \u0026gt; Users \u0026gt; 위에서 생성한 acloud-client 선택 \u0026gt; User ARN 값 입력 ) username: admin groups: - system:masters ``` iii. kubectl apply -f aws-auth-cm.yaml iv. kubectl get cm --all-namespaces ``` NAMESPACE NAME DATA AGE kube-system aws-auth 2 20s ``` v. kubectl get nodes 9. EFS(Elastic File System) 생성 a. EFS 연동을 위한 Security Group 생성 i. EC2 \u0026gt; Security Groups 화면 에서 \u0026quot;Create Security Group\u0026quot; 버튼 클릭 -- Create Security Group -- Security group name : efs-sg Description : sfs-sg VPC : 4번에서 생성한 vpc 선택 ii. Add rule 버튼 클릭 Type : Custom TCP Protocol : TCP Port Range : 2049 Source : Custom, eks-cocktail-nodes-NodeSecurityGroup 을 선택하고 Create 버튼 클릭 b. Node에 생성된 Security Group 적용 i. EC2 \u0026gt; Instances 화면 에서 개별 노드 선택 ii. Action 클릭 \u0026gt; Networking 클릭 \u0026gt; Change Security Groups 선택 iii. Change Security Groups 화면 에서 생성된 efs-sg를 포함하여 \u0026quot;Assign Security Groups\u0026quot; 버튼을 클릭. c. EFS 생성 i. EFS \u0026gt; Create File System 버튼 클릭 ii. VPC : 4번에서 생성한 vpc 선택 iii. 하단의 Create mount targets의 security groups에 EFS 연동을 위해 생성한 security group만 선택 iv. Next Step 버튼 클릭 v. Configure optional settings 화면에서 수정없이 \u0026quot;Next Step\u0026quot; 버튼클릭. vi. Review and create 화면에서 \u0026quot;Create File System\u0026quot;버튼 클릭하여 생성함. 10. Cocktail Addon i. 제공되스 스크립트를 사용 하여 일관 apply 한다. i. ./addon.sh \u0026lt;your kubeconfig-path\u0026gt; \u0026lt;your efs-file-system-id\u0026gt; \u0026lt;your region\u0026gt; \u0026lt;your cocktail cluster-id\u0026gt; 11. Cocktail Cluster 등록 i. Cluster 등록 -- Provider -- i. Provider : Amazon Web Service ii. Type : EKS iii. Region : Tokyo iv. Cluster ID : \u0026lt;your EKS Cluster Name\u0026gt; -- Basic Infomation -- i. name : \u0026lt;your Cocktail Cluster Name\u0026gt; ii. Kubernetes Version : 1.12.6 (EKS Kubernetes Version) iii. ID : \u0026lt;your Cocktail Cluster Id\u0026gt; -- Endpoint -- i. Master address : \u0026lt;your EKS API server endpoint\u0026gt; ii. Ingress host address : \u0026lt;your Load Balancer DNS name\u0026gt;(선택사항) 1) LB 없을시 Work node의 public ip를 사용 해서 확인할 수 있다. iii. Node Port Host Address : \u0026lt;your Load Balancer DNS name\u0026gt;(선택사항) 1) LB 없을시 Work node의 public ip를 사용 해서 확인할 수 있다. iv. Node Port Range : 30000-32767 -- User Account Authentication Information -- i. Cluster CA Certification : \u0026lt;your EKS화면에서 Certificate authority 값\u0026gt; ii. AWS Access Key ID : \u0026lt;your acloud-client Access Key ID\u0026gt;(보안을 위해 생성한 더미 유저) iii. AWS Secret Access Key : \u0026lt;your acloud-client Secret Access Key\u0026gt;(보안을 위해 생성한 더미 유저) ii. Workspace Create i. Workspace Name : \u0026lt;your workspaces name\u0026gt; ii. Image Registry : \u0026lt;your workspaces name\u0026gt; iii. Cluster Add 클릭 1) 등록한 Cocktail Cluster를 선택 후 생성 / 업데이트 한다. 참조 : EKS cluster 에서 elb 생성 없이 nodepott / ingress 확인 하기 i. Work Node의 public ip로 확인 할 수 있다. 1) Cocktail cluster 등록 / 수정 화면에서 인그레스 호스트 주소 , 노트 포트 호스트 주소 를 수정 한다. ## 별첨 목록 1. AWS instance별 pod 제한 개수: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt 2. 최신 ami 목록 참조 : https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.2.manual-install/8.1.2.1-onpremise/",
	"title": "8.1.2.1 Onpremise에 설치",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.2.manual-install/8.1.2.2-aws/",
	"title": "8.1.2.2 AWS에 설치",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.2.manual-install/8.1.2.3-azure/",
	"title": "8.1.2.3 Azure에 설치",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.2.manual-install/8.1.2.4-gcp/",
	"title": "8.1.2.4 GCP에 설치",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.1.installation/8.1.2.manual-install/8.1.2.5-aliyun/",
	"title": "8.1.2.5 Aliyun에 설치",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.2.operation/",
	"title": "8.2 클러스터 운영",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.3.troubleshooting/",
	"title": "8.3 TroubleShooting",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.3.troubleshooting/8.3.1-cluster/",
	"title": "8.3.1 k8s cluster의 health check",
	"tags": [],
	"description": "",
	"content": "Cube로 설치한 장비에서 Kubernetes와 칵테일의 주요 구성 요소에 대해 상태 체크를 하기 위한 방법은 아래와 같다.\n이는 cube로 설치후 k8s 주요 구성 요소의 정상 동작 여부를 확인하는데 사용할 수 있다.\n cube status 명령을 통한 etcd, k8s pod, monitoring pod, 칵테일 pod 상태를 체크할 수 있다.  etcd daemon은 kubernetes에서 생성되는 각종 object의 데이터 저장소로서 HA환경일 경우 etcd cluster로 구성(최소 3대)된다. 아래 명령으로 etcd cluster leader 및 상태를 조회할 수 있다.\netcd cluster의 정상 동작여부를 확인할 수 있다. 특정 node가 unhealth상태일 경우 해당 메시지가 출력된다. $ etcdlet endpoint health member 343f53e92dacc502 is healthy: got healthy result from http://192.168.0.172:2379 member c9e93f4efc1a9813 is healthy: got healthy result from http://192.168.0.173:2379 member f95af587a997f2bf is healthy: got healthy result from http://192.168.0.171:2379 cluster is healthy etcd cluster를 구성하는 node의 ip, port와 어느 node가 leader로 구성되었는지 확인할 수 있다. $ cube status   Docker 상태 체크  # systemctl status docker   kubelet 데몬 상태 체크  # systemctl status kubelet   system log message 조회  시스템 로그에는 k8s 기본 컴포넌트의 로그정보가 실시간으로 기록되기 때문에 문제 발생시 해당 log를 tail로 조회하여 오류 사항을 먼저 파악하는 것이 효과적이다.\n# tail -f /var/log/messages   master와 node간 정상 접속 여부 확인  kubectl 명령을 활용하여 master 서버에서 cluster에 포함되어 있는 node 연결 상태를 조회할 수 있다.\n# kubectl get nodes  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.3.troubleshooting/8.3.2-dns/",
	"title": "8.3.2 사용자의 micro service가 서로 연결이 안될때.",
	"tags": [],
	"description": "",
	"content": "일반적으로 k8s에서 각 Container들은 연관된 Container와 통신하기 이해 Kubernetes DNS를 사용한다. 즉 POD는 생성과 삭제가 일어나면 POD가 가지고 있던 내부 cluster ip는 변동되게 됨으로 일반적으로 micro service들을 k8s dns를 통해 필요한 container를 찾는다.\n먼저 k8s의 DNS 서비스가 정상 동작 중인지 확인하는 방법은 busybox를 생성하고 컨테이너에 접속해서 nslookup명령으로 k8s의 DNS에 접속되는지 확인한다.\n busybox container 생성  아래는 busybox의 yaml 파일로 이를 busybox.yaml로 저장한다.\napiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - image: busybox command: - sleep - \u0026quot;3600\u0026quot; imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always   마스터 서버에 ssh로 접속하여 kubectl 명령으로 busybox container를 생성한다.  // busybox 생성 # kubectl create -f busybox.yaml // busybox container가 생성되는지 확인 # kubectl get pods [root@master1 test]# kubectl get pods NAME READY STATUS RESTARTS AGE busybox 1/1 Running 0 1m ...   Busybox container에 접속하여 nslookup 명령으로 kubernetes의 DNS 접속여부를 확인한다.  // busybox에 접속후 shell 실행 # kubectl exec -it busybox -- /bin/sh // kubernetes.default dns 접속여부 확인 / # nslookup kubernetes.default Server: 100.64.0.10 Address 1: 100.64.0.10 kube-dns.kube-system.svc.cube Name: kubernetes.default Address 1: 100.64.0.1 kubernetes.default.svc.cube // 다른 서비스 접속 여부 확인 (아래는 예시로 청지윈 component중 api server를 lookup한 예임) / # nslookup cocktail-api.cocktail-system Server: 100.64.0.10 Address 1: 100.64.0.10 kube-dns.kube-system.svc.cube Name: cocktail-api.cocktail-system Address 1: 100.72.213.63 cocktail-api.cocktail-system.svc.cube  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.3.troubleshooting/8.3.2-nfs/",
	"title": "8.3.3 NFS 서버의 접속 확인 방법.",
	"tags": [],
	"description": "",
	"content": "칵테일 서비스는 기본적으로 Storage class를 이용하여 persistence volume을 사용한다. local 및 baremetal 환경에서는 NFS 서버를 구축하여 Storage class로 활용하며 public cloud에서는 해당 서비스 제공자가 기본적으로 제공해 주는 storage class를 사용하게 된다.\n본 문서는 local 및 Baremetal 환경에서 NFS 서버의 상태 및 칵테일 service와의 연결이 정상적인지 확인하는 방법을 기술한다.\n NFS 서버의 설치 및 기동 ( nfs server 주소가 192.168.0.175로 가정하고, cluster를 구성하는 node들이 192.168.0.0/24 대역에 위치한다고 가정함)  # yum install nfs-utils # mkdir -p /cocktail-nfs # vi /etc/exports /cocktail-nfs 192.168.0.0/24(rw, sync, no_root_squash, no_all_squash) # systemctl enable rpcbind # systemctl enable nfs-server # systemctl restart rpcbind # systemctl restart nfs-server   master node 및 worker node에서 nfs server로의 접속 여부 확인  NFS 서버 및 클라이언트는 UDP, TCP 각각 111, 1039, 1047, 1048, 2049 port를 사용함으로 해당 port가 방화벽에서 차단되어 있지 않은지 먼저 ping이나 telnet 명령으로 확인하고 nfs server가 share하는 디렉토리가 정상적으로 조회되는지 확인한다.\n# yum install nfs-utils # ping 192.168.0.175 # telnet 192.168.0.175 2049 showmount 명령으로 NFS서버에서 share하는 디렉토리를 조회할 수 있다. # showmount -e 192.168.0.175 /cocktail-nfs 192.168.0.0/24   Minikube상에 배포한 경우  k8s dashboard \u0026gt; Config and Storage \u0026gt; Persistent Volume Claims 에서 Volume명을 확인한 다음 아래 명령으로 해당 directory로 이동하여 조회할 수 있다. (pvc :pvc-1fb67e4b-d821-11e7-ba90-0800271793b1 로 가정)\n# minikube ssh # cd /mnt/sda1/hostpath-provisioner/pvc-1fb67e4b-d821-11e7-ba90-0800271793b1 # ls -al total 176200 drwxrwxrwx 5 999 999 4096 Dec 3 12:03 . drwxr-xr-x 4 root root 4096 Dec 3 11:57 .. -rw-rw---- 1 999 999 16384 Dec 3 12:03 aria_log.00000001 -rw-rw---- 1 999 999 52 Dec 3 12:03 aria_log_control drwx------ 2 999 999 12288 Dec 3 12:03 cocktail -rw-rw---- 1 999 999 50331648 Dec 3 12:20 ib_logfile0 -rw-rw---- 1 999 999 50331648 Dec 3 12:03 ib_logfile1 -rw-rw---- 1 999 999 79691776 Dec 3 12:20 ibdata1 -rw-rw---- 1 999 999 0 Dec 3 12:03 multi-master.info drwx------ 2 999 999 4096 Dec 3 12:03 mysql drwx------ 2 999 999 4096 Dec 3 12:03 performance_schema -rw-rw---- 1 999 999 24576 Dec 3 12:03 tc.log   PV(Persistence Volume)를 갖는 POD의 nfs 서버상의 mount directory 조회  # login to nfs server NFS 서버에서 공유한 디렉토리로 이동(/cocktail-nfs라고 가정함) # cd /cocktail-nfs # cd /  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.3.troubleshooting/8.3.4-nodeip/",
	"title": "8.3.4 Cube로 설치한 node의 ip 정보가 변경될 경우.",
	"tags": [],
	"description": "",
	"content": "cube로 설치한 master node, worker node에서 ip 정보가 변경되면 현재로서는 cube를 이용하여 재설치하는 방식을 권장한다.\n이는 k8s의 component중 api server, scheduler, control manager가 ip 인증서 기반으로 통신하며, etcd의 flannel subnetwork정보도 ip정보가 저장되기 때문이다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/8.operation/8.3.troubleshooting/8.3.5-yum/",
	"title": "8.3.5 Centos yum 자동 upgrade 방지",
	"tags": [],
	"description": "",
	"content": "칵테일은 kubernetes package의 버전에 따라 설정파일의 차이가 있으므로, 시스템에서 yum upgrade가 자동으로 설정되어 있으면 시스템 업그레이드시에 칵테일 관련 설정이 override 될 수 있으므로 반드시 security 부분만 자동으로 업그레이드가 되도록 수정해야 한다.\n만약 그렇지 않으면, 시스템의 재기동이 필요한 업그레이드나 칵테일 관련 설정파일이 override 되면서 서비스가 중단될 수 있다.\n yum-cron 설정 파일 수정.\n# vi /etc/yum/yum-cron.conf ... update_cmd = security 로 수정 # vi /etc/yum/yum-cron-hourly.conf ... update_cmd = security 로 수정   참고) https://www.lesstif.com/pages/viewpage.action?pageId=46366785, http://webdir.tistory.com/175\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/ko/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]