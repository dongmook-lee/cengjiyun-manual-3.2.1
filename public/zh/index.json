[
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/",
	"title": "层积云",
	"tags": [],
	"description": "",
	"content": " 层积云是什么? 层积云是一种 容器一体化管理平台(All-in-one Container Management Platform)。\n随着云服务的普遍使用，除了基础架构，对应用程序、服务管理的要求也越来越高。像过去的 开发和运营方式充分发挥云的优势是有限的。尤其是应用程序领域，持续整合以及部署 (Continuous Integration/Deploy，CI/CD)、迁移(Migration)、多云/混合云的构筑等自动化、高 效化、整合管理的需求日益增加。\n可以说容器技术的扩散是理所当然。目前很多企业已经引进了容器技术，呈现继续增强趋势。 (参考: http://redmonk.com/fryan/2017/09/10/cloud-native-technologies-in-the-fortune-100)\n容器是将应用程序或服务压缩成可独立运行单位的技术，无论基础架构环境如何，都可以提供 相同的开发和运营经验。可以实现从基础架构到服务的云管理体系标准化，也可以减少开发及 运营工作量。尤其是不变的环境下，能管理好多云/混合云是它的优点所在。\n层积云将容器优点适用于云管理服务，实现了开发及运营业务的高效化，为单一或多云/混合云 战略实施提供了平台。\n层积云的主要职能如下\n 从代码到 Build、部署、更新环节的渠道自动化; 工作负载(服务)中心的容器管理:压缩、生命周期、资源等; 全栈监控:从基础架构到容器的状态以及资源的监控、报警管理; 多云/混合云集群配置及管理:Baremetal、私人/公共云。  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/0.1/",
	"title": "层积云组件概要",
	"tags": [],
	"description": "",
	"content": "层积云根据其功能划分了 5 个图层。\n 集群管理层(Cluster Management Layer): 负责容器部署/运行的基础架构(集群)及编排 (Orchestration)的图层。编排由 Kubernetes(https://kubernetes.io)负责，提供基础架构管理、 监控等扩展管理职能。\n 服务管理层(Service Management Layer): 负责基于服务(Workload)的容器配置和管理的 图层。压缩服务配置的多个容器及相关对象，并管理其生命周期及监控。\n 渠道(Pipeline): 实现了从代码到容器 Build、部署过程的自动化，并执行持续的整合/部署。 用户可通过设置和脚本构筑所需要的渠道。\n 目录(Catalog): 提供通用运行时间(DB、中间件等)模板的图层。需要模板时，不需要额 外配置，可直接部署和使用。并且，还可以保存和管理用户应用程序的快照。\n 仪表盘(Dashboard): 提供集群、服务现状和监控视图。\n  下面继续了解各图层的详细内容。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/",
	"title": "2.1 시스템",
	"tags": [],
	"description": "List the child pages of a page",
	"content": "시스템 이용에 필요한 시스템 정보, 계정 정보, 클라우드 미터링 계정, 시스템 사용자를 관리 할 수 있다.\na) 좌측 메인메뉴 → 시스템\n   시스템 정보 설명     시스템 명 칵테일 클라우드의 시스템 이름   시스템 접속 계정 칵테일 클라우드 접속을 위한 시스템 계정   시스템 유형 칵테일 클라우드 시스템 사용 유형   기본 언어 칵테일 클라우드 기본 언어   시스템 로고 칵테일 클라우드 시스템 로고   시스템 설명 칵테일 클라우드 시스템 설명       계정 정보 설명     조직명 칵테일 클라우드 시스템 이용 조직명   계정 담당자 명 칵테일 클라우드 계정 관리 담당자 명   계정 담당자 이메일 칵테일 클라우드 계정 관리 담당자 이메일   주소 칵테일 클라우드 시스템 이용 조직 주소   라이센스 코드 칵테일 클라우드 라이센스 코드       클라우드 미터링 계정 설명     계정 이름 클라우드 미터링 계정 이름   프로바이더 클라우드 프로바이더 유형   설명 클라우드 미터링 계정 설명       시스템 사용자 설명     이름 시스템 사용자 이름   아이디 칵테일 클라우드 접속 사용자 아이디   권한 시스템 사용자 권한 (관리자, 사용자)   마지막 접속일 시스템 사용자 마지막 접속 시간   설명 시스템 사용자 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/",
	"title": "8.1 클러스터 구성 및 등록",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.1.kass/",
	"title": "8.1.1 클러스터 서비스 사용",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/0.2/",
	"title": "集群管理层(Cluster Management Layer)",
	"tags": [],
	"description": "",
	"content": "集群管理层又分为组成集群的基础架构脚本和容器编排引擎部分。其具体配置如下图。\n层积云根据用户设置，自动配置(Provisioning)集群基础架构和编排引擎，CUBE 就是负责该领 域的工具。CUBE 组建高可用性(High Availability，HA)的 Kubernetes 集群。由 CUBE 配置的集 群提供高稳定性和安全性以及扩展性。(叫做 CUBE 集群)\n目前 CUBE 集群支持以下云平台及供应商。CUBE 可基于基础架构独立配置，所以任何基础架 构上都可以配置。以下是基础架构自动配置为准。\n 物理架构(Baremetal)\n 云平台 : Openstack, Cloudstack, VMWare\n 云服 : AWS, GCP, Azure\n  CUBE 工具还提供集群 Kubernetes 版本的升级，以及节点(物理/虚拟机)的添加/删除、备份 等集群管理职能。\n层积云对多云进行整合管理。即，分配到需要一个以上集群的服务，并对整个集群执行管理和 监控。集群可按照需求添加。\nCUBE 集群额外提供管理为目的的扩展组件。\n 监控(Monitoring) : 基础架构、容器、服务(Workload)的状态、配置、资源监控 预警(Alerting) : 当满足特定条件时，通过邮件、MSN 方式发送警告以及管理 计量(Metering) : 如果是公共云集群，查询使用费用 检查(Inspecting) : 跟踪容器及 Kubernetes 对象变化及事件管理  由扩展组件提供的信息和管理职能，通过‚集群管理‛视图(View)实现。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.2.onpremise/",
	"title": "8.1.2 클러스터 직접 구성",
	"tags": [],
	"description": "List the child pages of a page",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/",
	"title": "시스템 관리",
	"tags": [],
	"description": "",
	"content": "칵테일 클라우드 이용에 필요한 시스템, 클러스터, 워크스페이스를 관리 할 수 있다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/0.3/",
	"title": "服务管理层(Service Management Layer)",
	"tags": [],
	"description": "",
	"content": "通常情况下一个应用程序及服务由多个容器组成。尤其是类似 Kubernetes 的编排引擎，容器以 外还需额外增加服务端口、控制器、卷等对象。因此基于容器的应用程序及服务管理，需要压 缩和管理容器以及相关对象的抽象单位。服务管理层就是负责该领域的图层。\n层积云的服务管理拥有如下阶层架构。\n 服务(Service) : 是最上游阶层，分配和管理服务所必要的集群资源、映像注册表、映像 Build 的群组。根据开发人员和运营人员的角色赋予权限，能独立进行管理。层积云可根据需求， 创建多个服务，并按组或服务分配以及管理。 服务地图(Service Map) : 是组成和管理应用程序的单位，相当于一个工作负载 (Workload)。例如一个服务可以由开发阶层、验证阶层、运营阶层的应用程序表组成，也 可以按照数据管理、分析系统类似功能组成应用程序表。从逻辑角度上，可以说应用程表 是管理工作负载为目的的由多个容器组成的单位。如前所述，服务由一个以上的应用程序 表组成。 层积云服务器(Cocktail Server) : 是容器和相关编排对象的压缩单位。应用程序表由一个 以上的层积云服务器组成。例如，可以 Web 服务器、DB 服务器等类似组件为准组成服务 器，也可以由小规模微服务为准组成层积云服务器。关于层积云服务器，后面进一步详细 说明。  下图是服务管理图层的配置图。\n层积云服务器(Cocktail Server)\n层积云服务器在服务管理层发挥最核心的功能。如前所述，层积云服务器是容器和相关编排对 象的压缩单位。层积云管理压缩对象的创建、更新等生命周期，同时把状态和资源作为一个压 缩单位进行监控以及管理。 以下是图实例。\n层积云提供的管理职能有如下内容:\n 工作负载的创建、修改(更新)、停止、重启、删除、自动缩放\n 滚动更新\n 卷管理\n 服务端口管理\n 监控\n 检查 : 编排对象的状态以及部署信息查询\n 网络终端、日志查询 : 容器壳连接、容器日志查询\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/3.dashboard/",
	"title": "仪表盘",
	"tags": [],
	"description": "",
	"content": "通过仪表盘，可一目了然层积云所服务的信息。\n 摘要信息  可查看当前已注册管理的服务程序数量、工作负载数量、已注册集群数、当前启用报警数。\n 资源现状  可查看 CPU、内存的总量、请求量、使用量以及存储(节点、Pv)总量。\n 供应商集群现状  可查看不同供应商已注册的集群数、服务程序数量、工作负载数量。\n 集群费用现状  可查看正在使用的集群费用。\n 集群上游资源请求率  可查看正在使用的集群中，上游 5 个集群的资源分配率(request)。\n 各服务程序的上游资源请求量  可查看正在使用的服务程序中，上游 5 个服务程序的资源分配量。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/0.4/",
	"title": "管道层(Pipeline Layer)",
	"tags": [],
	"description": "",
	"content": "管道层实现了从代码到容器的 Build 及部署的自动化。\nBuild 按照用户设置，依次执行代码下载、代码 Build 以及测试等作业(Task)、容器映像 Build。 作业(Task)使用的是执行内部作业的容器，除了默认提供的容器以外，用户可扩展使用公开或 用户制作的容器。\n层积云服务器容器对已 Build 的映像进行滚动更新。通过 Build 制作的映像，将自动生成 Build 编码，再通过这个编码执行最新版本或者回滚。\n从 Build 到部署的所有作业执行环节，均通过管道层实现了自动化。因此，只需创建一次 Build 和管道层，开发人员可以集中编写代码和修改作业。\n下图为管道层的配置图。\n管道层通过持续整合和部署(CI/CD)，为 DevOps 提供基础架构。\nDevOps 强调开发和运营之间的连续性和有机协作。CI/CD 是 DevOps 的必要组件之一，同样在基于容器的开发和运营领域也非常重要。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/",
	"title": "服务",
	"tags": [],
	"description": "",
	"content": "创建和管理应用程序表和 Build 作业、工作负载的功能。    目录 说明      显示服务里已注册的集群种类和集群数    切换到 Build 管理页面    切换到应用程序表创建页面    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/0.5/",
	"title": "目录层(Catalog Layer)",
	"tags": [],
	"description": "",
	"content": "容器对其组成进行图像化处理，然后部署以及运行。因此，只要把容器映像和部署信息模板化，可以节省常用 DB 或中间件等反复作业的运行时间，必要时可随时轻松部署使用。\n目录层是负责创建和管理上述模板的图层。目录层的配置图如下:\n目录按照其目的划分为两种类型。\n 服务目录(Service Catalog) : 服务目录存储和管理用户以服务目的组成的应用程序快照。基 础架构或云供应商的交替等原因需要迁移时，可使用服务目录的快照功能。快照原样保存 应用程序表的配置。所以部署在其他环境时，应根据需求修改快照设置后再部署。 平台目录(Platform Catalog) : 指层积云通过‚层积云集线器‛提供的运行时间模板。用户 可以模板本身或根据自身环境修改设置后再部署。目前平台目录提供 DB、中间件、开发/ 运营工具、博客、机器学习等广泛使用的开源模板，并持续更新。  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/",
	"title": "Build 管理",
	"tags": [],
	"description": "",
	"content": " 管理 Docker 映像的 Build 作业。\n下载应用程序、Build 源文件和映像文件，上传到 Habor 注册表。\na) 服务→选择 Build 管理点击。    目录 说明      Build 创建按钮   Build 名称 Build 名称   映像名称 被存储的 Docker 映像名称   映像尺寸 被存储的 Docker 映像大小。以 MB 单位来标记   最近操作 显示 Build 的状态   状态 显示 Build 的运行状态   日期 显示 Build 运行日期    Build 操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/1.intro/0.6/",
	"title": "仪表板层(Dashboard Layer)",
	"tags": [],
	"description": "",
	"content": "层积云的仪表板层以 Web UI 形式，提供服务现状、监控、集群现状等。\n仪表板按照权限分类，提供和用户角色相对应的视图。层积云的用户权限如下:\n 管理员(Administrator)(Admin) : 执行集群管理，对层积云的所有服务拥有管理权限。\n DevOps : 只对一个以上的服务分配拥有管理权限。仪表板只提供没有权限的服务视图，不能管理集群管理、环境设置等。\n  层积云提供的仪表盘视图如下\n 综合仪表盘 : 对所有服务和集群现状进行综合显示的视图\n 服务管理视图 : 创建、部署、监控、运营服务为目的的 DevOps 用户视图\n 目录视图 : 能查询和部署服务目录和平台目录的视图\n 集群视图 : 能整合一个以上的集群，并管理和监控的视图\n 环境设置 : 层积云平台的设置视图\n  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/",
	"title": "目录",
	"tags": [],
	"description": "",
	"content": "通过应用程序配置的模板化，可轻松创建、部署、管理工作负载的功能。\n当一次性捆绑使用多个工作负载时，使用目录功能。\n目录分为服务目录和平台目录。\n   目录菜单 说明     服务目录 用户创建的目录   平台目录 层积云默认提供的正式目录   过滤目录 通过搜索，查看所需要的模板   模板 组成应用程序的工作负载集合   部署 通过目录，部署工作负载   编辑 对目录内容进行编辑    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/",
	"title": "集群",
	"tags": [],
	"description": "",
	"content": "提供集群的注册管理，以及节点和应用程序为单位的监控、卷、报警和计量服务。\n   集群菜单 说明     节点 所有节点的 CPU、内存、网络使用量以及个别节点的监控服务   应用程序 所有应用程序的 CPU、内存、网络使用量以及个别应用程序的监控 服务   卷 存储管理以及卷现状   报警 集群事件信息相关的报警服务   计量 集群费用及各种资源费用服务   注册管理 集群的查看、修改、删除    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/",
	"title": "시스템 운영",
	"tags": [],
	"description": "",
	"content": "\u0026mdash; |\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/9.member/",
	"title": "구성원",
	"tags": [],
	"description": "",
	"content": "현재 워크스페이스에 속한 구성원들 정보를 보여준다.\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/10.terminology/",
	"title": "术语说明",
	"tags": [],
	"description": "",
	"content": "为了帮助用户了解层积云的运用，下面对 Kubernetes 及层积云的术语进行了整理。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/11.support/",
	"title": "10.售后服务",
	"tags": [],
	"description": "",
	"content": "非常感谢使用层积云。\n可享受产品功能购买咨询等技术服务以及各种优惠。所提供的各种服务如下:\n 可通过访问服务，咨询 Q\u0026amp;A、产品相关信息以及其他疑问。 对客户已注册产品和可咨询产品，可通过电话或传真进行专业技术咨询服务。 提供各种活动邀请以及信息。  服务支持\n邮箱 : byoungoh.lee@namutech.co.kr\n产品开发商\nACON 软件(株)\n 地址 : 首尔特别市 江南区 驿三路 239 号 华光中心 4 层 TEL : 02-554-0301 FAX : 02-554-0302  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/",
	"title": "Cocktial Cloud Manual",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/10.terminology/10.1-kubenetes/",
	"title": "10.1 Kubernetes(k8s)",
	"tags": [],
	"description": "",
	"content": " 1.集群    术语 说明     Namespace k8s 的虚拟集群，可以为用户提供分组或分项目的额外操作环境   Nodes 驱动 k8s 已分组的应用程序物理服务器或 VM   Persistent Volumes(PV) 支持在外部存储、NFS、iSCSI、云上提供的存储系统   Roles 根据使用情况授权   Storage Classes 对 PV 进行动态配置时，用于识别 NFS 服务器的标识符    2.工作负载    术语 说明     Cron Jobs 如\u0026rdquo;在特定时点一次\u0026rdquo;或者\u0026rdquo;在特定时点反复\u0026rdquo;,基于时间的管理作业   Deployments 用于创建 Pod 的设定值注册信息   Jobs Job 是执行批处理的 Pod 的 Supervisor。即，如特定的计算或备份，只在特定 时间内运行的处理器   Pods 由 1 个以上容器组成的 k8s 里最小的部署单位。Pod 是在应用程序全栈中由 不同的 Docker 映像组合而成   Replica Sets 使指定 Pod 副本经常(保持)运行   Replication Controllers 检查指定数量的 Pod 副本是否正在运行   Stateful Sets 管理 Pod 的部署及扩展，并提供 Pod 的顺序和特点设置    3.负载发现及均衡    术语 说明     Ingresses 对外部访问请求，提供基于域名的虚拟主机   Services 多个容器组成一个逻辑单位分组，可为外部访问提供单一结点    4.config and storage    术语 说明     Config Maps 保存数据，并支持其使用   Persistent Volume Claime(PVC) 用户对 PV 的请求。Pod 可通过 PVC 请求资源(CPU 和内存)，同时可 请求存储空间大小及访问权限(rw、readOnly\u0026hellip;)。   Secrets 密码或 OAuth 令牌以及 ssh 秘钥等存储库    5.设置 可调整 K8s 的 Global Settings 值。\n   옵션 说明     Cluster name 可调整 Kubernetes 集群名称值。   Items per page 可调整第 1 页显示的 Items 值。   Auto-refresh time interval 可调整自动刷新 Log 的时间    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/10.terminology/10.2-system/",
	"title": "10.2 Cocktail Cloud",
	"tags": [],
	"description": "",
	"content": " 1.环境设置 为了使用层积云，事先注册信息的服务。\n   术语 说明     用户 为层积云使用用户，提供管理支持   账号 支持 k8s 集群用户认证信息的管理   服务 业务或项目为单位，分散管理作业。相比一般用户，管理员可赋予访问服务单位 的权限    C.集群:为了使用 k8s，管理 k8s 信息的页面\n2.服务 应用程序为单位细分作业，可通过 Build 创建 Docker 映像。通过已 Build 映像，对应用程序实 施创建、查看、修改、删除等管理的服务。\n   术语 说明     应用程序表 k8s 的命名空间   Build 支持 Docker 映像作业的服务   监控 在应用程序内部，对服务器提供 Pod、容器为单位的资源监控   管道 从 Build 到部署，可一次性方便运行的服务   设置 应用程序单位，能保存并使用参数值或文件的服务    3.目录 通过应用程序配置的模板化，可轻松创建、部署、编辑管理服务器的服务。\n4.集群 可查询集群、节点、应用程序、卷/存储、报警、计量等的注册管理以及现状的服务。\n   术语 说明     集群 组成 k8s 编排的服务器包   节点 k8s 集群服务器中的个别服务器。分为 Master 和 Worker。   应用程序 由 1 个以上容器组成的服务   卷/存储 外部存储信息   报警 对集群的事件通知   计量 集群计费信息    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/10.terminology/10.3-error-code/",
	"title": "10.3 Cocktail Cloud System Error Code",
	"tags": [],
	"description": "",
	"content": " 1. 故障代码    代码 信息     CCCM001 无法显示具体错误代码。   CCCM002 未知错误。   CCCM004 无效函数导出的参数。   CCCM005 未经认证的请求。   CCCM006 传递信息不正确。   CCCM007 服务器、作业等不正确或者无法处理请求。   CCCM019 内部服务器错误。   CCCM020 外部导出API返回错误或导出失败。   CCCM023 无需要执行的作业。   CCCM026 发生了错误。   CCCM027 注册中发生了错误。   CCCM028 修改中发生了错误。   CCCM029 删除中发生了错误。   CCCM030 查询中发生了错误。   CDCM031 数据库作业中发生了错误。请稍后再尝试。仍未改善时，请咨询管理员。   CDCM032 数据库连接中发生了错误。请稍后再尝试。如果可以，请检查数据库连接状态，或者咨询管理员。   CKSY001 Cube集群API发生了错误。   CKSY002 层积云不支持Cube集群版本。   CKSY003 加密中发生了错误。   CCUS001 未注册的用户ID。   CCUS002 用户密码不一致。   CCUS003 用户指定权限未分配的用户。   CCUS004 已注册用户。   CCUS005 停止使用的账号。   CCUS006 Root Admin用户不能操作。   CCAC001 供应商证书格式不正确。   CCAC002 无供应商证书。   CCAC003 供应商访问秘钥或者secret秘钥不正确。   CCAC004 即将输入的新供应商秘钥值与以前不一致。   CCAC005 无法找到供应商。   CCAC006 已存在使用该供应商的集群。   CCCT001 集群访问账号不正确。   CCCT002 该主URL已存在注册集群。   CCCT003 集群证书不正确。   CCCT005 集群包含服务器，所以无法修改或删除集群。   CCCT006 集群现状查询中发生了错误。   CCCT007 应用程序现状查询中发生了错误。   CCCT008 卷现状查询中发生了错误。   CCGP001 集群包含服务器，所以无法删除集群。   CCGP002 重复的群组名称。   CCSV001 已注册服务的集群。   CCSV002 应用程序表正在使用集群。   CCBD001 Build Job创建错误   CCBD002 Build Job删除错误   CCBD003 注册表已有的映像名称。请输入其他映像名称。   CCBD004 已注册的映像。   CCBD005 Build服务器连接不顺畅。   CCBD006 Build查询中发生了错误。   CCBD007 Build创建中发生了错误。   CCBD008 Build修改中发生了错误。   CCBD009 Build删除中发生了错误。   CCBD010 Build历史记录查询中发生了错误。   CCCV001 未找到集群卷名称或者不正确。   CCCV002 未找到集群卷的存储类别名称或不正确。   CCCV003 未找到集群卷的插件名称。   CCCV004 未找到集群卷的回收策略的名称。   CCCV005 未找到集群卷的集群序列号。   CCCV006 无法找到集群卷。   CCCV007 已存在相同设置的卷。   CCCV008 无法找到集群卷类型。   CCCV009 集群卷正在使用中，所以无法删除。   CCCV010 参数是必项。   CCCV011 已存在相同设置的静态卷。   CCCV012 该集群卷不是固定的集群卷。   CKCF001 K8S配置图创建失败。   CKCF002 无法找到指定的K8S配置图。   CCCF003 未找到配置图名称或不正确。   CCCF004 配置图秘钥不正确。   CCCF005 需要添加的配置图名称已存在。   CCCF006 配置图数据不正确。   CKCF007 服务器在使用配置图。   CKST001 K8S Secret创建失败。   CCST002 无法找到指定的K8S Secret。   CCST003 未找到Secret名称或不正确。   CCST004 Secret数据不正确。   CCST005 需要添加的Secret名称已存在。   CCST006 服务器在使用Secret。   CCCL001 作业对象目录没有序列号。   CCCL002 需要适用目录的现有应用程序表无序列号。   CCCL003 已存在相同的目录名称。   CCCL004 已存在相同的目录版本。   CCCL005 目录注册中发生了错误。   CCCL006 目录删除中发生了错误。   CCCL007 目录修改中发生了错误。   CCCL008 目录部署中发生了错误。   CCCL010 无法找到目录所使用的注册表。   CCCL011 无法找到目录所使用的集群卷。   CCCL012 未找到需要注册的目录名称或不正确。   CCCL013 未找到需要适用目录的新应用程序表名称或不正确。   CCCL015 未找到新命名空间名称或不正确。   CCSR001 需要运行的Job类型不正确。   CCSR002 作业在运行中，所以不能运行新作业。   CCSR003 服务器处于不能运行指定操作的状态。   CCSR004 当前状态下无法运行的操作。   CCSR006 不存在需要运行的作业（task）。   CCSR007 超了内存可用量。   CCSR008 超了CPU可用量。   CCSR009 超了Pod可用数量。   CCSR010 准备创建服务器时发生了错误。   CCSR011 无法修改服务器的状态。   CCSR012 准备修改服务器时发生了错误。   CCSR013 准备关闭服务器时发生了错误。   CCSR014 准备重启服务器时发生了错误。   CCSR015 已经在使用的名称。   CCSR016 未找到需要查询的资源数量。   CCSR017 未知的资源类型。   CCSR018 不支持的服务器类型。   CCSR019 不支持的卷插件。   CKSR020 K8S部署创建等待时间超时了。   CKSR021 K8S Pod创建等待时间超时了。   CKSR022 K8S部署删除失败。   CKSR023 K8S Replicatset删除失败。   CKSR024 K8S服务或负载均衡创建等待时间超时了。   CCSR025 无法找到K8S Pod。   CKSR026 K8S对象查询结果无状态。   CKSR027 K8S持久化卷创建失败。   CKSR028 无法找到K8S持久化卷。   CKSR029 K8S持久化卷集群创建失败。   CKSR030 K8S持久化卷集群创建超时了。   CKSR031 无法找到指定的K8S命名空间。   CKSR032 K8S部署创建失败。   CKSR033 K8S服务创建失败。   CKSR034 K8S Ingress创建失败。   CKSR035 K8S HorizontalPodAutoscaler创建失败。   CCSR036 已存在相同名称的卷。   CCSR037 无法找到指定的服务器。   CCSR038 服务器不是Cube类型。   CKSR039 服务器设置修改失败。   CCSR040 服务器创建/修改前，k8s资源检测失败。   CKSR042 无法找到K8S部署信息。   CCSR043 服务器类型不能修改。   CCSR044 服务器无容器。   CCSR045 主机端口重复。   CCSR046 Ingress路径已经在使用。   CCSR047 不是K8S的日志类型。   CCSR048 准备状态下服务器删除失败。   CCSR050 节点端口超了指定范围。   CCSR051 节点端口重复。   CCSR053 卷处于无法使用的状态，所以不能重启。   CCSR054 重复的容器名称。   CCSR055 重复的命名空间。   CCSR056 服务器不是可停止的状态。   CCSR057 服务器不是可开启的状态。   CCSR058 服务器不是可重启的状态。   CCSR059 服务器不是可删除的状态。   CKSR062 Cube集群信息查询中发生了错误。   CKSR063 Cube集群信息创建中发生了错误。   CKSR064 Cube集群信息修改中发生了错误。   CKSR065 Cube 集群信息删除中发生了错误。   CCSR066 服务端口不正确。(范围:1 ~ 65535)   CCSR067 服务器端口范围不正确。   CCSR068 服务器端口范围格式不正确。(e.g. 8080-8082)   CCSR069 服务器目标端口和端口范围不一致。(e.g. 8080-8082, 9090-9092)   CKSR070 Cube 集群资源限量查询中发生了错误。   CCSR071 服务不存在。   CCSR072 服务器运行中发生了错误。   CCSR073 服务器卷设置不正确。请重新确认。   CCSR074 端口范围不能设置指定节点端口。   CCSR075 无指定节点端口值。   CCSR076 准备删除服务器时发生了错误。   CCPL001 服务器状态不是正在运行，所以无法运行管道。   CCPL002 管道正在运行，所以不能执行其他工作。   CCPL003 无法找到指定的管道。   CCPL004 管道创建失败。   CCPL005 管道修改失败。   CCPL006 管道运行中存在相同 Build 作业，所以无法运行。包含相同 Build 的管道，请 分别运行。   CCPL007 管道运行中发生了错误。   CCRG001 无法从注册表获取映像目录。   CCRG002 无法登陆注册表。   CCRG003 无法从注册表获取映像标签目录。   CCRG004 用户不能登陆注册表。   CCRG005 注册表登陆用户未被注册。   CCRG006 无法修改注册表用户的密码。   CCRG008 注册表已注册的用户无法分配到项目。   CCRG009 项目不能排除注册表已注册的用户。   CCRG010 项目不能登陆注册表。   CCRG011 注册表已存在的项目。   CCRG012 注册表项目不能删除。    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.1-edit/",
	"title": "2.1.1 시스템 편집",
	"tags": [],
	"description": "",
	"content": " 시스템 이용에 필요한 시스템 정보, 계정 정보를 수정 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 우측 상단 편집 버튼 시스템 정보(기본 언어, 시스템 로고, 시스템 설명)와 계정 정보(조직명, 계정 담당자 명, 계정 담당자 이메일, 주소)를 수정 할 수 있다.\nb) 좌측 메인메뉴 → 시스템 → 편집 → 수정완료 후 수정버튼을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.2-meteringadd/",
	"title": "2.1.2 클라우드 미터링 계정 등록",
	"tags": [],
	"description": "",
	"content": " 클라우드 미터링 계정 등록 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 계정 등록  Amazon Web Service  Google Cloud Plalform   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.3-meteringedit/",
	"title": "2.1.3 클라우드 미터링 계정 편집",
	"tags": [],
	"description": "",
	"content": " 클라우드 미터링 계정 편집 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 계정 액션 메뉴 → 계정 편집 → 수정완료 후 수정버튼을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.4-meteringdelete/",
	"title": "2.1.4 클라우드 미터링 계정 삭제",
	"tags": [],
	"description": "",
	"content": " 클라우드 미터링 계정을 삭제 할 수 있다.\na) 좌측 메인메뉴 → 시스템 → 계정 액션 메뉴 → 계정 삭제 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.5-useradd/",
	"title": "2.1.5 시스템 사용자 추가",
	"tags": [],
	"description": "",
	"content": " 시스템 사용자를 추가 하는 기능이다.\na) 좌측 메인메뉴 → 시스템 → 사용자 추가를 클릭한다.    시스템 사용자 설명     이름 시스템 사용자 이름   아이디 칵테일 클라우드 접속 사용자 아이디   설명 시스템 사용자 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.6-useredit/",
	"title": "2.1.6 시스템 사용자 편집",
	"tags": [],
	"description": "",
	"content": " 시스템 사용자 정보(이름, 설명)를 편집 하는 기능이다.\na) 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 편집를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.7-userreset/",
	"title": "2.1.7 시스템 사용자 비밀번호 초기화",
	"tags": [],
	"description": "",
	"content": " Cocktail Cloud 사용자 비밀번호를 초기화 하는 기능이다.(초기화 후 제공받는 초기 비밀번호는 \u0026lsquo;Pass0000\u0026rsquo;이다.)\na) 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 비밀번호 초기화를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.1.system/2.1.8-userdelete/",
	"title": "2.1.8 시스템 사용자 삭제",
	"tags": [],
	"description": "",
	"content": " 시스템 사용자를 삭제하는 기능이다.\na) 좌측 메인메뉴 → 시스템 → 사용자 액션 메뉴 → 사용자 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/",
	"title": "2.2 클러스터",
	"tags": [],
	"description": "",
	"content": "클러스터의 등록관리와 노드와 어플리케이션 단위의 모니터링, 볼륨, 알람, 미터링 서비스가 제공된다.    클러스터 메뉴 설명     노드 전체 노드의 CPU, Memory, Network 사용량 및 개별 노드의 모니터링 서비스   어플리케이션 전체 어플리케이션의 CPU, Memory, Network 사용량 및 개별 어플리케이션의 모니터링 서비스   볼륨 스토리지 관리 및 볼륨 현황   알람 클러스터의 이벤트 정보 관련 알람 서비스   미터링 클러스터 비용과 리소스별 비용 서비스   등록관리 클러스터 조회 및 수정 및 삭제    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.1-add/",
	"title": "2.2.1 集群添加",
	"tags": [],
	"description": "",
	"content": " 可添加注册集群的功能。\na) 点击集群→右上角带+符号的按钮。 b) 输入基本信息(名称、k8s 版本、说明)    基本信息 说明     名称 需要注册的集群名称   k8s 版本 集群已安装的 Kubernetes 版本信息。e.g)1.8.13   ID 对集群默认 ID(用户指定)、报警消息进行重定向时需要   说明 集群的用户说明    c) 输入供应商信息(账号、类型、地域)。 根据账号的供应商和类型，输入栏毁被修改。如果是 Baremetal，重定向提供默认值，可以修改。    供应商 说明     账号 已注册账号   类型 Kubernetes 的使用类型，从 MANAGED、PROVIER、GKE 中选择   重定向 已安装 Kubernetes 服务器的重定向       类型 说明     MANAGED 使用 CUBE 安装程序组成 kubernetes 的集群   PROVIDER Kubernetes 基于公共云 VM 使用，但 PROVIDER 基于公共云使用。(使用负 载均衡、存储等其他服务时)   GKE 基于谷歌云平台的谷歌 Kubernetes 引擎组成的集群     供应商类型 - MANAGED      补充输入 说明     节点端口 URL 节点附加端口服务模式下，端口前面需加的 IP 服务(Master IP or Loadbelancer IP)   节点端口范围 节点附加端口服务模式下，IP 后面需加的端口范围。(建议 30000~32767 권장)     供应商类型 - PROVIDER ㅤ\n 供应商类型 - GKE      补充输入 说明     项目 ID 谷歌云平台的账号所需要使用的项目 ID(需要使用 GKE 的项目)    d) 输入监控信息(Master URL、Ingress Host、监控主机、监控端)。    监控信息 说明     主 URL Kubernetes API 地址。使用 \u0026ldquo;https://host:port\u0026rdquo; 的格式。   Ingress Host 基于 Ingress 模式的主机 IP 地址服务(Master IP or Loadbelancer IP)   监控主机 已安装监控 API 的服务器 IP 地址\u0026rdquo; http://host\u0026ldquo;   监控端口 监控 API 的端口信息    e) 输入集群类型。    集群类型信息 说明     认证类型 使用 k8s 集群为目的的认证手段     认证类型 - certification     认证信息 说明     用户 ID k8s 用户 ID   PW k8s 用户 PW   集群 CA 认证 连接主服务器后，切换到/etc/kubernetes/pki 路径，输入 ca.crt 文件值   客户端证书数据 连接主服务器后，切换到/etc/kubernetes/pki 路径，输入 admin.crt 文件值   客户端关键数据 连接主服务器后，切换到/etc/kubernetes/pki 路径，输入 admin.key 文件值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.1-add_ko/",
	"title": "2.2.1 클러스터 등록",
	"tags": [],
	"description": "",
	"content": " 클러스터를 추가 등록할 수 있는 기능이다.\na) 클러스터 → 오른쪽 상단 + 모양의 버튼을 클릭한다. b) 기본 정보(이름, 쿠버네티스 버전, 아이디, 설명)를 입력한다.    기본 정보 설명     이름 등록할 클러스터의 이름   쿠버네티스 버전 클러스터에 설치된 Kubernetes의 버전정보. e.g) 1.8.13   아이디 클러스터 고유 아이디(사용자 지정), 알람 메시지 Redirect시 필요   설명 클러스터에 대한 사용자 설명    c) 프로바이더 정보(프로바이더, 유형, 리전)를 입력한다. 계정의 프로바이더와 유형에 따라 입력란이 변경된다.\n   프로바이더 설명     프로바이더 프로바이더를 선택   유형 Kubernetes의 사용 유형으로 MANAGED, PROVIER, GKE, EKS, AKS 선택   리전 Kubernetes가 설치된 서버의 리전 선택       추가입력 설명     Billing Account 클라우드 미터링 계정 선택   프로젝트 아이디 Google Cloud Platform의 계정이 사용할 프로젝트의 아이디(GKE를 사용할 프로젝트)    d) 엔드포인트 정보(마스터 URL, Ingress Host, 노드 포트 호스트 주소, 노드 포트 범위)를 입력한다.    엔드포인트 설명     마스터 주소 Kubernetes API 주소. \u0026ldquo;https://host:port\u0026rdquo; 형식을 사용한다.   인그레스 호스트 주소 인그레스 방식에 사용할 Host IP Address 서비스(Master IP or Loadbelancer IP)   노드 포트 호스트 주소 노드에 포트를 붙여 서비스 노출하는 방식에서 포트 앞에 사용할 IP서비스(Master IP or Loadbelancer IP)   노드 포트 범위 노드에 포트를 붙여 서비스 노출하는 방식에서 IP뒤에 사용할 포트의 범위.(30000~32767 권장)    e) 클러스터 유형을 입력한다.    인증 정보 설명     Cluster CA Certification 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 ca.crt파일값 입력   Client Certificate Data 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 admin.crt파일 값 입력   Client Key Data 마스터 서버 접속 후 /etc/kubernetes/pki 경로 이동 후 admin.key파일 값 입력    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.2-update/",
	"title": "2.2.2 集群编辑",
	"tags": [],
	"description": "",
	"content": " 修改集群的基本信息、供应商、监控、集群类型的功能。但不能修改供应商的账号、类型、重 定向。\na) 集群→选择集群→注册管理→修改按钮→修改结束后，点击保存按钮。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.3-delete/",
	"title": "2.2.3 集群删除",
	"tags": [],
	"description": "",
	"content": " 删除集群信息的功能。如有应用程序表正在使用该集群，则不能删除。\na) 集群→选择集群→注册管理→点击‘删除’按钮。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.4-volume/",
	"title": "2.2.4 卷",
	"tags": [],
	"description": "",
	"content": " 注册公共云存储以及外部存储的信息，提供当前卷的监控服务。\na) 集群→选择集群→点击卷。    存储设置信息 说明     +按钮 切换到存储添加页面   名称 存储名称(用户指定)   类型 存储种类(NFS/EBS/Google Persistent Disk/Azure Disk)   存储类别名称 已注册在 k8s 的类别名称   策略 存储卷策略设置(Retain,Recyle,Delete)   状态 存储使用状态       卷设置信息 说明     卷名称 PVC 名称   状态 PVC 挂载状态   使用量 PV 已分配的可用量和使用量   访问模式 PV 访问权限   Age PVC 创建所需时间    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.5-add/",
	"title": "2.2.5 存储添加",
	"tags": [],
	"description": "",
	"content": " 可以添加卷使用。\na) 选择集群→集群。 b) 进入集群详细画面，选择卷。 c) 进入集群详细画面，选择卷    存储注册信息 说明     名称 PV 名称   说明 PV 用户说明   类型 存储类型 (Single, Shared)   存储插件 存储种类(NFS, NFS Named, EBS, Google Persistent Disk, Azure Disk)   策略 PV 使用策略(RETAIN/DELETE)       类型 说明     Single 只能用于一个工作负载的存储   Shared 可用于多个工作负载之间共享的存储       策略 说明     Retain 即使持久化卷请求(PVC)被删除，数据仍保留在持久化卷(PV)中。之后 可以重复使用，但重复使用时，需要重新注册 PV。   Delete PVC 被删除的同时该 PV 也一起被删除。    d) 根据所使用的存储插件，设置存储类别和参数。  使用 NFS 存储插件时      存储插件 NFS     总容量 存储总容量(Gb)   策略 动态插件支持保留和删除策略   存储类别名称 默认-存储(默认值)   存储供应商 输入存储配置值     使用 NFS Named 存储插件时      存储插件 NFS Named     策略 NFS 命名插件支持保留单一策略   总容量 存储总容量(Gb)   服务器 存储 IP 地址   path 挂载路径     使用亚马逊存储插件时      存储插件 AWS EBS(AWS 的存储服务)     策略 支持保留和删除策略   存储类别名称 默认(固定值)   类型 选择io1, gp2, sc1, st1     使用谷歌存储插件时      存储插件 Google Persistent Disk(GCP 的存储服务)     策略 支持保留和删除策略   存储类别名称 标准(默认值)   类型 选择 pd-standard、pd-ssd     使用 Azure 存储插件时      存储插件 Azure Disk(Azure 的存储服务)     策略 支持保留和删除策略   存储类别名称 标准(默认值)   存储账号 输入存储账号值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.5-add_ko/",
	"title": "2.2.5 스토리지 추가",
	"tags": [],
	"description": "",
	"content": " 볼륨을 추가하여 사용 할 수 있다.\na) 클러스터 → 클러스터 선택 → 볼륨을 클릭 → 오른쪽 상단 + 모양의 버튼을 클릭한다. b) 클러스터, 이름, 설명, 스토리지 플러그인, 정책, 스토리지 클래스 이름을 차례로 기입한다.    스토리지 등록 정보 설명     이름 PV 이름   설명 PV 사용자 설명   유형 스리지 유형 (Single, Shared)   스토리지 플러그인 스토리지 종류(NFS, NFS Named, EBS, Google Persistent Disk, Azure Disk)   정책 PV 사용 정책(RETAIN/DELETE)       유형 설명     Single 워크로드 하나에서만 사용 가능한 스토리   Shared 워크로드 간에 공유하여 사용 가능한 스토리지       정책 설명     Retain PersistentVolumeClaim(PVC)가 삭제되도 PersistentVolume(PV)안에 데이터가 남는다. 추후 재사용 가능하지만 재사용시 PV를 다시 등록해야 한다.   Delete PVC가 삭제되면서 해당 PV도 함께 삭제된다.    c) 사용하는 스토리지 플러그인에 따라 스토리지 클래스와 파라미터 설정.  NFS 스토리지 플러그인 사용 시      스토리지 플러그인 NFS     총용량 스토리지 총용량(Gb)   정책 Retain과 Delete 정책을 지원   스토리지 클래스 이름 default-storage(기본값)   스토리지 프로비저너 스토리지 프로비저닝 값 입력     NFS Named 스토리지 플러그인 사용 시      스토리지 플러그인 NFS Named     정책 NFS Named 플러그인은 Retain 단일 정책을 지원   총용량 스토리지 총용량(Gb)   server 스토리지의 IP Address   path mount 경로    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.6-edit/",
	"title": "2.2.6 存储编辑",
	"tags": [],
	"description": "",
	"content": " 编辑已注册存储信息的功能。但，不能编辑 NFS 命名类型的存储。\na) 集群→选择集群→卷→卷操作按钮→点击存储编辑。 b) 不能修改名称、说明、策略、存储类别名称、参数值。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.2.cluster/2.2.7-delete/",
	"title": "2.2.7 存储删除",
	"tags": [],
	"description": "",
	"content": " 删除已注册存储信息的功能。\na) 集群→选择集群→卷→卷操作按钮→点击卷删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.3.workspace/",
	"title": "2.3 워크스페이스",
	"tags": [],
	"description": "",
	"content": "빌드 및 배포, 운영할 수 있는 워크스페이스를 할당한다. 워크스페이스 별로 사용자, 클러스터, 라이브러리 등록이 가능하다.\na) 좌측 메인메뉴 → 워크스페이스    서비스 설정 설명     워크스페이스 명 워크스페이스 이름 (사용자지정)   클러스터 수 워크스페이스에 할당된 클러스터 수   구성원 수 워크스페이스를 사용하는 구성원 수   시스템 명 시스템 명   설명 워크스페이스 설명    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.3.workspace/2.3.1-add/",
	"title": "2.3.1 워크스페이스 생성",
	"tags": [],
	"description": "",
	"content": "워크스페이스를 생성 하는 기능이다. 워크스페이스 별로 구성원 등록 및 클러스터, 레지스트리(저장소) 등록을 할 수 있다.\na) 좌측 메인메뉴 → 워크스페이스 → 오른쪽 상단 워크스페이스 생성 버튼을 클릭한다. b) 서비스에 사용할 이름, 설명, 색상, 레지스트리 및 구성원, 클러스터 정보를 등록한다. c) 구성원, 클러스터 추가는 체크박스가 선택 된것만 반영이 된다.\n 클러스터 편집  구성원 편집   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.3.workspace/2.3.2-edit/",
	"title": "2.3.2 워크스페이스 편집",
	"tags": [],
	"description": "",
	"content": "워크스페이스 정보를 편집하는 기능이다. 워크스페이스 이름, 설명, 색상 변경이 가능하며 해당 서비스를 사용할 구성원 편집, 워크스페이스에 등록할 클러스터 편집이 가능하다.\na) 좌측 메인메뉴 → 워크스페이스 → 워크스페이스 액션 메뉴 → 워크스페이스 편집 → 수정완료 후 수정버튼을 클릭한다. b) 워크스페이스 명, 설명, 색상 및 구성원, 클러스터 수정이 가능하다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/2.system-management/2.3.workspace/2.3.3-delete/",
	"title": "2.3.3 워크스페이스 삭제",
	"tags": [],
	"description": "",
	"content": "워크스페이스를 삭제 하는 기능이다. 삭제 시, 워크스페이스에 클러스터가 등록되어 있을 경우 삭제가 불가능하며 워크스페이스 편집에서 클러스터를 제거 후 삭제를 진행할 수 있다.\na) 좌측 메인메뉴 → 워크스페이스 → 워크스페이스 액션 메뉴 → 워크스페이스 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/3.dashboard/3.1-user/",
	"title": "3.1 用户管理",
	"tags": [],
	"description": "",
	"content": " 可查看和管理当前用户信息。\na) 点击仪表盘→页面左下角的用户形状按钮。    目录 说明     ID 可查看当前使用的账号 ID。   用户语言选择 可选韩语、日语、英语其中一个。   主题 可修改 Light 和 Dark 主题。   密码修改 可输入新旧密码修改密码。   帮助 提供关于层积云使用功能的帮助手册。   产品名称 显示产品名称层积云。   版本 可查看当前层积云版本。   注销 注销当前使用账号。    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/",
	"title": "4.1 应用程序管理",
	"tags": [],
	"description": "",
	"content": "管理应用程序表和相应的工作负载群组以及工作负载。\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/",
	"title": "4.1.1 应用程序表",
	"tags": [],
	"description": "",
	"content": " 应用程序的组成单位。\n作为一种逻辑单位建群，并部署群组的工作负载。\n可查看组成应用程序表的集群计量信息或工作负载的监控信息。\n可通过部署作业，创建工作负载或更新。\na) 选择服务→应用程序表点击。    目录 说明      由逻辑单位组成的群组。如激活操作菜单，可修改群组名称和列数， 以及往右或往左移、右或左边添加群组、删除群组。点击+按钮可 创建工作负载。    被部署在应用程序里的工作负载。可利用拖\u0026amp;放自由移动。    当前正在部署的工作负载如上图，一目了然地简单显示。    被部署在应用程序的工作负载单位，对正使用的 CPU、内存、网 络的监控服务菜单。    Build 映像，可一次性执行到部署作业，可修改正在部署中的工作 负载映像版本重新部署，同时批量部署多个工作负载。    可保存使用配置图等多个设置文件。    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.1-servicemapgroup/",
	"title": "4.1.1-1 서비스 맵 그룹관리",
	"tags": [],
	"description": "",
	"content": " 서비스맵에 대하여 그룹으로 관리한다. 해당 그룹에 서비스맵을 생성할 수 있다. 그룹명, 그룹색상을 변경하거나, 그룹 추가, 이동, 삭제 가능하다.\na) 좌측 메인메뉴 → 서비스 → 서비스맵 그룹의 ▼(액션 버튼) 클릭하여 메뉴를 활성화한다.    목록 설명     그룹명 변경 그룹명, 그룹색상을 변경   위로 이동 그룹을 위로 이동   아래로 이동 그룹을 아래로 이동   위에 그룹 추가 그룹 위에 새로운 그룹을 생성   아래에 그룹 추가 그룹 아래에 새로운 그룹을 생성   그룹 삭제 그룹을 삭제(그룹에 서비스맵이 존재하면 삭제가 불가능)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.2-add/",
	"title": "4.1.1-2 应用程序表创建",
	"tags": [],
	"description": "",
	"content": " 服务里创建新的应用程序表。\na) 点击服务→服务右侧的+。 b) 选择需要使用的集群，指定表的名称和命名空间名称，并创建应用程序表。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.2-add_ko/",
	"title": "4.1.1-2 서비스 맵 생성",
	"tags": [],
	"description": "",
	"content": " 서비스에 새로운 서비스 맵을 생성 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스의 우측 + 버튼을 클릭한다. b) 사용하고자 하는 클러스터를 선택하고, 맵 이름과 네임스페이스 이름을 지정하여 서비스 맵을 생성한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.3-rename/",
	"title": "4.1.1-3 应用程序表名称修改",
	"tags": [],
	"description": "",
	"content": " 修改应用程序表的名称。\na) 点击服务→用用程序表右侧的✎。 b) 被激活的名称输入栏里输入修改内容后，按 Enter 键修改应用程序表的名称。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.3-rename_ko/",
	"title": "4.1.1-3 서비스 맵 이름변경",
	"tags": [],
	"description": "",
	"content": " 서비스 맵의 이름을 변경한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵의 우측 ✎ 버튼 ㅡ을 클릭한다. b) 활성화된 이름 입력란에 내용을 변경 후, Enter 키를 눌러 서비스 맵의 이름을 변경한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.4-delete/",
	"title": "4.1.1-4 应用程序表删除",
	"tags": [],
	"description": "",
	"content": " 删除应用程序表的功能。为了删除应用程序表，需先删除正在部署中的工作负载。\na) 服务→应用程序表选择→应用程序表删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.1.servicemap/4.1.1.4-delete_ko/",
	"title": "4.1.1-4 서비스 맵 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스 맵을 삭제하는 기능이다. 서비스 맵을 삭제하기 위해서는 배포 중인 워크로드가 없어야 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 서비스 맵 삭제를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.1.servicemanagement/4.1.2-group/",
	"title": "4.1.2 群组管理",
	"tags": [],
	"description": "",
	"content": " 对工作负载进行群组管理。可以针对相应的群组，创建工作负载。可以修改群组名称，添加、移动、删除群组，以及修改列数。\na) 服务→选择应用程序表→点击工作负载群组的▼(操作按钮)，激活菜单。    目录 说明     群组名称修改 修改群组名称   列数修改 修改列数(最小:1~最多:8)   向左移动 群组向左移   向右移动 群组向右移   左边添加群组 群组左边创建新的群组   右边添加群组 群组右边创建新的群组   群组删除 删除群组(如群组有工作负载，则不能删除)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/",
	"title": "4.2 工作负载",
	"tags": [],
	"description": "",
	"content": " 组成应用程序的最小单位。可由 1 个工作负载组成应用程序，也可以多个工作负载组成 1 个应 用程序。\na) 选择服务→应用程序表，可查看表里面的工作负载。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.1-add/",
	"title": "4.2.1 工作负载创建",
	"tags": [],
	"description": "",
	"content": " 在群组管理创建和运行工作负载。\na) 点击服务→应用程序表选择→工作负载群组+按钮(工作负载创建)。 选择工作负载类型。 b) 输入基本信息、容器、实例、更新策略、服务端口、卷的内容，点击创建按钮生成工作负 载。  工作负载基本信息定义      项目 说明     名称 需要创建的工作负载名称   群组 创建工作负载的工作负载群组   类型 工作负载的实例类型(Single/Multi)   说明 工作负载的说明     容器生成\n 点击容器右‘+’键  输入容器名称，在容器的 Build 或映像中选择需要使用的映像，定义需要使用的 CPU/ 内存的资源值。  一旦容器创建，可输入需要使用的命令。  输入容器的环境参数值，或者可选择 Secret 设置。  可设置安全策略  可设置容器的健康检查   实例(只适用于服务器类型为 Multi 时)\n 点击实例项目的‘✎’       项目 说明     自动缩放类型 定义自动缩放的基准   CPU 使用率(百分比) 自动缩放的基准(使用率)   内存使用率(百分比) 自动缩放的基准(使用率)   最大个数 缩放实例最大值   实例数 默认实例台数     更新策略(只适用于服务器类型为 Multi 时)\n 点击更新策略项目的‘✎’按钮       项目 说明     更新策略 定义滚动更新/重建。通过滚动更新定义，中断 Pod 和扩展 Pod，可无 中断更新以增强资源效率，也可通过重建一次性更新   服务去除(中断 Pod 比率) 滚动更新时，被中断的 Pod 比率   扩展 Pod 比率 滚动更新时，能增加 Pod 比率     服务端口  部署在 Baremetal(使用节点端口时，Ingress 使用为选项)  部署在公共云(使用节点端口时，Ingress 使用为必项)      项目 说明     Internal 容器内部通信网络   Cluster IP Pod内部通信网络   Node Port 对外泄露网络   Loadbalancer 负载均衡通信网络   Sticky Session 按照特定客户端的请求，保持 Session 的选项       项目 说明     名称 定义端口名称   目标端口 定义容器端口   端口 定义外部端口   指定与否 是否指定节点端口使用   Ingress 使用与否 Ingress 使用与否。(部署到公共云时，负载均衡类型不能使用 Ingress， 使用节点端口类型时，Ingress 是必项。但，Ingress 在 Baremetal 里是作 为选项，可同样使用)     卷      项目 说明     卷 在工作负载创建需要使用的卷   挂载卷 工作负载挂在已创建卷上   持久化挂载卷 工作负载挂在已创建持久化卷上       卷种类 说明     Empty Dir 容器里面的临时目录卷   Host Path 挂在容器的集群服务器卷   Config Map 使用工作负载上已注册的配置文件   Secret 使用工作负载上已注册的 Secret 文件(可设置有无权限使用，使用 CHMOD 规 则。不使用时的默认权限为 644)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.1-add_ko/",
	"title": "4.2.1 워크로드 생성",
	"tags": [],
	"description": "",
	"content": " 그룹관리에서 워크로드를 생성하고 실행한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 그룹 + 버튼(워크로드 생성)을 클릭한다. 워크로드 유형을 선택 한다. b) 기본정보, 컨테이너, 인스턴스, 업데이트 정책, 서비스 포트, 볼륨 내용을 작성하고 생성버튼을 눌러 워크로드를 생성한다.  워크로드 기본 정보 정의      항목 설명     이름 생성하는 워크로드의 이름   그룹 워크로드가 생성되는 워크로드 그룹   유형 워크로드의 인스턴스 유형(Single/Multi)   설명 워크로드의 설명   서비스 계정 명 서비스 계정 명   노드 셀렉터 키 노드 셀렉터 키   노드 셀렉터 값 노드 셀렉터 값     컨테이너 생성\n 컨테이너 우측 \u0026lsquo;+ 버튼\u0026rsquo; 클릭  컨테이너 이름을 입력하고, 컨테이너로 사용할 이미지를 빌드나 이미지에서 선택하고, 사용할 CPU/Memory 리소스값을 정의한다.  컨테이너가 생성하면서 사용할 커맨드를 입력 할 수 있다.  컨테이너의 환경변수 값을 입력하거나 시크릿을 선택하여 설정할 수 있다.  보안정책을 설정 할 수 있다.  컨테이너의 헬스체크를 설정 할 수 있다.   인스턴스\n 인스턴스 항목의 \u0026lsquo;✎ 버튼\u0026rsquo; 클릭  서버 유형이 Single인 경우 인스턴스 종료 대기시간만 조절 가능  (서버 유형이 Multi일 경우만 해당)       항목 설명     오토스케일링 유형 오토스케일링을 하기 위한 기준을 정의   CPU 사용율(백분율) 오토스케일링의 기준(사용율)   메모리 사용률(백분율) 오토스케일링의 기준(사용율)   최대개수 스케일링의 인스턴스 최댓값   인스턴스 수 기본 인스턴스의 대수     업데이트 정책(서버 유형이 Multi일 경우만 해당)\n 업데이트 정책 항목의 \u0026lsquo;✎ 버튼\u0026rsquo; 클릭       항목 설명     업데이트 정책 Rolling Update/Recreate로 정의. 롤링업데이트를 통해 중단 Pod와 확장 Pod를 정의하여 자원의 효율성을 증대한 무중단 업데이트가 가능하고, Recreate 통하여 한 번에 업데이트가 가능   서비스 제거(중단Pod비율) 롤링업데이트를 할 경우, 중단되는 Pod의 비율   확장 Pod비율 롤링업데이트를 할 경우, 늘어날 수 있는 Pod의 비율     서비스 포트      항목 설명     Internal 컨테이너 내부통신 네트워크   Cluster IP Pod 내부통신 네트워크   Node Port 외부노출 네트워크   Loadbalancer 로드밸런스 통신 네트워크   Sticky Session 특정 클라이언트 요청에 세션을 유지하는 옵션   타임아웃(초) Sticky Session 유지 시간       항목 설명     이름 포트 이름을 정의   타겟 포트 컨테이너 포트를 정의   포트 외부 포트를 정의   지정 여부 node port를 지정하여 사용할 것인지에 대한 여부   인그레스 사용 여부 인그레스를 사용할 것인지에 대한 여부. (Public Cloud로 배포할 경우 Loadbalancer 타입으로는 인그레스 사용이 불가)     볼륨      항목 설명     볼륨 워크로드에서 사용할 볼륨을 생성   볼륨 마운트 생성된 볼륨에 워크로드를 마운트   퍼시스턴트 볼륨 마운트 생성된 퍼시스턴트 볼륨에 워크로드를 마운트       볼륨 종류 설명     Empty Dir 컨테이너 안에 임시 디렉토리 볼륨   Host Path 컨테이너가 올라가 있는 클러스터 서버 볼륨   Config Map 워크로드에 등록 되어있는 Config File을 사용   Secret 워크로드에 등록 되어있는 Secret File을 사용 (권한사용 유무 설정이 가능하고, CHMOD 룰을 사용. 미사용시 Default 권한은 644)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.2-edit/",
	"title": "4.2.2 工作负载编辑",
	"tags": [],
	"description": "",
	"content": " 编辑和重新部署运行或者停止状态的工作负载。\na) 服务→选择应用程序表→点击工作负载。 b) 编辑基本信息、容器、实例、更新策略、服务端口、卷内容后，按修改重新部署工作负载。 如果工作负载正在运行中，不能修改工作负载类型和容器映像。但，可以修改容器映像标签。 工作负载中断后，可以编辑以及修改。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.2-edit_ko/",
	"title": "4.2.2 워크로드 편집",
	"tags": [],
	"description": "",
	"content": " Running 또는 Stopped 상태의 워크로드를 편집하고 재배포 한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드를 클릭한다. b) 기본정보, 컨테이너, 인스턴스, 업데이트 정책, 서비스 포트, 볼륨 내용을 편집하고 수정버튼을 눌러 워크로드를 재배포 한다. 워크로드 Running 중이라면 워크로드유형과 컨테이너의 이미지 변경이 불가능. 단, 컨테이너 이미지태그 변경은 가능. 이를 변경하기 위해서는 워크로드 중지 후 편집하면 편집이 가능 하다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.3-instance/",
	"title": "4.2.3 工作负载实例信息查询",
	"tags": [],
	"description": "",
	"content": " 可查询工作负载实例信息。\n   实例 说明     Pods 详细信息、状态信息、事件信息、部署信息、网络终端(容器)、日 志(容器)   Services 详细信息、事件信息、部署信息   Persistent Volume Claims 详细信息、卷详细信息、部署信息   Deployments 详细信息、事件信息、部署信息、自动缩放信息(选项)   Replica Sets 详细信息、事件信息、部署信息   Ingresses 详细信息、部署信息    a) 服务→选择应用程序表→点击工作负载。  在实例页面点击刷新时，只重新查询实例信息。   b) 实例 Pod→容器和网络终端连接 c) 实例 Pod→容器日志 d) 服务→选择应用程序表→选择工作负载→点击实例的‘操作’按钮(激活)→点击详细信 e) 服务→选择应用程序表→选择工作负载→点击实例的‘操作’按钮(激活)→点击事件信 息 f) 服务→选择应用程序表→选择工作负载→点击实例的‘操作’按钮(激活)→点击部署信息 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.3-instance_ko/",
	"title": "4.2.3 워크로드 인스턴스 정보 조회",
	"tags": [],
	"description": "",
	"content": " 워크로드 인스턴스의 정보를 조회 할 수 있다.\n   인스턴스 정보     Instance 상세 정보, 상태 정보, 이벤트 정보, 배포 정보, 웹터미널(컨테이너), 로그(컨테이너)   Services 상세 정보, 이벤트 정보, 배포 정보   Persistent Volume Claims 상세 정보, 볼륨 상세 정보, 배포 정보   Controller 상세 정보, 이벤트 정보, 배포 정보, 오토스케일러(Deployments만 해당) 정보(옵션)   Ingresses 상세 정보, 배포 정보    a) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 워크로드 선택  상태에서 새로고침 클릭시 상태정보(서버상태, 사용량, 인스턴스 등등)가 재조회 된다.   b) 인스턴스 이름 클릭 c) 인스턴스 → 컨테이너와 웹 터미널 연결 d) 인스턴스 → 컨테이너의 로그 인스턴스 액션메뉴 e) 인스턴스의 액션 버튼을 클릭(활성화) → 상태 정보 클릭 f) 인스턴스의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭 g) 인스턴스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 h) 컨트롤러 이름 클릭 컨트롤러 액션메뉴  Type Replicaset  Type Deployments   i) 컨트롤러의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭 j) 컨트롤러의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 k) 컨트롤러의 액션 버튼을 클릭(활성화) → 오토스켈러 정보(Type: Deployments) 클릭 l) 오토스켈러 정보 팝업 → 이름 클릭 오토스켈러 팝업 액션메뉴 m) 오토스켈러 팝업 액션 버튼을 클릭(활성화) → 상태 정보 클릭 n) 오토스켈러 팝업 액션 버튼을 클릭(활성화) → 배포 정보 클릭 o) 서비스 이름 클릭 서비스 액션메뉴 p) 서비스의 액션 버튼을 클릭(활성화) → 이벤트 정보 클릭 q) 서비스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 r) 볼륨 요청 이름 클릭 s) 볼륨 요청 볼륨 클릭 볼륨 요청 액션메뉴 t) 볼륨 요청의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 u) 인그레스 이름 클릭 인그레스 액션메뉴 v) 인그레스의 액션 버튼을 클릭(활성화) → 배포 정보 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.4-stop/",
	"title": "4.2.4 工作负载中断",
	"tags": [],
	"description": "",
	"content": " 中断已部署的工作负载。如果中断工作负载，POD 被删除。\na) 服务→选择应用程序表→点击运行状态的工作路径操作菜单(激活)→点击中断  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.4-stop_ko/",
	"title": "4.2.4 워크로드 중지",
	"tags": [],
	"description": "",
	"content": " 배포된 워크로드를 중지한다. 워크로드를 중지하면 POD는 삭제된다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Running 상태의 워크로드 액션메뉴를 클릭(활성화) → 중지를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.5-start/",
	"title": "4.2.5 工作负载开启",
	"tags": [],
	"description": "",
	"content": " 开启已中断的工作负载。\na) 服务→选择应用程序表→点击停止状态的工作路径操作菜单(激活)→点击开始 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.5-start_ko/",
	"title": "4.2.5 워크로드 시작",
	"tags": [],
	"description": "",
	"content": " 중지된 워크로드를 시작한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Stopped 상태의 워크로드 액션메뉴를 클릭(활성화) → 시작을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.6-restart/",
	"title": "4.2.6 工作负载重启",
	"tags": [],
	"description": "",
	"content": " 重启工作负载。类似计算机重启的概念。\na) 服务→选择应用程序表→点击运行状态的工作路径操作菜单(激活)→点击重启 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.6-restart_ko/",
	"title": "4.2.6 워크로드 재시작",
	"tags": [],
	"description": "",
	"content": " 워크로드를 재시작한다. 컴퓨터의 Rebooting과 같은 개념이다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Running 상태의 워크로드 액션메뉴를 클릭(활성화) → 재시작을 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.7-delete/",
	"title": "4.2.7 工作负载删除",
	"tags": [],
	"description": "",
	"content": " 删除已中断的工作负载。\na) 服务→选择应用程序表→点击停止状态的工作路径操作菜单(激活)→点击删除 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.2.workload/4.2.7-delete_ko/",
	"title": "4.2.7 워크로드 제거",
	"tags": [],
	"description": "",
	"content": " 중지된 워크로드를 제거한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → Stopped 상태의 워크로드 액션메뉴를 클릭(활성화) → 제거를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.3.monitoring/",
	"title": "4.3 监控",
	"tags": [],
	"description": "",
	"content": " 根据不同的工作负载、POD、容器，可查询正在使用的 CPU、内存、网络使用量。\na) 服务→选择应用程序表→点击监控 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.4.pipeline/",
	"title": "4.4 管道",
	"tags": [],
	"description": "",
	"content": " 通过管道功能，可一次性进行映像 Build 作业到工作负载部署作业。\n并且，可修改正在部署的工作负载映像版本再重新部署。\na) 服务→选择应用程序表→点击管道    管道菜单 说明     批量运行 批量运行管道作业   运行 运行相应管道作业       映像菜单 Build 说明     映像标签 X 输入注册表标签时，部署相应版本的工作负载   指定 ⃝ 映像版本中部署指定版本的工作负载   最新 ⃝ 映像版本中部署最新版本的工作负载   Build\u0026amp;部署 ⃝ 新的 Build 作业后，部署相应映像版本的工作负载    b) 管道运行 1. 使用通用映像创建工作负载时 输入映像标签后，点击‘运行’或‘批量运行’(只有部署版本和输入版本不一致时可以运行。 但 latest 除外) 2. 使用 Build 映像创建工作负载时  使用指定映像部署  在管道作业目录右侧点击‘指定’以及选择映像后，点击‘运行’或者‘批量运行’(只有部 署版本和输入版本不一致时可以运行)  使用指定映像部署  在管道作业目录右侧选择‘Build\u0026amp;部署’后，确认‘运行与否’。之后点击‘运行’或‘批量 运行’(只有确认运行与否时可以运行)  使用最新映像部署  在管道作业目录右侧选择‘最新’后，点击‘运行’或‘批量运行’。(只有部署版本和输入版 本不一致时可以运行)  切换到 Build 编辑画面及日志查看  点击 Build 映像名称时，切换到 Build 编辑画面。 点击 Build 标签名称时，可以看到日志。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.5.persistencevolume/",
	"title": "4.5 持久化卷",
	"tags": [],
	"description": "",
	"content": " 提供持久化卷的现状以及创建和删除。\na) 服务→应用程序表→持久化卷    项目 说明     卷名称 点击已创建持久化卷名称时，切换到持久化卷详细页面   卷类型 卷类型(Single、Shared)   状态 卷的状态(Ready, Mounted)   使用量 使用量、请求量、总量   Age 卷创建需要时间   Filter 通过搜索查询所必要的持久化卷   + 按钮 切换到持久化卷创建页面     持久化卷详细  持久化卷操作菜单   a) 卷请求信息 b) 卷信息 c) 存储信息 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.5.persistencevolume/4.5.1-add/",
	"title": "4.5.1 持久化卷创建",
	"tags": [],
	"description": "",
	"content": " 可以创建持久化卷。\na) 服务→应用程序表→持久化卷    项目 说明     持久化卷类型 卷类型(Single, Shared)   存储 显示集群里已注册的存储目录   访问模式 持久化卷类型为 Single 时和 ReadWriteOnce, Shard 时分别选择 ReadWriteMany、ReadOnlyMany   名称 需要创建的持久化卷名称   容量 需要创建的持久化卷容量(GB)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.5.persistencevolume/4.5.1-add_ko/",
	"title": "4.5.1 퍼시스턴트 볼륨 생성",
	"tags": [],
	"description": "",
	"content": " 퍼시스턴트 볼륨 생성 할 수 있다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨    항목 설명     퍼시스턴트 볼륨 타입 볼륨 타입(Single, Shared)   스토리지 클러스터에 등록된 스토리지 목록 노출   엑세스 모드 퍼시스턴트 볼륨 타입이 Single일 경우 ReadWriteOnce, Shard일 경우 ReadWriteMany, ReadOnlyMany를 선택   이름 생성할 퍼시스턴트 볼륨의 이름   용량 생성할 퍼시스턴트 볼륨의 용량(GB)    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.5.persistencevolume/4.5.2-delete/",
	"title": "4.5.2 持久化卷删除",
	"tags": [],
	"description": "",
	"content": " 删除持久化卷。\na) 点击服务→应用程序表→持久化卷→持久化操作菜单(激活)→删除 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.5.persistencevolume/4.5.2-delete_ko/",
	"title": "4.5.2 퍼시스턴트 볼륨 삭제",
	"tags": [],
	"description": "",
	"content": " 퍼시스턴트 볼륨을 삭제한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 → 퍼시스턴트 볼륨 → 퍼시스턴트 액션메뉴(활성화) → 삭제 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/",
	"title": "4.6.1 配置图",
	"tags": [],
	"description": "",
	"content": " 可将配置文件以 Map 形式保存于服务器，如同卷使用。\na) 服务→选择应用程序表→设置→点击配置图    项目 说明      切换到配置图添加页面   名称 配置图名称(用户指定)   说明 配置图说明(用户指定)    配置图的操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/4.6.1.1-add/",
	"title": "4.6.1.1 配置图添加",
	"tags": [],
	"description": "",
	"content": " 服务里添加配置图添加。\na) 服务→选择应用程序表→设置→配置图→点击配置图创建按钮，切换到注册页面。 b) 输入名称和数据值，按右上角创建按钮，创建配置图。    项目 说明     名称 输入名称的栏。之后不能编辑   说明 输入配置图说明的栏。   数据添加按钮 能添加配置图数据的按钮   KEY 配置图的秘钥值   VALUE 配置图的卷值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/4.6.1.1-add_ko/",
	"title": "4.6.1.1 컨피그 맵 추가",
	"tags": [],
	"description": "",
	"content": " 서비스에 컨피그 맵을 추가한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 생성 버튼을 클릭하여 등록페이지로 이동한다. b) 이름과 데이터의 값을 입력하고 우측 상단 생성 버튼을 눌러 컨피그 맵을 생성한다.    항목 설명     이름 이름을 입력하는 란입니다. 추후 편집이 불가   설명 컨피그 맵의 설명을 입력하는 란입니다.   데이터 추가 버튼 컨피그 맵의 데이터 추가할 수 있는 버튼   KEY 컨피그 맵의 Key 값   VALUE 컨피그 맵의 Value 값    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/4.6.1.2-edit/",
	"title": "4.6.1.2 配置图修改",
	"tags": [],
	"description": "",
	"content": " 可修改服务里已添加的配置图数据。(名称不能修改)\na) 服务→选择应用程序表→设置→配置图→配置图操作菜单(激活)→点击配置图修改 b) 修改数据值后，点击修改按钮，编辑配置图。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/4.6.1.2-edit_ko/",
	"title": "4.6.1.2 컨피그 맵 수정",
	"tags": [],
	"description": "",
	"content": " 서비스에 추가 된 컨피그 맵의 데이터를 수정 할 수 있다.(이름 변경불가)\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 액션메뉴(활성화) → 컨피그 맵 수정을 클릭 b) 데이터 값 변경 후, 수정 버튼을 클릭하여 컨피그 맵을 편집한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/4.6.1.3-delete/",
	"title": "4.6.1.3 配置图删除",
	"tags": [],
	"description": "",
	"content": " 删除服务里已添加的配置图。\na) 服务→选择应用程序表→设置→配置图→配置图操作菜单(激活)→点击配置图删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.1.configmap/4.6.1.3-delete_ko/",
	"title": "4.6.1.3 컨피그 맵 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스에 컨피그 맵을 삭제한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 컨피그 맵 → 컨피그 맵 액션메뉴(활성화) → 컨피그 맵 삭제 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/",
	"title": "4.6.2 Secret",
	"tags": [],
	"description": "",
	"content": " 以 Secret 文件形式保存于服务器，如同环境参数及卷使用。\na) 服务→选择应用程序表→设置→点击 Secret。    项目 说明      切换到 Secret 添加页面   名称 Secret 名称(用户指定)   说明 Secret 说明(用户指定)    Secret 的操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/",
	"title": "4.6.2 Secret",
	"tags": [],
	"description": "",
	"content": " 以 Secret 文件形式保存于服务器，如同环境参数及卷使用。\na) 服务→选择应用程序表→设置→点击 Secret。    项目 说明      切换到 Secret 添加页面   名称 Secret 名称(用户指定)   说明 Secret 说明(用户指定)    Secret 的操作菜单    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/4.6.2.1-add/",
	"title": "4.6.2.1 Secret 添加",
	"tags": [],
	"description": "",
	"content": " 服务里添加 Secret。\na) 服务→选择应用程序表→设置→Secret→点击 Secret 创建按钮，切换到注册页面。 b) 输入名称和数据值，点击右上角创建按钮，创建 Secret。    项目 说明     名称 输入名称的栏。之后不能编辑   说明 输入 Secret 说明的栏。   数据添加按钮 能添加 Secret 数据的按钮   KEY Secret 的秘钥值   VALUE Secret 的卷值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/4.6.2.1-add_ko/",
	"title": "4.6.2.1 시크릿 추가",
	"tags": [],
	"description": "",
	"content": " 서비스에 시크릿을 추가한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 생성 버튼을 클릭하여 등록페이지로 이동한다. b) 이름과 데이터의 값을 입력하고 우측 상단 생성 버튼을 눌러 시크릿을 생성한다.    항목 설명     이름 이름을 입력하는 란입니다. 추후 편집이 불가   설명 시크릿의 설정의 설명 입력 란입니다.   데이터 추가 버튼 시크릿의 데이터를 추가할 수 있는 버튼   KEY 시크릿의 Key 값   VALUE 시크릿의 Value 값    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/4.6.2.2-edit/",
	"title": "4.6.2.2 Secret 修改",
	"tags": [],
	"description": "",
	"content": " 可修改服务里已添加的 Secret 数据。(名称不能修改)\na) 服务→选择应用程序表→设置→Secret→Secret 操作菜单(激活)→点击 Secret 修改。 b) 修改数据值后，点击修改按钮，编辑 Secret。(为了安全考虑 Secret 值不显示) "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/4.6.2.2-edit_ko/",
	"title": "4.6.2.2 시크릿 수정",
	"tags": [],
	"description": "",
	"content": " 서비스에 추가 된 시크릿의 데이터를 수정 할 수 있다.(이름 변경불가)\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 액션메뉴(활성화) → 시크릿 수정을 클릭 b) 데이터 값 변경 후, 수정 버튼을 클릭하여 시크릿을 편집한다.(시크릿 값은 보안상 보여주지 않음) "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/4.6.2.3-delete/",
	"title": "4.6.2.3 Secret 删除",
	"tags": [],
	"description": "",
	"content": " 删除服务里的 Secret。\na) 服务→选择应用程序表→设置→Secret→Secret 操作菜单(激活)→点击 Secret 删除。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/4.service/4.6.config/4.6.2.secret/4.6.2.3-delete_ko/",
	"title": "4.6.2.3 시크릿 삭제",
	"tags": [],
	"description": "",
	"content": " 서비스에 시크릿을 삭제한다.\na) 좌측 메인메뉴 → 서비스 → 서비스 맵 선택 → 설정 → 시크릿 → 시크릿 액션메뉴(활성화) → 시크릿 삭제를 클릭 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.1-add/",
	"title": "5.1 Build 添加",
	"tags": [],
	"description": "",
	"content": " 添加 Docker 映像的 Build 作业。应用程序下载、Build、映像 Build 共 3 种可选。\na) 服务→Build 管理→选择 Build 添加，切换到该页面。 b) 输入应用程序下载、应用程序 Build、映像 Build 内容，点击创建按钮，创建服务器。    Build 生成阶段 说明     应用程序下载 下载 Build 必要的源代码   应用程序 Build 需要编译源代码时使用命令 - Build 时将运行的作业主机路径 - 容器工作路径和其挂载的主机路径Working dir - 实际容器内部工作路径，和容器路径对接映像 - Build 时所使用的映像   映像 Build 利用前述作业的源代码创建 Docker 文件生成映像后，该映像存储于 注册表存储库的阶段     Build 基本信息     基本信息 说明     名称 需要创建的 Build 名称   最新操作 Build 的最新工作状态   状态 Build 的当前状态     应用程序下载     应用程序下载 说明     知识库种类 下载源文件版本管理工具的种类。当前版本只支持 GIT   协议类型 支持知识库提供的 HTTP/HTTPS 协议   Git 存储库类型 支持 Private/Common   知识库 URL 下载为目的地存储库 URL   知识库 User ID 拥有存储库权限的 ID   知识库 Password 拥有存储库权限的 ID 密码   目标分支(Branch) 需要下载的源代码分支信息     应用程序 Build     应用程序 Build 说明     命令 Build 时将运行的命令语   主机路径 带已下载源代码的路径   容器路径 需要工作的容器路径   Working dir 实际容器内部的工作路径，与容器路径对接   映像 需要运行 Build 的容器映像     映像 Build     映像 Build 说明     Dockerfile 为了创建映像准备的 Dockerfile   注册表名称 选择保存已创建映像的注册表   映像 保存在注册表的已创建映像的名称和标签    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.1-add_ko/",
	"title": "5.1 빌드 추가",
	"tags": [],
	"description": "",
	"content": " 도커 이미지에 대한 빌드 작업을 추가한다. 어플리케이션 다운로드, 빌드, 이미지 빌드로 총 3가지를 선택 할 수 있다.\na) 좌측 메인 메뉴 → 빌드 화면에서 [+] 버튼을 클릭하여 해당 페이지로 이동한다. b) 기본 정보, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드 내용을 작성하고 [생성] 버튼을 눌러 빌드를 생성한다.    빌드 생성 단계 설명     어플리케이션 다운로드 빌드에 필요한 소스를 다운로드   어플리케이션 빌드 소스의 컴파일이 필요할 경우 사용커맨드 - 빌드하면서 실행될 작업호스트 경로 - 작업이 이루어질 컨테이너 경로와 그와 마운트되는 호스트경로Working dir -실제 컨테이너안에서 작업할 경로로 컨테이너 경로와 맞춤이미지 - 빌드시 사용될 이미지   이미지 빌드 앞서 작업된 소스로 도커파일을 만들어 이미지를 만들고 해당이미지를 레지스트리 저장소에 저장하는 단계     빌드 기본 정보     기본 정보 설명     이름 생성할 빌드의 이름   최근 액션 빌드의 최근 동작 상태   상태 빌드의 현재 상태     어플리케이션 다운로드     어플리케이션 다운로드 설명     리파지토리 종류 소스파일을 내려받을 버전관리툴의 종류. 현재 버전에서는 GIT만을 지원   프로토콜 유형 리파지토리에서 제공하는 HTTP/HTTPS 프로토콜을 지원   깃 저장소 유형 Private/Common을 지원   리파지토리 URL 다운로드 하기위한 저장소 URL   리파지토리 User ID 저장소의 권한을 갖는 ID   리파지토리 패스워드 저장소의 권한을 갖는 ID의 패스워드   타겟 브랜치(Branch) 다운로드할 소스의 브랜치 정보     어플리케이션 빌드     어플리케이션 빌드 설명     커맨드 빌드하면서 실행될 명령어   호스트경로 다운로드 된 소스가 있는 경로   컨테이너 경로 작업이 이루어질 컨테이너 경로   Working dir 실제 컨테이너안에서 작업할 경로로 컨테이너 경로와 맞춤   이미지 빌드를 실행할 컨테이너 이미지     이미지 빌드     이미지 빌드 설명     Dockerfile 이미지를 만들기 위한 Dockerfile   레지스트리명 생성한 이미지를 저장하는 레지스트리를 선택   이미지 생성한 이미지를 레지스트리에 저장 할 이미지 명   버전 생성한 이미지를 레지스트리에 저장 할 태그    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.2-edit/",
	"title": "5.2 作业设置编辑",
	"tags": [],
	"description": "",
	"content": " 编辑 Build 作业。Build 名称不能编辑。在编辑页面，可删除现已下载的源代码，也可以分别运行应用程序下载、应用程序 Build、映像 Build3 种作业。\na) 服务→Build 管理→Build 操作菜单(激活)→选择作业设置编辑，切换到该页面。    作业设置编辑 说明     源文档删除 删除下载源文档   应用程序下载运行 只运行应用程序下载   Build 高速缓存删除 删除现已 Build 的容器   应用程序 Build 运行 只运行应用程序 Build   映像 Build 运行 只运行映像 Build    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.2-edit_ko/",
	"title": "5.2 작업 설정 편집",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 편집한다.\na) 좌측 메인 메뉴 → 빌드 → 빌드 액션 메뉴(활성화) → [작업 설정 편집]을 선택하여 해당 페이지로 이동한다. b) 기본 정보, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드 내용을 수정하고 [수정] 버튼을 눌러 작업 설정을 편집한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.3-history/",
	"title": "5.3 历史记录",
	"tags": [],
	"description": "",
	"content": " 切换到该 Build 历史页面，可针对 Build 运行项查看历史信息。可通过 Build 运行单位查看日志。\na) 服务→Build 管理→Build 操作菜单(激活)→选择历史记录，切换到该页面。 b) Build 作业历史菜单中，确认该 Build 作业的操作名称、状态、日期、作业时间、映像名称、 映像大小、Build 日志。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.3-history_ko/",
	"title": "5.3 히스토리",
	"tags": [],
	"description": "",
	"content": " 해당 빌드의 히스토리 페이지로 이동하여 빌드 실행 항목에 대하여 히스토리 정보를 볼 수 있다.\na) 좌측 메인 메뉴 → 빌드 화면에서 빌드 액션 메뉴(활성화) → [히스토리]를 클릭하여 해당 페이지로 이동한다. b) 액션 메뉴(활성화) → [로그보기]를 클릭하여 빌드 로그를 확인할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.4-run/",
	"title": "5.4 Build 运行",
	"tags": [],
	"description": "",
	"content": " 运行 Build 作业。如果运行 Build，就一次性运行应用程序下载、应用程序 Build、映像 Build。\na) 选择服务→Build 管理→Build 操作菜单(激活)→Build 运行，执行 Build 作业。 b) 运行 Build 时，当前操作被修改，同时日志窗被激活。 Build 生成时，只限于选框中被选的数量标签进行激活，可查看各阶段 Build 过程。\n(如果 DOWN 阶段发生错误，不会进行下一步。)\n如果 Build 运行作业中看到 \u0026ldquo;Application_Create_Image Step is done\u0026hellip;\u0026rdquo; 的日志, 说明映像 Build 已 成功完成。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.4-run_ko/",
	"title": "5.4 빌드 실행",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 실행한다. 빌드 실행을 하면, 어플리케이션 다운로드, 어플리케이션 빌드, 이미지 빌드를 한 번에 실행한다.\na) 빌드 목록에서 실행할 빌드 작업의 빌드 액션 메뉴(활성화) → [빌드 실행]을 선택하면 빌드실행 창이 활성화된다. b|) 설명을 입력 후, [승인] 버튼을 클릭하면 빌드 작업을 실행한다. c) 빌드 실행 시, 현재 액션이 변경되면서 빌드 로그 창이 활성화 된다. 빌드생성시 체크박스에서 선택된 순서대로 순차적으로 실행되며 각 단계별로 빌드 과정을 확인 할 수 있다.\n(DOWN 단계에서 오류가 발생하면 다음 단계로 진행이 되지 않는다.)\n빌드 실행 작업이 \u0026ldquo;CREATE IMAGE STEP DONE.\u0026rdquo; 이라는 로그가 보인다면 이미지 빌드는 성공적으로 마무리된 것이다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.5-cancel/",
	"title": "5.5 Build 取消",
	"tags": [],
	"description": "",
	"content": " 取消 Build 作业。只能取消运行状态(RUNNING)的 Build 作业。\na) 选择服务→Build 管理→Build 操作菜单(激活)→Build 取消，取消 Build 作业。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.5-cancel_ko/",
	"title": "5.5 빌드 취소",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 취소한다. 빌드작업이 실행 상태(RUNNING)인 것만 취소할 수 있다.\na) 빌드 목록에서 취소할 빌드 작업의 빌드 액션 메뉴(활성화) → [빌드 취소]를 선택하여 빌드 작업을 취소한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.6-log/",
	"title": "5.6 日志查看",
	"tags": [],
	"description": "",
	"content": " 显示 Build 作业的最新运行日志。如果 Build 作业在运行中，就可以查看日志。\na) 选择服务→Build 管理→Build 操作菜单(激活)→日志查看，可查看该 Build 作业的 Build 运行日志。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.6-log_ko/",
	"title": "5.6 로그 보기",
	"tags": [],
	"description": "",
	"content": " 빌드작업의 최근 실행 로그를 보여준다. 빌드실행 도중이라면 실행 중인 로그를 볼 수 있다.\na) 빌드 목록에서 로그 보기할 빌드 작업의 빌드 액션 메뉴(활성화) → [로그 보기]를 선택하여 해당 빌드 작업의 빌드 실행 로그를 볼 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.7-delete/",
	"title": "5.7 作业删除",
	"tags": [],
	"description": "",
	"content": " 删除 Build 作业。删除时，先删除该高速缓存和已下载源文档后，再进行删除作业。\na) 选择服务→Build 管理→Build 操作菜单(激活)→作业删除，可删除该 Build 作业。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/5.build/5.7-delete_ko/",
	"title": "5.7 작업 삭제",
	"tags": [],
	"description": "",
	"content": " 빌드 작업을 삭제한다.\na) 빌드 목록에서 삭제할 빌드 작업의 빌드 액션 메뉴(활성화) → [작업 삭제]를 선택하여 해당 빌드 작업을 삭제할 수 있다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/6.1-add/",
	"title": "6.1 目录保存",
	"tags": [],
	"description": "",
	"content": " 可以创建目录，一次性捆绑部署所需要的服务器。当工作负载容器映像为 Build 时，只保存映像，并转换成共享映像后，保存于目录。在服务目录页面，可查看已保存的目录。\n目录保存时，以模板形式保存设置的配置图、Secret。 (但，为了安全考虑 Secret 值不保存。)\na) 在服务页面，选择保存为目录的应用程序表。 b) 点击页面右上角的 \u0026ldquo;保存为目录\u0026rdquo; 按钮。 c) 创建新模板时，选择\u0026rdquo;现有\u0026rdquo;目录，更新\u0026rdquo;新\u0026rdquo;目录和现有模板，然后撰写版本、摘要信息 和编辑内容，再点击下面\u0026rdquo;保存\u0026rdquo;按钮。(编辑、输入字段采用 Mark Down 文件格式, 该文件 的撰写方法可点击页面右侧的\u0026rdquo;Mark Down support\u0026rdquo;查看。) "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/6.2-deploy/",
	"title": "6.2 目录部署",
	"tags": [],
	"description": "",
	"content": " 可以部署已创建目录。必要时，可以排除或编辑、部署不需要的实例。\na) 点击目录→需要部署模板的\u0026rdquo;部署\u0026rdquo;按钮。 b) 在模板目录页面，选择服务、集群、应用程序表后, 再点击\u0026rdquo;部署\u0026rdquo;。除了不需要的实例， 均可以部署。  部署新的应用程序表时，选择服务和集群，撰写应用程序表、命名空间名称后，可进行部 署。  部署现有应用程序表时，选择服务、应用程序表后，可进行部署。   c) 为了安全考虑 Secret 值不保存，应在部署前输入值。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/6.3-edit/",
	"title": "6.3 目录编辑",
	"tags": [],
	"description": "",
	"content": " 可编辑已创建目录内容。可修改模板的版本、摘要信息和编辑内容，同时可以修改各种实例的部署信息。\na) 点击目录→需要编辑模板的\u0026rdquo;编辑\u0026rdquo;按钮。 b) 更改需要修改的模板的版本、摘要信息、编辑内容, 再点击页面右下角的\u0026rdquo;编辑\u0026rdquo;按钮， 可修改实例的部署信息。 c) 编辑页面上只能查看配置图和 Secret 的内容, 不能编辑。 d) 编辑服务器时, 只能查看容器的环境设置, 卷和挂载卷的内容, 不能编辑。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/6.4-delete/",
	"title": "6.4 目录删除",
	"tags": [],
	"description": "",
	"content": " 可删除不必要的目录。(Platform 目录不能删除)\na) 点击目录→需要删除模板的\u0026rdquo;编辑\u0026rdquo;按钮。 b) 选择需要删除模板的版本后, 点击\u0026rdquo;删除\u0026rdquo;按钮, 删除该模板的版本。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/6.5-export/",
	"title": "6.5 目录出口",
	"tags": [],
	"description": "",
	"content": " 존재하는 카탈로그를 내보내기할 수 있다. (플랫폼 카탈로그는 내보내기 불가)\na) 카탈로그 → 내보내기 하고자 하는 템플릿 \u0026lsquo;편집\u0026rsquo; 버튼을 클릭 b) 화면 왼쪽 상단의 \u0026lsquo;내보내기\u0026rsquo; 버튼을 클릭 한다. 해당 카탈로그가 카탈로그명-버전.zip 로 다운로드 된다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/6.catalog/6.6-import/",
	"title": "6.6 目录进口",
	"tags": [],
	"description": "",
	"content": " 내보내기된 카탈로그 파일과 가져오기 기능을 통해 카탈로그를 생성할 수 있다. (플랫폼 카탈로그는 가져오기 불가)\na) 카탈로그 → \u0026lsquo;가져오기\u0026rsquo; 버튼을 클릭 b) \u0026lsquo;Choose File\u0026rsquo; 버튼을 클릭해 가져오기 할 카탈로그 압축파일을 선택한 후 \u0026lsquo;저장\u0026rsquo; 버튼을 누른다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.1-cluster/",
	"title": "7.1 集群管理",
	"tags": [],
	"description": "",
	"content": " 注册、编辑、删除从供应商那里得到分配的 Kubernetes 集群信息的功能。\na) 点击集群→\u0026rdquo;排列\u0026rdquo;按钮，选择需要的排列标准。     集群设置 说明     集群 集群的种类   节点 集群的节点数字   CPU 集群所有的 CPU 容量、请求量、使用量   内存 集群所有的内存容量、请求量、使用量   应用程序 集群的应用程序表(命名空间)数   工作负载 正在部署在集群的工作负载数    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.2-node/",
	"title": "7.2 节点",
	"tags": [],
	"description": "",
	"content": " 可查询所有节点的全部磁盘、CPU、内存、网络使用量以及个别节点的 Label、状态信息等的 画面。点击节点名称，即可查询节点详细信息。\na) 集群→选择集群→点击节点。 b) 点击节点名称，即可查询节点详细信息。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.3-application/",
	"title": "7.3 应用程序",
	"tags": [],
	"description": "",
	"content": " 可查看所有应用程序的全部 CPU、内存、网络使用量以及个别应用程序的命名空间、实例现状 等的画面。点击应用程序名称，可切换到该应用程序表的页面。\na) 集群→选择集群→点击应用程序。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.4-volume/",
	"title": "7.4 卷",
	"tags": [],
	"description": "",
	"content": " 注册公共云存储以及外部存储的信息，提供当前卷的监控服务。\na) 集群→选择集群→点击卷。    存储设置信息 说明     +按钮 切换到存储添加页面   名称 存储名称(用户指定)   类型 存储种类(NFS/EBS/Google Persistent Disk/Azure Disk)   存储类别名称 已注册在 k8s 的类别名称   策略 存储卷策略设置(Retain,Recyle,Delete)   状态 存储使用状态       卷设置信息 说明     卷名称 PVC 名称   状态 PVC 挂载状态   使用量 PV 已分配的可用量和使用量   访问模式 PV 访问权限   Age PVC 创建所需时间    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.5-alarm/",
	"title": "7.5 报警",
	"tags": [],
	"description": "",
	"content": " 使用此功能接收有关集群的事件。\na) 集群→选择集群→点击报警。 b) 点击报警名称，即可查询详细内容。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.5.1/",
	"title": "7.5.1 报警目录",
	"tags": [],
	"description": "",
	"content": "在以下目录的持续时间内，触发条件仍未改善时报警。\n 报警管理     报警 ID ALM-001     重要程度 警告   报警名称 AlertmanagerDown   持续时间 5分钟   触发条件 无法搜集报警管理度量时启动   解决方案 检查 Prometheus 日志以及报警管理日志和事件。必要时重启 Pod。       报警 ID ALM-002     重要程度 警告   报警名称 AlertmanagerFailedReload   持续时间 10分钟   触发条件 修改管理设置时，重读设置失败时启动   解决方案 检查该 Pod 日志，纠正配置图设置错误。     ETCD3     报警 ID ETC-001     重要程度 危急   报警名称 InsufficientMembers   持续时间 3分钟   触发条件 无法搜集 ETCD 度量时启动   解决方案 检查 ETCD 集群状态。检查 Prometheus 日志和该节点的 etcd 状态。       报警 ID ETC-002     重要程度 危急   报警名称 NoLeader   持续时间 1分钟   触发条件 没有 ETCD leader 时启动   解决方案 检查 ETCD 集群状态。可能是磁盘延迟引起的问题，对 ETCD 集群的所有节 点执行以下命令。 (ETCD Tuning)\n$ sudo ionice -c2 -n0 -p `pgrep etcd`       报警 ID ETC-003     重要程度 警告   报警名称 HighNumberOfLeaderChanges   持续时间 即时   触发条件 最近 1 小时内 Leader 修改 3 次以上时   解决方案 检查 ETCD 集群状态。可能是磁盘延迟引起的问题，对 ETCD 集群的所有节 点执行以下命令。 (ETCD Tuning)\n$ sudo ionice -c2 -n0 -p `pgrep etcd`       报警 ID ETC-004     重要程度 警告   报警名称 HighNumberOfFailedGRPCRequests   持续时间 10分钟   触发条件 最近 5 分钟内 gRPC 方法导出失败达到 1%以上时   解决方案 需要增加 ETCD 集群和 Kubernetes 集群带宽，或者按比例增加集群。       报警 ID ETC-005     重要程度 危急   报警名称 HighNumberOfFailedGRPCRequests   持续时间 5分钟   触发条件 最近 5 分钟内 gRPC 方法导出失败达到 5%以上时   解决方案 需要增加 ETCD 集群和 Kubernetes 集群带宽，或者按比例增加集群。       报警 ID ETC-006     重要程度 危急   报警名称 GRPCRequestsSlow   持续时间 10分钟   触发条件 最近 5 分钟内 gRPC 方法请求等待时间第 99 位百分位大于 150ms 时   解决方案 需要增加 ETCD 集群和 Kubernetes 集群带宽，或者按比例增加集群。       报警 ID ETC-007     重要程度 警告   报警名称 HighNumberOfFailedHTTPRequests   持续时间 10分钟   触发条件 最近 5 分钟内 HTTP end point 请求失败达到 1%以上时   解决方案 需要增加 ETCD 集群和 Kubernetes 集群带宽，或者按比例增加集群。       报警 ID ETC-008     重要程度 危急   报警名称 HighNumberOfFailedHTTPRequests   持续时间 5分钟   触发条件 最近 5 分钟内 HTTP end point 请求失败达到 5%以上时   解决方案 需要增加 ETCD 集群和 Kubernetes 集群带宽，或者按比例增加集群。       报警 ID ETC-009     重要程度 警告   报警名称 HTTPRequestsSlow   持续时间 10分钟   触发条件 最近 5 分钟内 HTTP 请求等待时间第 99 位百分位大于 150ms 时   解决方案 需要增加 ETCD 集群和 Kubernetes 集群带宽，或者按比例增加集群。       报警 ID ETC-010     重要程度 警告   报警名称 EtcdMemberCommunicationSlow   持续时间 10分钟   触发条件 最近 5 分钟内会员之间通信等待时间第 99 位百分位大于 150ms 时   解决方案 需要增加 ETCD 集群带宽，或者按比例增加集群。       报警 ID ETC-011     重要程度 警告   报警名称 HighNumberOfFailedProposals   持续时间 即时   触发条件 最近 1 小时内 raft 协议请求失败达到 5 个以上时\n(RAFT 协议是 ETCD 同步协议)   解决方案 据ETCD metric 文件，Leader 选举暂时失败或者会员人数不足，导致 ETCD 集 群中断时间延长时启动。\n检查有无 Leader 以及被中断的 ETCD 会员。       报警 ID ETC-012     重要程度 警告   报警名称 HighFsyncDurations   持续时间 10分钟   触发条件 最近 5 分钟内 Wal fsync 持续时间第 99 位百分位大于 500ms 时\n(Wal fsync:适用日志项之前，保存于磁盘时导出)   解决方案 据ETCD metric 文件文件，磁盘出现问题时启动。       报警 ID ETC-013     重要程度 警告   报警名称 HighCommitDurations   持续时间 10 分钟   触发条件 最近 5 分钟内承诺持续时间第 99 位百分位大于 250ms 时\n(backend commit: 近期修改磁盘内容的增量快照承诺.)   解决方案 据ETCD metric 文件磁盘出现问题时启动。     General     报警 ID GEN-001     重要程度 警告   报警名称 TargetDown   持续时间 10 分钟   触发条件 无法搜集度量时启动。显示哪些操作失败。   解决方案 检查 Prometheus 日志以及相应作业的 Pod 日志及事件。       报警 ID GEN-002     重要程度 none   报警名称 DeadMansSwitch   持续时间 即时   触发条件 DeadMansSwitch 通知。   解决方案 该报警不通知用户。       报警 ID GEN-003     重要程度 危急   报警名称 TooManyOpenFileDescriptors   持续时间 10 分钟   触发条件 文件描述符的使用率超过 95%时启动   解决方案 修改节点限值。(需要重启节点)       报警 ID GEN-004     重要程度 警告   报警名称 FdExhaustionClose   持续时间 10 分钟   触发条件 通过简单直线回归，预测 4 小时内将发生文件描述符枯竭时启动   解决方案 检查该 Pod 日志及事件。必要时修改节点限值。(需要重启节点)       报警 ID GEN-005     重要程度 危急   报警名称 FdExhaustionClose   持续时间 10 分钟   触发条件 通过简单直线回归，预测 1 小时内将发生文件描述符枯竭时启动   解决方案 检查该 Pod 日志及事件。 必要时修改节点限值。(需要重启节点)     Kube-ApiServer     报警 ID KAS-001     重要程度 危急   报警名称 K8SApiserverDown   持续时间 5 分钟   触发条件 无法搜集 Kube-apiserver 度量时启动   解决方案 检查 Prometheus 日志、kube-apiserver 的日志和事件。 必要时重启 Pod。       报警 ID KAS-002     重要程度 警告   报警名称 K8SApiServerLatency   持续时间 10 分钟   触发条件 最近 10 分钟内请求等待时间第 99 位百分位大于 1s 时启动   解决方案 仍未改善时，添加主节点。     Kube-ControllerManager     报警 ID KCM-001     重要程度 危急   报警名称 K8SControllerManagerDown   持续时间 5 分钟   触发条件 无法搜集 Kube-controller-manager 度量时启动   解决方案 检查 Prometheus 日志、kube-controller-manager 的日志及事件。 必要时重启 Pod。     Kube-Scheduler     报警 ID KSC-001     重要程度 危急   报警名称 K8SSchedulerDown   持续时间 5 分钟   触发条件 无法搜集 Kube-scheduler 度量时启动   解决方案 检查 Prometheus 日志、kube-scheduler 的日志及事件。 必要时重启 Pod。     Kube-State-Metrics     报警 ID KSM-001     重要程度 警告   报警名称 DeploymentGenerationMismatch   持续时间 15 分钟   触发条件 部署所设置的 generation 和所搜集的 generation 不一致时启动   解决方案 检查部署日志及事件。 必要时重新部署 Deployment。       报警 ID KSM-002     重要程度 警告   报警名称 DeploymentReplicasNotUpdated   持续时间 15 分钟   触发条件 部署所设置的拷贝数量被修改，或者可利用状态的拷贝数量不一致时启动   解决方案 因部署修改内容未上报状态导致，检查部署、Pod 日志及事件。       报警 ID KSM-003     重要程度 警告   报警名称 DaemonSetRolloutStuck   持续时间 15 分钟   触发条件 DaemonSet 中有非就绪状态 Pod 时启动   解决方案 检查该 Daemonset、Pod 日志及事件。       报警 ID KSM-004     重要程度 警告   报警名称 K8SDaemonSetsNotScheduled   持续时间 10 分钟   触发条件 DaemonSet 上需要运行的 Pod 数量少于正在运行的 Pod 数量时启动   解决方案 检查该 Daemonset、Pod 日志及事件。\n检查未经部署的节点是否正常。\n主节点被隔离的，检查 Daemonset 是否设置了容忍值。       报警 ID KSM-005     重要程度 警告   报警名称 DaemonSetsMissScheduled   持续时间 10 分钟   触发条件 DaemonSet 错误排列 Pod 时启动   解决方案 检查该 Daemonset、Pod 日志及事件。       报警 ID KSM-006     重要程度 警告   报警名称 PodFrequentlyRestarting   持续时间 10 分钟   触发条件 最近 1 小时内 Pod 重启次数超过 5 次时启动   解决方案 检查该 Pod 日志及事件。必要时重启 Pod。     Kubelet     报警 ID KBL-001     重要程度 警告   报警名称 K8SNodeNotReady   持续时间 1 小时   触发条件 节点状态为非就绪时启动   解决方案 检查该节点状态及事件。通过 ssh 访问节点，检查 kubelet 状态。       报警 ID KBL-002     重要程度 危急   报警名称 K8SManyNodesNotReady   持续时间 1분   触发条件 所有集群中节点状态为非就绪的比例超过 20%时启动   解决方案 检查该节点状态及事件。\n通过 ssh 访问节点，检查 kubelet 状态。       报警 ID KBL-003     重要程度 警告   报警名称 K8SKubeletDown   持续时间 1 小时   触发条件 所有集群中 kubelet metric 搜集失败比例超过 3%时启动   解决方案 检查 Prometheus 日志、该节点状态及事件。\n通过 ssh 访问节点，检查 kubelet 状态。       报警 ID KBL-004     重要程度 危急   报警名称 K8SKubeletDown   持续时间 1 小时   触发条件 所有集群中 kubelet metric 搜集失败比例超过 10%时启动   解决方案 检查 Prometheus 日志、该节点状态及事件。\n通过 ssh 访问节点，检查 kubelet 状态。       报警 ID KBL-005     重要程度 警告   报警名称 K8SKubeletTooManyPods   持续时间 即时   触发条件 节点已部署 Pod 数量超过 100 个时启动(限值为 110)   解决方案 达到限值时，不再创建 Pod。\n同样检查其他节点状态，如没有富余增加节点。     节点     报警 ID NOD-001     重要程度 警告   报警名称 NodeExporterDown   持续时间 10 分钟   触发条件 无法搜集节点导出度量时启动   解决方案 检查 Prometheus 日志、节点导出日志及事件。\n必要时重启 Pod。       报警 ID NOD-002     重要程度 危急   报警名称 K8SNodeOutOfDisk   持续时间 即时   触发条件 节点状态为 OutOfDisk 时启动   解决方案 增加该节点的磁盘。       报警 ID NOD-003     重要程度 警告   报警名称 K8SNodeMemoryPressure   持续时间 即时   触发条件 节点状态为内存压力时启动   解决方案 增加该节点的内存。       报警 ID NOD-004     重要程度 警告   报警名称 K8SNodeDiskPressure   持续时间 即时   触发条件 节点状态为磁盘压力时启动   解决方案 删除节点的日志、未使用 dodkcer 映像、pv 备份等，确保磁盘空间。\n仍未改善时，增加该节点的磁盘。       报警 ID NOD-005     重要程度 警告   报警名称 NodeCPUUsage   持续时间 30 分钟   触发条件 节点的最近 5 分钟内平均 CPU 使用量超过 90%时启动   解决方案 增加该节点的 CPU。       报警 ID NOD-006     重要程度 警告   报警名称 NodeMemoryUsage   持续时间 30 分钟   触发条件 节点内存使用量超过 90%时启动   解决方案 增加该节点的内存。     Prometheus     报警 ID PRM-001     重要程度 警告   报警名称 PrometheusFailedReload   持续时间 10 分钟   触发条件 修改 Prometheus 设置时，设置重读操作失败则启动   解决方案 检查该 Pod 日志，纠正配置图的设置错误。     System     报警 ID CKT-001     重要程度 警告   报警名称 PvLowRequestDisk   持续时间 30 分钟   触发条件 相比 PV 请求的磁盘大小，其使用量超过 80%时启动   解决方案 增加 PV 值。但，需要重新部署服务器。       报警 ID CKT-002     重要程度 警告   报警名称 PvLowTotalDisk   持续时间 30 分钟   触发条件 相比已挂载 PV 的磁盘大小，其使用量超过 80%时启动   解决方案 检查已挂载磁盘状态，删除未使用的 PV。\n必要时增加磁盘。       报警 ID CKT-003     重要程度 警告   报警名称 PodCPULimitUsage   持续时间 30 分钟   触发条件 相比资源设定限值，CPU 使用量超过 90%时启动   解决方案 仍未改善时，修改 Deployment 的 CPU 限值       报警 ID CKT-004     重要程度 警告   报警名称 PodMemoryLimitUsage   持续时间 30 分钟   触发条件 相比资源设定限值，其内存使用量超过 90%时启动   解决方案 仍未改善时，修改部署内存限值    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.6-metering/",
	"title": "7.6 计量",
	"tags": [],
	"description": "",
	"content": " 对正在使用的各种集群，提供计量(计费)监控服务。\n计量服务仅限于适用公共云的 AWS 和 GCP。\na) 集群→选择集群→点击计量。 "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/7.cluster/7.7-registration/",
	"title": "7.7 등록정보",
	"tags": [],
	"description": "",
	"content": " 현재 클러스터에 대한 등록정보를 확인할 수 있다.\nAccount 인증정보는 보안을 위해 노출되지 않는다.\na) 클러스터 → 클러스터 선택 → 등록관리를 클릭한다. "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.1-cluster-kass-gke/",
	"title": "8.1.1.1 GKE 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " GKE 클러스터를 배포하는 방법을 알아봅니다. Google Kubernetes Engine(GKE)에 로그인 https://cloud.google.com/kubernetes-engine/ 에서 Google cloud에 로그인합니다.\n서비스 계정 만들기 1. IAM 및 관리자에서 Service Account 만들기.  서비스 계정에는 다음 역할이 필요합니다.\n project/viewer\n kubernetes-engine/admin\n service-account/user\n 역할 기반 액세스 제어(Identity and Access Management)를 사용하기 위한 필요 조건:\nrbac 생성시 사전 필수 요소 필요.\n 다음 명령어를 실행하여 Kubernetes에서 역할을 만들 수 있는 능력을 사용자에게 부여해야 합니다. [USER_ACCOUNT]는 사용자의 이메일 주소입니다.\n참조: https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control  kubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin --user [USER_ACCOUNT]  or\nkubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $(gcloud config get-value account)   서비스 계졍 만들기를 선택합니다.\n참조: 서비스 계정으로 Cloud Platform 인증\n 서비스 계정 세부 정보\n 서비스 계정 이름:\n이 서비스 계정의 표시 이름입니다.\n 만들기를 선택합니다.\n  서비스 계정 권한(선택사항)\n 역할:\n이 사용 설명서에서는 편의상 소유자 권한을 부여합니다.  Private key 만들기\n CREATE KEY 선택합니다.\n CREATE 합니다.\n서비스 계정이 생성되었으면 서비스 계정의 사용자 인증 정보가 포함된 JSON 키 파일이 컴퓨터에 다운로드됩니다. 이 키 파일은 사용자 API에 인증을 수행하도록 애플리케이션을 구성하기 위해 사용됩니다.      GKE 클러스터 만들기 참조: 클러스터 아키텍처\n1. 프로젝트 만들기 참조: https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster\n 프로젝트 만들기를 선택합니다.자세히 알아보기\n Project Name:\n프로젝트 이름을 입력합니다.\n Preject ID\n프로젝트 ID는 프로젝트의 글로벌 고유 식별자입니다. 프로젝트를 만든 후에는 프로젝트 ID를 변경할 수 없습니다.\n프로젝트를 만들 때 또는 프로젝트 ID를 만들어야 하는 API를 활성화할 때 선택한 맞춤설정된 이름입니다. 삭제된 프로젝트의 프로젝트 ID는 재사용할 수 없습니다.  Location:\n상위 조직 또는 폴더를 선택 합니다.\n    1.1 Google Kubernetes Engine API가 사용 설정되었는지 확인합니다. **GOOGLE KUBERNETES ENGINEAPI 사용 설정됨** 을 확인 합니다. ![gke-create-api-lib-1](/cocktail-manual-3.3.0-public/images/assets/ZH/3.1.1/GKE/gke-create-api-lib-1.png)   Cloud SDK가 설치되었는지 확인합니다.\n gcloud 명령줄 도구의 기본값을 설정합니다.\ngcloud 명령줄 도구에서 프로젝트 ID 및 Compute Engine 영역 옵션을 입력하는 시간을 절약하기 위해 기본값을 설정할 수 있습니다.\ngcloud config set project [PROJECT_ID] gcloud config set compute/zone us-central1-b  gcloud를 최신 버전으로 업데이트합니다.\ngcloud components update    2. VPC 네트워크 만들기  VPC 네트워크 만들기 선택\n 이름 :\nVPC에 고유 이름을 부여합니다\n 서브넷 생성:\n서브넷을 사용하면 Google Cloud 내에 자체 비공개 클라우드 토폴로지를 만들 수 있습니다. 각 지역에 서브넷을 만들려면 \u0026lsquo;자동\u0026rsquo;을 클릭하고, 서브넷을 직접 정의하려면 \u0026lsquo;맞춤설정\u0026rsquo;을 클릭하세요. 자세히 알아보기\n 이름:\n서브넷 고유 이름을 부여합니다\n Region:\n이 사용 설명서에서는 asia-northeast1(Tokyo)를 사용합니다.\n자세히 알아보기\n IP 주소 범위:\nCIDR 표기법으로 나타낸 이 서브넷의 주소 범위입니다. 표준 사설 VPC 네트워크 주소 범위(예: 10.0.0.0/9)를 사용하세요.\n자세히 알아보기\n 완료를 선택합니다.\n 위와 같은 방법으로 subnet을 추가 합니다.\n  만들기를 선택합니다.\n    3. 클러스터 템플릿 GCP 콘솔을 사용해서 새 클러스터를 만들 경우 사용 가능한 모든 클러스터 템플릿이 표시됩니다. 기본적으로 표준 템플릿이 선택됩니다.\n 다음 템플릿을 사용할 수 있습니다.\n 표준 클러스터:\n지속적 통합, 웹 제공, 백엔드용입니다. 추가 맞춤설정이 필요하거나 어떤 템플릿을 선택할지 확실하지 않은 경우에 선택하면 가장 적합합니다.\n 첫 번째 클러스터:\n덜 강력한 노드를 실행하고 자동 확장과 같은 일부 고급 기능을 사용하지 않는 작은 클러스터입니다.\n CPU 집중 애플리케이션:\n해당 노드가 표준 클러스터보다 더 강력한 다중 코어 CPU를 제공하는 클러스터입니다.\n 메모리 집중 애플리케이션:\n해당 노드가 일반적으로 강력한 다중 코어 CPU와 대용량 메모리를 제공하는 클러스터입니다.\n GPU 가속 컴퓨팅:\n기본 노드 풀이 덜 강력한 노드로 구성되었고 GPU 설정 노드 풀이 추가로 포함된 클러스터입니다. 자동 확장은 기본적으로 사용되지 않습니다.\n 고가용성:\n클러스터가 지역 클러스터로 구성되었고, 특정 지역의 각 영역에 클러스터 마스터가 제공됩니다. 자동 확장 및 유지관리 기간이 사용 설정됩니다.\n  Kubernetes 클러스터 만들기\n 표준 클러스터\n템플릿을 선택하거나 해당 작업 부하에 적합한 템플릿을 선택합니다.\n 필요한 경우 템플릿을 맞춤설정합니다. 다음 필드는 필수입니다.\n 이름:\n클러스터 이름을 선택합니다. 프로젝트 및 영역 내에서 고유해야 합니다.\n 위치 유형:\n클러스터의 모든 노드가 동일 영역에 있는지 아니면 지정된 지역의 모든 영역에 있을 수 있는지 여부입니다.\n 영역:\n위치 유형이 영역인 경우 클러스터를 만들 Compute Engine 컴퓨팅 영역입니다.\n이 사용 설명서에서는 asia-northeast1-a(Tokyo)를 사용합니다.\n 지역:\n기본적으로 클러스터는 개발자가 생성 시 지정하는 단일 컴퓨팅 영역에 클러스터 마스터와 해당 노드를 만듭니다. 지역 클러스터를 만들어서 클러스터의 가용성 및 복구성을 향상시킬 수 있습니다.\n자세히 알아보기\n  마스터 버전:\nkubernetes version을 선택합니다.\n 노드 풀:\n노드 풀은 클러스터에서 Kubernetes를 실행하는 별도의 인스턴스 그룹입니다.\n 노드 수:\n클러스터에 만들 노드 수입니다. 노드 및 리소스에 대해 사용 가능한 리소스 할당량이 있어야 합니다(예: 방화벽 경로).\n 머신 유형:\n인스턴스에 사용할 Compute Engine 머신 유형입니다. 각 머신 유형은 서로 다르게 청구됩니다. 기본 머신 유형은 n1-standard-1입니다. 머신 유형 가격 정보는 머신 유형 가격표를 참조하세요.\n 고급 수정:\n 이름:\n노드 풀 이름을 부여합니다.\n 부팅 디스크 크기(GB):\n이 사용 설명서에서는 10GB를 사용합니다.\n 보안\n 서비스 계정:\nCompute Engine default service account 를 선택 합니다.\nVM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 콘솔 메뉴의 권한을 사용하여 서비스 계정을 만들거나 기본 서비스 계정이 있으면 이 계정을 사용하세요. 자세히 알아보기  저장을 선택합니다.\n   고급옵션:\n 네트워킹\n VPC 네이티브:\nVPC 네이티브 사용 설정(별칭 IP 사용) 체크합니다.\n 네트워크:\n항목에서 생성한 VPC를 선택합니다.\n 노드 서브넷:\n항목에서 생성한 subnet을 선택합니다.\n    만들기를 선택합니다.\n  GCP 콘솔에서 클러스터를 만든 다음에는 해당 클러스터와 상호작용하도록 kubectl을 구성해야 합니다. 자세한 내용은 kubeconfig 항목 생성을 참조하세요.\n  4. 클러스터에 연결  Cloud Shell에서 실행:\n Kubernetes 클러스터 목록에서 생성된 클러스터 연결을 선택합니다.   kubectl 을 사용하려면:\n kubectl 구성이 선행되야 합니다.\nkubectl은 Kubernetes Engine에서 사용되는 클러스터 조정 시스템인 Kubernetes를 관리하기 위해 사용됩니다.\n gcloud를 사용하여 kubectl을 설치할 수 있습니다.\ngcloud components install kubectl   클러스터 연결 화면에서 명령줄을 복사 콘솔에서 실행합니다.\n 아래 명령어로 확인할 수 있습니다.\nkubectl get svc     NFS Sever 만들기 Google Compute Engine 영구 디스크를 사용하여 네트워크 파일 시스템 (NFS) 서버를 만들어 컨테이너에 마운트하는 것입니다.\n1. Create an instance  이름(Name):\n인스턴스 이름을 부여합니다.\n 지역(Region):\n지역은 리소스를 실행할 수 있는 특정 지리적 위치입니다.\n 영역(Zone):\n영역은 지역 내의 격리된 위치입니다. 영역은 사용할 수 있는 컴퓨팅 리소스와 데이터를 저장하고 사용할 위치를 결정합니다.\n 머신 유형(Machine type):\n맞춤설정을 클릭하여 코어, 메모리, CPU를 선택합니다.\n 부팅 디스크(Boot disk):\n각 인스턴스에는 부팅을 위한 디스크가 필요합니다. 이미지나 스냅샷을 선택하여 새 부팅 디스크를 생성하거나 기존 디스크를 인스턴스에 연결하세요.\n이 사용 설명서에서는 CentOS 7 사용합니다.\n ID 및 API 액세스(Identity and API access):\n생성한 서비스 계정을 선택합니다.\nVM에서 실행되는 애플리케이션은 서비스 계정을 사용하여 Google Cloud API를 호출합니다. 사용할 서비스 계정과 허용할 API 액세스 수준을 선택하세요. 자세히 알아보기\n  Management, security, disks, networking, sole tenancy 확장 섹션을 활성화 해서 아래 설정을 합니다.\n 디스크(Disks) 설정\n Add new disk를 선택 합니다.\n 노드에 필요한 디스크 사양을 설정 합니다.    네트워크(Networking) 설정\n네트워크는 인스턴스에서 액세스할 수 있는 네트워크 트래픽을 결정합니다.\n Network tags(네트워크 태그):\n네트워크 태그를 할당하여 특정 VM 인스턴스에 방화벽 규칙을 적용합니다.\n Network interfaces(네트워크 인터페이스)\n Network(네트워크):\n목록에서 VPC network에서 생성한 VPC를 선택합니다.\n Subnetwork(하위 네트워크):\n목록에서 VPC network에서 생성한 subnet을 선택합니다.\n Primary internal IP(기본 내부 IP):\n임시의 경우 인스턴스를 다시 시작해도 내부 IP가 변경되지 않지만 인스턴스를 삭제하고 다시 만들면 내부 IP가 변경됩니다.\n\u0026lsquo;임시(자동)\u0026lsquo;를 선택하여 하위 네트워크 범위의 주소를 할당하거나 \u0026lsquo;임시(커스텀)\u0026lsquo;를 선택하여 직접 입력하세요.\n인스턴스를 삭제하고 다시 만들 때 IP를 유지하려면 고정 내부 IP 주소를 선택하거나 만드세요.\n자세히 알아보기\n done(완료)**를 선택합니다.\n   Create(만들기)를 선택합니다.\n  2. 생성된 인스턴스(노드) 방화벽 설정(SSH / NFS).  인스턴스 화면에서 생성한 인스턴스의 확장 메뉴에서 View network details를 선택.\n또는 VPC network \u0026gt; Firewall rules를 선택한다.   SSH 방화벽 규칙을 추가 한다.\n Name:\n예) cocktail-test-gke-demo-storage-ssh\n Network:\n인스턴스에서 설정한 VPC를 선택합니다.\n Targets(대상):\nSpecified target tags를 선택.\n가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.\n Target tags:\nNFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.\n Source filter:\nIP ranges를 선택.\n필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.\n Source IP ranges:\n접근하고자 하는 IP ranges를 CIDR 표기법으로 추가 합니다.\n예)10.0.10.0/24 192.168.100.5\u0026frasl;32\n Protocols and ports:\n허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.\n Specified protocols and ports\n tcp: 22      NFS 방화벽 규칙을 추가 한다.\n Name:\n예) cocktail-test-gke-demo-storage-nfs\n Network:\n인스턴스에서 설정한 VPC를 선택합니다.\n Targets(대상):\nSpecified target tags를 선택.\n가상 네트워크에서 해당 인스턴스에만 방화벽 규칙이 적용됩니다.\n Target tags:\nNFS인스턴스(노드)에서 설정한 네트워크 태그를 입력합니다.\n Source filter:\nSubnets를 선택.\n필터를 설정해 특정 트래픽 소스에 규칙을 적용하세요.\n Subnets:\n이 소스 하위 네트워크의 트래픽만 허용됩니다.\n Protocols and ports:\n허용되는 프로토콜 및 포트에만 트래픽 규칙이 적용됩니다.\n Allow all 선택\n이 사용 설명서에서는 편의상 Alloww all을 선택합니다.      3. 생성된 인스턴스(노드)에 SSH 연결.  인스턴스 화면에서 생성된 인스턴스(노드)의 SSH항목을 선택 -\u0026gt; View gcloud command를 선택 한다(자동생성).\n Run IN CLOUD SHELL을 실행 한다.\n RSA key pair 가 자동생성 된다.\n아래명령으로 생성된 key pair 을 확인할 수 있다.\ncd ~/.ssh  RSA key pair GCP의 Compute Engine \u0026gt; Metadata \u0026gt; SSH Keys에 자동 등록 된다.\n아래 그림처럼 등록된 SSH Key를 확인 및 편집할 수 있다.\n    또는 RSA key pair 수동 생성 / 등록 방법.\n 아래 명령으로 RSA key pair를 생성한다.\nssh-keygen -t rsa -f ./rsa-gcp-key -C\u0026quot;\u0026lt;Your-email.com\u0026gt;\u0026quot;    생성된 RSA Key 내용을 복사 해서 Compute Engine \u0026gt; Metadata \u0026gt; SSH Keys 에 등록 한다.\ncat ./rsa-gcp-key.pub     4. 생성된 인스턴스(노드)에 NFS 서버 설치 / 구성  노드 OS 확인.\nsudo grep . /etc/*-release  노드 NFS 패키지 확인.\nsudo rpm -qa | grep nfs  노드 NFS 패키지 설치.\nsudo yum install nfs-utils nfs-utils-lib  클러스터(마스터)에서 NFS에 사용될 공유 디렉토리를 생성 한다.\nprovisioner Deployment에서 NFS_PATH에 지정된 디렉토리를 생성한다.\nsudo mkdir /storage/shared  공유 디렉토리 퍼미션 설정.\nsudo chmod -R 777 /storage  공유 디렉토리 NFS 권한을 부여하고, 동기화를 한다.\n인스턴스(노드)에서 설정한 Subnet 또는 Ip ranges를 부여한다.\n# sudo vi /etc/exports /storage/shared 10.0.10.0/24(rw,sync)  NFS service를 재시작 한다.\nsudo service nfs restart   5. 클러스터(마스터)에 NFS-Client Provisioner 구성. 참조: Kubernetes NFS-Client Provisioner\n Setup authorization:\n클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.\n kubectl apply -f 1-rbac.yaml\nNamespace, ServiceAccount, Setup authorization 적용  apiVersion: v1 kind: Namespace metadata: name: cocktail-addon --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: cocktail-addon --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;] verbs: [\u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: cocktail-addon roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: cocktail-addon roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io  Configure the NFS-Client provisioner\n kubectl apply -f 2-shared-storage-provisioner-dp.yaml\nDeployment, 다음 프로비저 배포 파일을 편집하여 NFS 서버에 대한 연결 정보를 추가해야합니다.  kind: Deployment apiVersion: apps/v1 metadata: name: shared-storage-provisioner namespace: cocktail-addon spec: replicas: 1 selector: matchLabels: acornsoft.io/provisioner-type: NFSDYNAMIC app: shared-storage-provisioner strategy: type: Recreate template: metadata: labels: app: shared-storage-provisioner acornsoft.io/provisioner-type: NFSDYNAMIC spec: serviceAccount: nfs-client-provisioner containers: - name: shared-storage-provisioner image: quay.io/external_storage/nfs-client-provisioner:v2.0.1 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME # YOUR PROVISIONER_NAME value: acornsoft.io/shared-storage-provisioner - name: NFS_SERVER # YOUR NFS SERVER HOSTNAME value: 10.0.10.8 - name: NFS_PATH value: /storage/shared resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: nfs-client-root nfs: # YOUR NFS SERVER HOSTNAME server: 10.0.10.8 path: /storage/shared  Storage class 등록\n참조: https://kubernetes.io/docs/concepts/storage/storage-classes/\n kubectl apply -f 3-single-sc.yaml\nsingle-storage : Persistent Volumes with Kubernetes on GKE  --- # single-storage : Persistent Volumes with Kubernetes on GKE apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: single-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: GCE acornsoft.io/type: SINGLE addonmanager.kubernetes.io/mode: EnsureExists kubernetes.io/cluster-service: \u0026quot;true\u0026quot; parameters: type: pd-standard provisioner: kubernetes.io/gce-pd reclaimPolicy: Delete   kubectl apply -f 4-shared-sc.yaml\nshared-storage : NFS Persistent Volumes with Kubernetes on GKE  --- # shared-storage : NFS Persistent Volumes with Kubernetes on GKE apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: shared-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: NFSDYNAMIC acornsoft.io/total-capacity: \u0026quot;100\u0026quot; acornsoft.io/type: SHARED provisioner: acornsoft.io/shared-storage-provisioner reclaimPolicy: Delete allowVolumeExpansion: true   6. 샘플 PVC 등록 / 확인.  아래 명령으로 샘플을 구성할 수 있다.\n kubectl apply -f 5-test-pod.yaml\n임시 파일 생성 pod를 생성 한다.  --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-pvc spec: storageClassName: shared-storage accessModes: - ReadWriteMany resources: requests: storage: 1Mi --- kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - \u0026quot;/bin/sh\u0026quot; args: - \u0026quot;-c\u0026quot; - \u0026quot;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026quot; volumeMounts: - name: nfs-test-pvc mountPath: \u0026quot;/mnt\u0026quot; restartPolicy: \u0026quot;Never\u0026quot; volumes: - name: nfs-test-pvc persistentVolumeClaim: claimName: nfs-pvc   kubectl get pv\n생성된 PV를 확인 한다.\n NFS 노드에 생성된 임시 파일을 확인할 수 있다.\n   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.2-cluster-kass-aks/",
	"title": "8.1.1.2 AKS 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " AKS 클러스터를 배포하는 방법을 알아봅니다. 사용 설명서 : Azure Portal을 사용하여 AKS(Azure Kubernetes Service) 클러스터 배포\nAzure에 로그인 https://portal.azure.com 에서 Azure Portal에 로그인합니다.\nAKS 클러스터 만들기 - Create a resource Azure Portal의 화면 왼쪽 상단 + Create a resource 선택 다음 \u0026gt; Kubernetes Service를 선택 합니다.\n1. Basics - 다음 옵션을 구성 합니다. Azure Kubernetes Service (AKS)는 호스팅 된 Kubernetes 환경을 관리하므로 컨테이너 오케스트레이션 전문 지식이 없어도 컨테이너 화 된 응용 프로그램을 빠르고 쉽게 배포 및 관리 할 수 있습니다.\n또한 응용 프로그램을 오프라인으로 설정하지 않고도 필요에 따라 리소스를 프로비저닝, 업그레이드 및 확장하여 진행중인 운영 및 유지 관리의 부담을 없애줍니다. Learn more about AKS\n PROJECT DETAILS\n Subscription :\nMicrosoft Azure 선택.\nAzure 서비스에 대한 엑세스 권한을 부여합니다. 자원 사용이 보고되고 서비스에 비용이 청구되는 방법이기도 합니다.\n ResourceGroup :\nmyResourceGroup 같은 Azure 리소스 그룹을 선택하거나 만듭니다.\n리소스 그룹은 동일한 수명주기, 사용 권한 및 정책을 공유하는 리소스 모음입니다.\n  CLUSTER DETAILS\n Kubernetes cluster name :\nmyAKSCluster 같은 Azure Kubernetes 서비스 클러스터 이름을 입력합니다.\n Region :\n클러스터가 배포되어야 하는 Azure 영역입니다. 이 사용 설명서에서는 Japan East를 선택 합니다..\n Kubernetes version :\n이 클러스터에 사용해야 하는 Kubernetes의 버전. 클러스터를 만든 후에는 버전을 업그레이드 할 수 있습니다.\nview the Kubernetes changelog\n DNS name prefix :\n호스팅 된 Kubernetes API 서버 FQDN과 함께 사용 할 DNS 이름 접두사.\n클러스터를 생성 한 후 컨테이너를 관리 할 때 Kubernetes API에 연결하기 위해 이를 사용 할 것입니다.\n  SCALE\n클러스터의 노드 수와 크기 입니다. Production workloads의 경우 복구를 위해 노드를 3 개 이상 권장합니다. 개발 또는 테스트 작업 부하의 경우 1개의 노드 만 필요합니다.\n클러스터를 만든 후에 노드 크기를 변경할 수는 없지만 만든 후에는 클러스터의 노드 수 를 변경할 수 있습니다.\nLearn more about scaling in Azure Kubernetes Service\n Node size :\n클러스터에서 노드를 형성 할 가상 시스템의 크기. 클러스터를 만든 후에는 변경할 수 없습니다.\n이 사용 설명서에서는 Standard DS2 v2 : 2 vcpus, 7 GB memory 를 사용 합니다.\n Node count :\n클러스터와 함께 생성되어야하는 노드의 수. 나중에 클러스터의 노드 수 를 조정할 수 있습니다\n이 사용 설명서에서는 노드 수를 1 로 설정합니다.\n Virtual nodes (preview) :\n가상 노드를 사용하면 서버가없는 Azure 컨테이너 인스턴스가 지원하는 노드에 컨테이너를 배포하거나 버스트 아웃 할 수 있습니다.\n이렇게하면 정의 된 클러스터 크기를 초과하는 빠른 버스트 확장 옵션을 제공 할 수 있습니다. Learn more\n   2. Authentication - 다음 옵션을 구성 합니다. 클러스터 인프라 서비스 원칙은 Kubernetes 클러스터에서 클러스터에 연결된 클라우드 리소스를 관리하는 데 사용됩니다. Learn more\nKubernetes 인증 및 권한 부여 는 Kubernetes 클러스터에서 클러스터에 대한 사용자 액세스를 제어하고 사용자가 인증을 받으면 수행 할 수있는 작업을 제어하는 ​​데 사용됩니다. Learn more\n CLUSTER INFRASTRUCTURE\n Service principal :\n기존 서비스 주체의 클라이언트 ID입니다.\n이 사용 설명서 에서는 (new) default service principal 를 선택하여 자동 생성 합니다.\n클러스터가 생성되 후 Properties 화면 INFRASTRUCTURE RESOURCE GROUP 에서 확인 할 수 있습니다.\n  KUBERNETES AUTHENTICATION AND AUTHORIZATION\n클러스터 리소스에 대한 세분화 된 제어를 제공하는 Kubernetes 역할 기반 액세스 제어를 사용합니다.\n Enable RBAC :\n이러한 컨트롤을 사용하면 AKS 클러스터에 배포된 Kubernetes 리소스에 대한 액세스를 정밀하게 제어할 수 있습니다.\n이 사용 설명서에서는 Kubernetes RBAC(역할 기반 액세스 제어)에 대한 옵션을 사용하도록 설정합니다.\n   3. Networking - 다음 옵션을 구성 합니다. HTTP 응용 프로그램 라우팅을 활성화하고 \u0026ldquo;기본\u0026rdquo;또는 \u0026ldquo;고급\u0026rdquo;의 두 가지 네트워킹 옵션 중에서 선택할 수 있습니다.\n* Basic 네트워킹은 기본값을 사용하여 클러스터에 새로운 VNet을 생성합니다.\n* Advanced 네트워킹을 통해 클러스터는 사용자 정의 가능한 주소로 새 VNet 또는 기존 VNet을 사용할 수 있습니다.\n응용 프로그램 포드는 VNet에 직접 연결되어있어 VNet 기능과의 기본 통합이 가능합니다.\nLearn more about networking in Azure Kubernetes Service\n HTTP application routing :\nHTTP application routing 솔루션을 사용하면 application endpoints 대해 공개적으로 액세스 할 수있는 DNS 이름을 만들어 클러스터에 배포 된 응용 프로그램에 쉽게 액세스 할 수 있습니다. Learn more about HTTP application routing\n이 사용 설명서에서는 NO 를 선택 합니다.\n Network configuration :\n기본 VNet과 함께 kubenet을 사용하는 Basic 네트워크 구성을 선택하거나\nAzure CNI를 사용하여 VNet을 사용자 지정하는 옵션과 함께 Advanced 구성을 선택하십시오.\n이 사용 설명서에서는 Basic 을 선택 합니다.\n  4. Monitoring - 다음 옵션을 구성 합니다. Azure Kubernetes Service를 사용하면 각 노드에 대한 CPU 및 메모리 사용량 메트릭을 얻을 수 있습니다.\n또한 컨테이너 모니터링 기능을 활성화하고 전체 Kubernetes 클러스터의 성능 및 상태에 대한 통찰력을 얻을 수 있습니다.\n처리 된 데이터의 양과 데이터 보존 설정에 따라 요금이 청구됩니다.\nLearn more about container performance and health monitoring\nLearn more about pricing\n AZURE MONITOR\n Enabe container monitoring :\n이 사용 설명서에서는 Yes를 선택 합니다.\n Log Analytics workspace :\n모니터링 데이터를 저장할 로그 분석 워크 스페이스 선택.\n이 사용 설명서에서는 Create new 를 선택해서 Japan East Region을 선택하고 생성 합니다.\n   5. Tags - 다음 옵션을 구성 합니다. 태그는 이름 / 값 쌍으로, 리소스를 분류하고 동일한 태그를 여러 자원 및 자원 그룹에 적용하여 통합 청구서를 볼 수 있습니다. Learn more\n태그를 만들고 다른 탭에서 리소스 설정을 변경하면 태그가 자동으로 업데이트됩니다.\n이 사용 설명서에서는 만들지 않습니다.\n6. Review + create - 검토 + 만들기를 구성 합니다.  검토 + 만들기를 선택하고, 완료되면 만들기를 선택 합니다.\n   AKS 클러스터를 만들고 사용 준비를 마칠 때까지 몇 분 정도 걸립니다.\n  7. 생성된 Resource groups 을 학인 완료되면 리소스 그룹에서 생성한 이름으로 리소스 그룹이 생성 됩니다. 생성된 클러스터를 확인 할 수 있습니다.\n 왼쪽 Resource groups 를 선택 후. 생성한 클러스트의 리소스 그룹을 선택 합니다.\n   생성한 클러스터의 Vitual machine 을 선택 합니다.  참고 사항 : Tags 를 변경 할 수 있습니다.    생성한 클러스터의 Vitual machine의 모니터링을 확인 할 수 있습니다.  클러스터에 연결 1. Kubernetes 클러스터를 관리하려면 - Cloud Shell 사용  Azure Portal 오른쪽 상단에 있는 \u0026gt;_ Cloud Shell을 엽니다.\n kubectl Kubernetes 명령줄 클라이언트를 사용합니다. kubectl 클라이언트가 Azure Cloud Shell에 사전 설치됩니다.\n  2. Kubernetes 클러스터에 연결하도록 kubectl을 구성하려면 - Cloud Shell 사용  az aks get-credentials 명령을 사용하여 kubectl 을 구성 합니다.\n이 명령은 자격 증명을 다운로드하고 Kubernetes CLI가 해당 자격 증명을 사용하도록 구성 합니다.\n다음 예제는 cocktail 이라는 리소스 그룹에서 cocktail-test-aks 클러스터의 자격 증명을 가져옵니다.\naz aks get-credentials --resource-group cocktail --name cocktail-test-aks  클러스터에 대한 연결을 확인하려면 kubectl get 명령을 사용하여 클러스터 노드의 목록(Computer name) 을 반환합니다.\nkubectl get nodes   AZURE Storage Accounts 만들기 Storage accounts 1. 왼쪽 메뉴 또는 Home 화면에서 Storage accounts 를 선택 합니다. 2. Storage accounts +Add 를 선택 합니다. Create storage account - 만들기 1. Basics - 다음 옵션을 구성 합니다. Azure 저장소는 가용성, 보안 성, 내구성, 확장 성 및 중복성이 뛰어난 클라우드 저장소를 제공하는 Microsoft 관리 서비스입니다.\nAzure 저장소에는 Azure Blob (개체), Azure Data Lake 저장소 Gen2, Azure 파일, Azure 대기열 및 Azure 테이블이 포함됩니다.\n스토리지 계정 비용은 아래에서 선택한 사용법 및 옵션에 따라 다릅니다. Learn more\n PROJECT DETAILS\n배포 된 리소스 및 비용을 관리하려면 Microsoft Azure 를 선택하십시오.\n폴더와 같은 리소스 그룹을 사용하여 모든 리소스를 구성하고 관리하십시오.\n Subscription :\nMicrosoft Azure 를 선택.\n Resource group :\n생성한 클러스터의 INFRASTRUCTURE RESOURCE GROUP 을 선택 합니다.\n  INSTANCE DETAILS\n기본 배포 모델은 최신 Azure 기능을 지원하는 Resource Manager입니다.\n대신 클래식 배포 모델을 사용하여 배포하도록 선택할 수 있습니다.\n Storage account name :\n이 이름은 Azure의 모든 기존 스토리지 계정 이름에서 고유해야합니다. 3 ~ 24 자 여야하며 소문자와 숫자 만 사용할 수 있습니다.\n Location :\n연결 할 클러스터와 같은 위치를 선택 합니다(권장).\n Performance :\nStandard(표준 스토리지) 계정은 마그네틱 드라이브로 백업되며 GB 당 최저 비용을 제공합니다. 대용량 저장 장치가 필요한 응용 프로그램이나 데이터 액세스 빈도가 낮은 응용 프로그램에 가장 적합합니다.\nPremium(프리미엄 스토리지) 계정은 솔리드 스테이트 드라이브를 기반으로하며 일관되고 낮은 대기 시간의 성능을 제공합니다. Azure 가상 머신 디스크에서만 사용할 수 있으며 데이터베이스와 같은 I / O 집약적 인 애플리케이션에 가장 적합합니다. 또한 모든 디스크에 Premium 스토리지를 사용하는 가상 시스템은 가용성 세트를 벗어나 실행하는 경우에도 99.9 % SLA를 사용할 수 있습니다. Learn more\n이 설정은 스토리지 계정을 만든 후에는 변경할 수 없습니다.\n Account kind :\n범용 V1, 범용V2(권장) 스토리지 계정은 통합 계정에서 blobs, 파일, 테이블 및 대기열에 대한 스토리지를 제공합니다.\nBlob 저장소 계정은 BLOB 데이터를 저장하고 액세스 계층을 선택하는 데 특화되어 있어 계정의 엑세스 빈도를 지정 할 수 있습니다.\n스토리지 요구사항과 일치하고 비용을 최저화 하는 계층을 선택하십시오. learn more\n Replication :\nAzure 스토리지 계정의 데이터는 항상 내구성과 고 가용성을 보장하기 위해 복제됩니다.\n내구성 요구 사항에 맞는 복제 전략을 선택하십시오.\n스토리지 계정을 만든 후에는 일부 설정을 별경 할 수 없습니다. learn more\n Access tier (default) :\n계정 액세스 계층은 명시 적으로 설정된 계층이없는 모든 BLOB에서 유추되는 기본 계층입니다.\nHot Access Tier는 자주 액세스하는 데이터에 이상적이며\nCool Access Tier는 자주 액세스하지 않는 데이터에 이상적입니다.\n아카이브 액세스 계층은 계정이 아닌 BLOB 수준에서만 설정할 수 있습니다. learn more\n   2. Advanced - 다음 옵션을 구성 합니다.  SECURITY\n Secure transfer required :\n보안 전송 옵션은 보안 연결을 통해 저장소 계정에 대한 요청 만 허용하여 저장소 계정의 보안을 강화합니다.\n예를 들어 저장소 계정에 액세스하기 위해 REST API를 호출 할 때는 HTTP를 사용하여 연결해야합니다.\nEnabled(보안 전송 필요)가 사용 설정된 경우 HTTP를 사용하는 모든 요청이 거부됩니다.\nAzure 파일 서비스를 사용하는 경우 SMB 2.1, 암호화없는 SMB 3.0 및 Linux SMB 클라이언트의 일부 버전을 비롯한 암호화없는 연결은 실패합니다.\nAzure 저장소는 사용자 정의 도메인 이름에 대한 HTTP를 지원하지 않으므로 사용자 정의 도메인 이름을 사용할 때는이 옵션이 적용되지 않습니다.\n이 사용 설명서에서는 Enabled 선택 합니다.  VIRTUAL NETWORKS\n Allow access from :\n모든 네트워크에서이 저장소 계정에 액세스 할 수 있습니다. Learn more\n이 사용 설명서에서는 All networks 선택 합니다.   DATA LAKE STORAGE GEN2 (PREVIEW)\n Hierarchical namespace :\nADLS Gen2 계층 적 네임 스페이스는 대규모 데이터 분석 작업을 가속화하고 파일 수준 ACL (액세스 제어 목록)을 활성화합니다. Learn more\n이 사용 설명서에서는 Disabled 선택 합니다.   3. Tags - 다음 옵션을 구성 합니다. 태그는 이름 / 값 쌍으로, 리소스를 분류하고 동일한 태그를 여러 자원 및 자원 그룹에 적용하여 통합 청구서를 볼 수 있습니다. Learn more\n태그를 만들고 다른 탭에서 리소스 설정을 변경하면 태그가 자동으로 업데이트됩니다.\n4. Review + create - 검토 + 만들기를 구성 합니다.  Review + create를 선택하고, 완료되면 Create를 선택 합니다.  4. 생성된 Storage Account 확인  완료된 화면 에서 Go to resource를 선택 하거나 , Resource groups 에서 추가되 Storage account를 확인 할 수 있습니다.  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.3-cluster-kass-eks/",
	"title": "8.1.1.3 EKS 클러스터 만들기",
	"tags": [],
	"description": "",
	"content": " 8.1.1.3 EKS 클러스터 만들기 Quick Start EKS 클러스터를 배포하는 방법. Kubernetes 용 Amazon Elastic Container Service (Amazon EKS)를 사용하면 AWS의 Kubernetes를 사용하여 컨테이너 화 된 응용 프로그램을 쉽게 배포, 관리 및 확장 할 수 있다.\n참조 : Amazon Elastic Container Service for Kubernetes(EKS) 클러스터 배포\nAmazon Eks에 로그인  Amazon Eks에 로그인.  Amazon EKS 필수 조건  Amazon EKS 클러스터를 생성하기 전에 Kubernetes가 AWS 리소스 생성 시 수임할 수 있는 IAM 역할을 생성해야 한다.\n AWS 관리자 권한이 없다면 필요한 역할 / 정책 / 권한 을 요청 해야 합니다.\n참조 : EKS 관리 권한 / 정책\n  AWS EKS 클러스터 만들기 Kubernetes 용 Amazon Elastic Container Service (Amazon EKS)는 AWS에서 Kubernetes를 쉽게 실행할 수 있도록 해주는 관리 형 서비스입니다.\n1. AWS 리소스 생성 시 수임할 수 있는 IAM 역할을 생성 합니다. 관리자 권한 또는 해당 IAM 권한이 필요 합니다.\n [Roles(역할)]를 선택한 다음 [Create role]을 선택합니다.\n 서비스 목록에서 EKS를 선택합니다.\n 사용 사례에 대해 Allows Amazon EKS to manage your clusters on your behalf(EKS에서 사용자를 대신하여 클러스터를 관리하도록 허용)를 선택합니다.\n  [Next: Permissions(다음: 권한)]를 선택합니다.\n AmazonEKSClusterPolicy :\n이 정책은 Kubernetes에게 사용자를 대신하여 리소스를 관리하는 데 필요한 권한을 제공합니다.\nKubernetes는 인스턴스, 보안 그룹 및 탄력적인 네트워크 인터페이스를 포함하되 이에 국한되지 않는 EC2 리소스에 대한 식별 정보를 배치하는 데 Ec2 : CreateTags 권한이 필요합니다.\n AmazonEKSServicePolicy :\n이 정책에 따라 Kubernetes 용 Amazon Elastic Container Service는 EKS 클러스터를 운영하는 데 필요한 리소스를 만들고 관리 할 수 있습니다.\n  [Next: Tags(다음: 태그)]를 선택합니다.\nIAM 태그는 사용자 역할에 추가할 수 있는 키-값 페어입니다.\n태그는 이메일 주소와 같은 사용자 정보를 포함하거나 직책과 같은 내용일 수 있습니다. 태그를 사용하여 이 역할에 대한 액세스를 구성, 추적 또는 제어할 수 있습니다. Learn more\n [Next: Review(다음: 검토)]를 선택합니다.\n생성하기 전에 아래에 필요한 정보를 입력하고 이 역할을 검토하십시오.\n Role name(역할 이름)에서 역할에 대한 고유 이름(예: eksServiceRole)을 입력합니다.\n  Create role(역할 생성 만들기)을 선택합니다.\n 만들어진 역할을 확인 합니다.\n [Roles(역할)]를 선택 :\n목록에서 새로 생성한 Role(역할)을 확인할 수 있습니다.   2. Amazon EKS 클러스터 VPC 생성 - CloudFormation 콘솔 사용해서 생성 참조 : AWS 사용 설명서를 참조.\n https://console.aws.amazon.com/cloudformation에서 AWS CloudFormation 콘솔에서 제공 되는 템플릿을 사용해서 생성.\n 상단 리전 모음에서 Amazon EKS를 지원하는 리전을 선택.\n참고\n현재 다음 리전에서 Amazon EKS를 사용할 수 있습니다. (한국어 사용 설명서는 아직 반영이 안되어 있다.)\n US West (Oregon) (us-west-2)\n US East (N. Virginia) (us-east-1)\n US East (Ohio) (us-east-2)\n EU (Frankfurt) (eu-central-1)\n EU (Stockholm) (eu-north-1)\n EU (Ireland) (eu-west-1)\n Asia Pacific (Tokyo) (ap-northeast-1)\n Asia Pacific (Seoul) (ap-northeast-2)\n Asia Pacific (Singapore) (ap-southeast-1)\n Asia Pacific (Sydney) (ap-southeast-2)\n  [Create stack]을 선택.\n 템플릿 선택에서 Amazon S3 템플릿 URL 지정을 선택.\nhttps://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml  주의 : 2019-01-09 이전 날짜의 템플릿파일을 서울리전에서 사용시 오류가 있다. (AZ가 없는 관계로)\n 스택 이름: AWS CloudFormation 스택에 대한 스택 이름을 선택합니다. 예를 들어 eks-vpc로 사용할 수 있다.\n VpcBlock: VPC에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet01Block: 서브넷 1에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet02Block: 서브넷 2에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n Subnet03Block: 서브넷 3에 대한 CIDR 범위를 선택합니다. 기본값을 유지할 수 있다.\n  (선택 사항) 옵션 페이지에서 스택 리소스에 태그를 지정합니다. [Next]를 선택.\n [Review] 페이지에서 [Create]을 선택합니다.\n 스택이 생성된 후 콘솔에서 이를 선택하고 출력을 선택합니다.\n 생성된 보안 그룹에 대한 SecurityGroups 값을 기록합니다. 이 값은 EKS 클러스터를 생성할 때 필요합니다. 이 보안 그룹은 서브넷에 생성된 계정 간 탄력적 네트워크 인터페이스에 적용되고, 이를 통해 Amazon EKS 제어 플레인이 작업자 노드와 통신할 수 있습니다.\n 생성된 VPC의 VpcId를 기록합니다. 작업자 노드 그룹 템플릿을 시작할 때 필요합니다.\n 생성된 서브넷에 대한 SubnetIds를 기록합니다. EKS 클러스터를 생성할 때 필요합니다. 작업자 노드가 시작되는 서브넷입니다.\n  3. Amazon EKS에 대한 kubectl을 설치합니다. Amazon EKS는 IAM을 사용하여 Kubernetes용 AWS IAM Authenticator를 통해 Kubernetes 클러스터에 인증을 제공합니다.\nKubernetes 버전 1.10부터 Kubernetes용 AWS IAM Authenticator를 설치하고 인증에 사용할 kubectl 구성 파일을 사용하여 Amazon EKS를 사용할 stock kubectl 클라이언트를 구성할 수 있습니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\n참조: kubectl 설치 : https://kubernetes.io/docs/tasks/tools/install-kubectl/\n Kubernetes 용 AWS IAM Authenticator는 AWS에서 Kubernetes 설치 프로그램을 작성하는 경우 부트 스트랩 프로세스를 간소화 할 수 있습니다.\n 새로 설치 한 클러스터에서 초기 admin 자격 증명을 어떻게 든 안전하게 훔쳐 낼 필요가 없습니다.\n 대신 클러스터 구축시 전용 KubernetesAdmin 역할을 만들고 Authenticator를 설정하여 클러스터 관리자 로그인을 허용 할 수 있습니다.\n  3-1. Amazon EKS용 aws-iam-authenticator를 설치합니다. Kubernetes 클러스터에 인증하기 위해 AWS IAM 자격 증명을 사용하는 도구 입니다. Kubernetes 용 AWS IAM Authenticator를 사용하면 Kubernetes 액세스에 대한 별도의 자격 증명을 관리하지 않아도됩니다.\n aws-iam-authenticator 바이너리를 다운로드 및 설치합니다.\nAmazon EKS는 사용할 수 있는 aws-iam-authenticator 바이너리를 판매합니다.\n또는 go get 을 사용하여 GitHub의 Kubernetes용 AWS IAM Authenticator 프로젝트에서 다른 운영 체제용 바이너리를 가져올 수 있습니다.\n 1. aws-iam-authenticator 바이너리를 다운로드 및 설치하려면 다음과 같이 합니다.\n Amazon S3에서 aws-iam-authenticator 바이너리를 다운로드합니다.\n Linux: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/linux/amd64/aws-iam-authenticator\n MacOS: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/darwin/amd64/aws-iam-authenticator\n Windows: https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/windows/amd64/aws-iam-authenticator.exe\n아래 명령을 사용하여 바이너리를 다운로드하고 플랫폼에 맞는 올바른 URL로 교체합니다. 아래 예는 MacOS 클라이언트에 해당됩니다.\ncurl -o aws-iam-authenticator https://amazon-eks.s3-us-west-2.amazonaws.com/1.12.7/2019-03-27/bin/darwin/amd64/aws-iam-authenticator  바이너리에 실행 권한을 적용합니다.\nchmod +x ./aws-iam-authenticator  바이너리를 $PATH의 폴더에 복사합니다. $HOME/bin/aws-iam-authenticator를 생성하고 $PATH가 $HOME/bin로 시작하는 것이 좋습니다.\ncp ./aws-iam-authenticator $HOME/bin/aws-iam-authenticator \u0026amp;\u0026amp; export PATH=$HOME/bin:$PATH  PATH 환경 변수에 $HOME/bin을 추가합니다.\n MacOS의 Bash 셸의 경우:\necho 'export PATH=$HOME/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile  Linux의 Bash 셸의 경우:\necho 'export PATH=$HOME/bin:$PATH' \u0026gt;\u0026gt; ~/.bashrc   aws-iam-authenticator 바이너리가 작동하는지 테스트합니다.\naws-iam-authenticator help  \u0026lt;!\u0026ndash;\n  2. 또는 go get을 사용하여 GitHub에서 aws-iam-authenticator 바이너리를 설치합니다.\n go가 이미 설치되지 않은 경우 운영 체제에 대해 Go 프로그래밍 언어를 설치합니다. 자세한 내용은 Go 문서의 Install the Go tools를 참조하십시오.\n go get을 사용하여 aws-iam-authenticator 바이너리를 설치합니다.\ngo get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator  PATH 환경 변수에 $HOME/go/bin을 추가합니다.\n MacOS의 Bash 셸의 경우:\nexport PATH=$HOME/go/bin:$PATH \u0026amp;\u0026amp; echo 'export PATH=$HOME/go/bin:$PATH' \u0026gt;\u0026gt; ~/.bash_profile  Linux의 Bash 셸의 경우:\nexport PATH=$HOME/go/bin:$PATH \u0026amp;\u0026amp; echo 'export PATH=$HOME/go/bin:$PATH' \u0026gt;\u0026gt; ~/.bashrc   aws-iam-authenticator 바이너리가 작동하는지 테스트합니다.\naws-iam-authenticator help  \u0026ndash;\u0026gt;\n   3-2. (선택 사항) 최신 AWS CLI 다운로드 및 설치 합니다. AWS CLI가 Amazon EKS 사용을 명시적으로 요구하지는 않지만, update-kubeconfig 명령을 사용하면 kubeconfig 생성 프로세스가 크게 간소화됩니다. AWS CLI에서 Amazon EKS를 사용하려면 1.16.18 버전 이상의 AWS CLI가 설치되어 있어야 합니다.\nAWS CLI를 설치 또는 업그레이드하려면 AWS Command Line Interface 사용 설명서의 AWS 명령줄 인터페이스 설치를 참조하십시오.\n AWS CLI 버전은 다음 명령을 통해 확인할 수 있습니다.\naws --version   4. 1단계: Amazon EKS 클러스터 생성 이제 Amazon EKS 클러스터를 생성할 수 있습니다.\n참조 : Amazon EKS 클러스터 생성 사용 설명서를 참고 합니다.\n 사전 요구 사항은 다음과 같습니다.\n Amazon EKS 클러스터의 요건을 충족하는 VPC 및 전용 보안 그룹을 생성했습니다.\n자세한 내용은 클러스터 VPC 고려 사항 및 클러스터 보안 그룹 고려 사항 단원을 참조하십시오\n Amazon EKS 서비스 역할을 생성하여 클러스터에 적용했습니다. Amazon EKS 서비스 IAM 역할 가이드를 참조 하시기 바랍니다.\n    AWS EKS 콘솔을 사용하여 클러스터를 생성 합니다.\n 1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n 2. [Create cluster]를 선택합니다.\n 참고\nIAM 사용자에게 관리 권한이 없는 경우 해당 사용자가 Amazon EKS API 작업을 호출하는 권한을 명시적으로 추가해야 합니다.\n자세한 내용은 Amazon EKS IAM 정책 만들기 단원을 참조하십시오.  3. 클러스터 생성 페이지에서 다음 필드를 입력한 다음 생성을 선택합니다.\n Create cluster\n Cluster name :\nAmazon EKS 클러스터에 대해 고유한 이름입니다.\n Kubernetes version :\n클러스터에 대해 사용할 Kubernetes 버전. 기본적으로 최신 버전이 선택됩니다.\n Role name :\nAmazon EKS 서비스 역할 생성 사용을 통해 생성한 IAM 역할을 선택합니다.\n  Networking\n VPC :\nAmazon EKS 클러스터 VPC 생성 사용을 통해 생성한 VPC를 선택합니다.\n드롭다운 목록에서 VPC의 이름을 찾을 수 있습니다.\n Subnets :\n기본적으로 위 VPC에서 사용 가능한 서브넷이 사전 선택됩니다.\n Security groups :\nAmazon EKS 클러스터 VPC 생성 사용을 통해 생성된 보안 그룹(SecurityGroups) 값을 선택 합니다.\n 중요\n작업자 노드 AWS CloudFormation 템플릿이 여기서 사용자가 지정하는 보안 그룹을 수정하므로 클러스터 제어 플레인에 대한 전용 보안 그룹을 사용하는 것이 바람직합니다. 다른 리소스와 공유할 경우 이러한 리소스를 방해하거나 막을 수 있습니다.   정보 입력이 완료 되면 생성(Create)을 선택합니다.\n  4. Clusters 화면에서 새로 생성된 클러스터를 선택하고 클러스터 정보를 확인할 수 있습니다.\n 5. Status(상태) 필드는 클러스터 프로비저닝 프로세스가 완료될 때까지 CREATING(생성 중)으로 표시됩니다.\n클러스터 프로비저닝이 완료될 때(보통 10분 이내) API server endpoint(API 서버 엔드포인트) 및 인증 기관 값을 기록합니다. 이 값은 kubectl 구성에 사용됩니다.\n 6. 클러스터를 생성했으므로 이제 Amazon EKS용 kubectl 구성 및 Amazon EKS에 대한 kubeconfig 생성의 절차를 따라 새 클러스터와의 통신을 활성화합니다.\n   4-1. Amazon EKS용 kubectl 구성 AWS CLI로 kubeconfig를 생성하려면 Amazon EKS에 대한 kubeconfig 생성을 참조 하세요.\n 기본 AWS 자격 증명 공급자 체인을 사용 하려면 :\nAWS IAM 권한이 있는 관리자에게 사용자 보안 자격 증명(액세스 키)을 요청 합니다.\n AWS 자격 증명 공급자 체인 만들기\n AWS IAM service 콘솔에서 사용자(Users)를 선택합니다.\n 보안 자격 증명(Security credentials) 섹션 에서 액세스 키를 생성합니다.\n   ~/.aws/credentials 파일 안에 해당 키 값을 입력 합니다.\n 윈도우 경우 사용자 디렉토리 안에 .aws\\redentials 파일이 있습니다.  [default] aws_access_key_id= aws_secret_access_key=   또는 AWS CLI를 사용해서 클러스터를 생성한 계정의 IAM유저의 보안자격증명을 설정합니다.  # aws configure AWS Access Key ID [None]: AKIAIHE#########FELA AWS Secret Access Key [None]: uqm8MSDK######################Ddz29PcAz5 Default region name [ap-northeast-2]: Default output format [None]: # aws sts get-caller-identity { “Account”: “4952894#####”, “UserId”: “AIDAJU5LO##########LQ”, “Arn”: “arn:aws:iam::4952894#####:user/[username]” }   AWS CLI로 kubeconfig를 생성하려면 :\n# aws eks --region region update-kubeconfig --name cluster_name Updated context arn:aws:eks:ap-northeast-2:235896307296:cluster/cocktail-test-eks-cluster in C:\\Users\\shapj\\.kube\\config # kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 1m   5. 2단계: Amazon EKS Work node 시작 및 구성 Amazon EKS work node는 AWS 계정에서 실행되고, 클러스터 API 서버 엔드포인트를 통해 Cluster Control Plane에 연결합니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/clusters.html\n 중요\nAmazon EKS Work node는 표준 Amazon EC2 인스턴스이고, 일반 Amazon EC2 온디맨드 인스턴스 가격을 기반으로 비용이 청구됩니다.\n자세한 내용은 Amazon EC2 요금을 참조하십시오.\n 이 주제의 사전 요구 사항은 다음과 같습니다.\n Amazon EKS 클러스터의 요건을 충족하는 VPC 및 보안 그룹을 생성했습니다.\n Amazon EKS 클러스터를 생성하고 위 VPC 및 보안 그룹에 사용한다고 지정했습니다.\n 시작 이후 SSH를 사용하여 Work node에 연결하는 데 사용할 수 있는 Amazon EC2 SSH 키 페어가 생성되어 있어야 합니다.\n참조 : Amazon EC2를 사용해 키 페어 만들기\n   5-1. 템플릿을 사용해서 Work node를 시작하려면  클러스터 상태가 ACTIVE가 되기를 기다립니다.\n※ 클러스터가 활성화되기 전에 Work node를 시작하면 Work node가 클러스터에 등록되지 않고 Work node를 다시 시작해야 한다.\n https://console.aws.amazon.com/cloudformation 에서 AWS CloudFormation 콘솔.\n Amazon EKS를 생성할 리전을 선택한다.\n이 사용 설명서에서는 서울 리전을 사용한다. \u0026lt;!\u0026ndash; 참고\n현재 다음 리전에서 Amazon EKS를 사용할 수 있습니다. (한국어 사용 설명서는 아직 반영이 안되어 있습니다.)\n US West (Oregon) (us-west-2)\n US East (N. Virginia) (us-east-1)\n US East (Ohio) (us-east-2)\n EU (Frankfurt) (eu-central-1)\n EU (Stockholm) (eu-north-1)\n EU (Ireland) (eu-west-1)\n Asia Pacific (Tokyo) (ap-northeast-1)\n Asia Pacific (Seoul) (ap-northeast-2)\n Asia Pacific (Singapore) (ap-southeast-1)\n Asia Pacific (Sydney) (ap-southeast-2) \u0026ndash;\u0026gt;\n  [Create stack]을 선택한다.\n 템플릿 선택에서 Amazon S3 템플릿 URL 지정을 선택한다.\n 텍스트 영역에 다음 URL을 붙여넣고 다음을 선택한다.\nhttps://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml  Specify Details(세부 정보 지정) 페이지에서 다음 파라미터를 입력하고 다음을 선택한다.\n 스택 이름 :\nAWS CloudFormation 스택에 대한 스택 이름을 선택합니다. 예를 들어 -worker-nodes로 사용할 수 있습니다.\n ClusterName :\nAmazon EKS 클러스터 생성 시 사용할 이름을 입력합니다.\n중요 : 이 이름은 Amazon EKS 클러스터 이름과 정확하게 일치해야 합니다. 그렇지 않은 경우 Work node가 조인할 수 없습니다.\n ClusterControlPlaneSecurityGroup :\nAmazon EKS 클러스터를 생성할 때 사용한 보안 그룹 또는 그룹을 입력합니다. 이 AWS CloudFormation 템플릿은 지정된 클러스터 제어 플레인 보안 그룹의 트래픽을 허용하는 Work node 보안 그룹을 생성합니다.\n NodeGroupName :\n노드 그룹의 이름을 입력합니다. 이 이름은 나중에 Work node에 대해 생성된 Auto Scaling 노드 그룹을 식별하는 데 사용할 수 있습니다.\n NodeAutoScalingGroupMinSize :\nAuto Scaling 그룹이 축소할 수 있는 Work node의 최소 노드 수를 입력합니다.\n NodeAutoScalingGroupDesiredCapacity : 스택을 생성할 때 조정할 원하는 노드 수를 입력합니다.\n NodeAutoScalingGroupMaxSize :\nAuto Scaling 그룹이 확장할 수 있는 Work node의 최대 노드 수를 입력합니다.\n NodeInstanceType :\nWork node에 대한 인스턴스 유형을 선택합니다.\n NodeImageId :\n리전에 대한 현재 Amazon EKS Work node AMI ID를 입력합니다.\n최신 Amazon EKS 최적화 AMI(GPU 지원 유무는 상이)의 AMI ID는 다음 표에 나와 있습니다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html\nKubernetes version 1.12.7\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     Asia Pacific (Tokyo) (ap-northeast-1) ami-0bfedee6a7845c26d ami-08e41cc84f4b3f27f   Asia Pacific (Seoul) (ap-northeast-2) ami-0a904348b703e620c ami-0c43b885e33fdc29e   Asia Pacific (Sydney) (ap-southeast-2) ami-0f0121e9e64ebd3dc ami-07079cd9ff1b312da    Kubernetes version 1.11\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     US West (Oregon) (us-west-2) ami-0a2abab4107669c1b ami-0c9e5e2d8caa9fb5e   US East (N. Virginia) (us-east-1) ami-0c24db5df6badc35a ami-0ff0241c02b279f50   US East (Ohio) (us-east-2) ami-0c2e8d28b1f854c68 ami-006a12f54eaafc2b1   EU (Frankfurt) (eu-central-1) ami-010caa98bae9a09e2 ami-0d6f0554fd4743a9d   EU (Stockholm) (eu-north-1) ami-06ee67302ab7cf838 ami-0b159b75   EU (Ireland) (eu-west-1) ami-01e08d22b9439c15a ami-097978e7acde1fd7c   Asia Pacific (Tokyo) (ap-northeast-1) ami-0f0e8066383e7a2cb ami-036b3969c5eb8d3cf   Asia Pacific (Seoul) (ap-northeast-2) ami-0b7baa90de70f683f ami-0b7f163f7194396f7   Asia Pacific (Singapore) (ap-southeast-1) ami-019966ed970c18502 ami-093f742654a955ee6   Asia Pacific (Sydney) (ap-southeast-2) ami-06ade0abbd8eca425 ami-05e09575123ff498b    Kubernetes version 1.10\n   리전 Amazon EKS 최적화 AMI (GPU 지원 포함)     US West (Oregon) (us-west-2) ami-09e1df3bad220af0b ami-0ebf0561e61a2be02   US East (N. Virginia) (us-east-1) ami-04358410d28eaab63 ami-0131c0ca222183def   US East (Ohio) (us-east-2) ami-0b779e8ab57655b4b ami-0abfb3be33c196cbf   EU (Frankfurt) (eu-central-1) ami-08eb700778f03ea94 ami-000622b1016d2a5bf   EU (Stockholm) (eu-north-1) ami-068b8a1efffd30eda ami-cc149ab2   EU (Ireland) (eu-west-1) ami-0de10c614955da932 ami-0dafd3a1dc43781f7   Asia Pacific (Tokyo) (ap-northeast-1) ami-06398bdd37d76571d ami-0afc9d14b2fe11ad9   Asia Pacific (Seoul) (ap-northeast-2) ami-08a87e0a7c32fa649 ami-0d75b9ab57bfc8c9a   Asia Pacific (Singapore) (ap-southeast-1) ami-0ac3510e44b5bf8ef ami-0ecce0670cb66d17b   Asia Pacific (Sydney) (ap-southeast-2) ami-0d2c929ace88cfebe ami-03b048bd9d3861ce9    KeyName :\n시작 이후 SSH를 사용하여 Work node에 연결하는 데 사용할 수 있는 Amazon EC2 SSH 키 페어 이름을 입력합니다.\nAmazon EC2 키 페어가 아직 없는 경우 AWS Management 콘솔에서 새로 생성할 수 있습니다.\n자세한 내용은 Linux 인스턴스용 Amazon EC2 사용 설명서의 Amazon EC2 키 페어를 참조하십시오.\n참고 : 여기에 키 페어를 입력하지 않으면 AWS CloudFormation 스택이 생성되지 않습니다.\n BootstrapArguments :\n별도의 kubelet 인수와 같이 Work node 부트스트랩 스크립트에 전달할 선택적 인수를 지정합니다.\n자세한 내용은 https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh에서 부트스트랩 스크립트 사용 정보를 참조하십시오.\n VpcId :\nWork node에서 시작해야 하는 VPC의 ID를 입력합니다.\n Subnets :\nWork node에서 시작해야 하는 위 VPC 내 서브넷을 선택합니다.\n 옵션 페이지에서 스택 리소스에 태그를 지정할 수 있습니다. [Next]를 선택합니다.\n 검토 페이지에서 정보를 검토하고, 스택이 IAM 리소스를 생성할 수 있음을 승인합니다. 항목을 체크 한 다음 생성을 선택합니다.\n 스택이 생성된 후 콘솔에서 이를 선택하고 출력을 선택합니다.\n 생성된 노드 그룹에 대해 NodeInstanceRole을 기록합니다. Amazon EKS Work node를 구성할 때 필요합니다.\n    5-2. Work node가 클러스터에 조인하도록 하려면  AWS IAM Authenticator 구성 맵을 다운로드, 편집 및 적용합니다.\n ConfigMap 다운로드:\ncurl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml  텍스트 편집기에서 파일을 엽니다. 인스턴스 역할의 \u0026lt;ARN(비인스턴스 프로파일)\u0026gt; 조각을 이전 절차에서 기록한 NodeInstanceRole 값으로 교체하고 파일을 저장합니다.\n중요:\n이 파일에서 어떠한 행도 수정하지 마십시오.\napiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: \u0026lt;ARN of instance role (not instance profile)\u0026gt; username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes  구성을 적용합니다. 이 명령을 완료하는 데 몇 분이 걸릴 수 있습니다.\nkubectl apply -f aws-auth-cm.yaml   노드의 상태를 확인하고 Ready 상태가 될 때까지 대기합니다.\nkubectl get nodes --watch  Kubernetes Metric Server 설치 한다.\n참조 : https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n Cocktail-addon을 설치 한다.\n  Amazon Elastic File System 만들기 Amazon Elastic File System (Amazon EFS)은 Amazon EC2에서 사용하기 위한 간단하고 확장 가능한 파일 스토리지를 제공합니다.\n참조 : https://docs.aws.amazon.com/ko_kr/efs/latest/ug/whatisefs.html\n1. 보안그룹 생성  EFS 파일 시스템이 사용할 보안 그룹을 생성 한다.  2. Amazon EFS 파일 시스템 생성  Amazon EFS 파일 시스템 생성\n Amazon EFS 관리 콘솔을 엽니다.\n Create File System(파일 시스템 생성)을 선택합니다.\n VPC와 연결된 시큐리티그룹을 선택(NFS 포트가 열려 있어야한다./ port : 2049)\n 목록에서 탑재 대상을 생성하기 원하는 서브넷을 선택한다.\n IP 주소를 자동으로 선택.\n [Next Step]을 선택합니다.\n 파일 시스템의 이름을 지정하고, 기본 성능 및 처리량 모드로 선택한 범용 및 버스팅 모드를 유지한 후 다음 단계를 선택합니다.\n Create File System(파일 시스템 생성)을 선택합니다.\n 목록에서 파일 시스템을 선택하고 File system ID(파일 시스템 ID)에 대한 값을 기록합니다. 다음 단계에 이 값이 필요합니다.\n   4. EFS Persistent Volumes on Kubernetes on AWS 참조 : Amazon EFS를 선택해야하는 경우\n Deploy the efs-provisioner\n참조 : https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs\nefs-provisioner를 사용하면 EFS 저장소를 kubernetes의 PersistentVolume으로 마운트 할 수 있습니다.\nAWS EFS 리소스에 액세스 할 수있는 컨테이너로 구성됩니다.\n컨테이너는 EFS 파일 시스템 ID, AWS 영역 및 efs-provisioner에 사용할 이름을 포함하는 configmap을 읽습니다.\n 아래 스크립트를 사용 합니다.\n kubectl apply -f 1-eks-cocktail-efs-rbac.yaml\n클러스터에 RBAC가 활성화되어 있거나 OpenShift를 실행중인 경우 공급자에게 권한을 부여해야합니다.\n\u0026ldquo;default\u0026rdquo;가 아닌 네임 스페이스 / 프로젝트에 있다면 rbac.yaml을 편집하십시오.\n  --- # ServiceAccount apiVersion: v1 kind: ServiceAccount metadata: name: efs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: efs-provisioner-runner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumes\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;persistentvolumeclaims\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-efs-provisioner subjects: - kind: ServiceAccount name: efs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: efs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-efs-provisioner rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;patch\u0026quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-efs-provisioner subjects: - kind: ServiceAccount name: efs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-efs-provisioner apiGroup: rbac.authorization.k8s.io   kubectl apply -f 2-eks-cocktail-efs-provisioner.yaml\nDeployment efs-provisione  --- apiVersion: v1 kind: ConfigMap metadata: name: efs-provisioner data: # your file.system.id , aws.region file.system.id: fs-b4c21dd5 aws.region: ap-northeast-2 provisioner.name: acornsoft.io/aws-efs dns.name: \u0026quot;\u0026quot; --- --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: efs-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: efs-provisioner spec: serviceAccount: efs-provisioner containers: - name: efs-provisioner image: quay.io/external_storage/efs-provisioner:latest env: - name: FILE_SYSTEM_ID valueFrom: configMapKeyRef: name: efs-provisioner key: file.system.id - name: AWS_REGION valueFrom: configMapKeyRef: name: efs-provisioner key: aws.region - name: DNS_NAME valueFrom: configMapKeyRef: name: efs-provisioner key: dns.name optional: true - name: PROVISIONER_NAME valueFrom: configMapKeyRef: name: efs-provisioner key: provisioner.name volumeMounts: - name: pv-volume mountPath: /persistentvolumes volumes: - name: pv-volume nfs: # your file-system DNS NAME server: fs-b4c21dd5.efs.ap-northeast-2.amazonaws.com path: / ---   kubetl apply -f 3-eks-cocktail-efs-storage-class.yaml\nCocktail 퍼시스턴트 볼륨 Storage Class 생성  # single-storage : EBS-PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: single-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: AWSEBS acornsoft.io/type: SINGLE k8s-addon: storage-aws.addons.k8s.io provisioner: kubernetes.io/aws-ebs reclaimPolicy: Delete volumeBindingMode: Immediate parameters: type: gp2 --- # shared-storage : EFS-PV apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: shared-storage annotations: storageclass.beta.kubernetes.io/is-default-class: \u0026quot;false\u0026quot; labels: acornsoft.io/provisioner-type: AWSEFS acornsoft.io/type: SHARED k8s-addon: storage-aws.addons.k8s.io # your provisioner.name provisioner: efs-test-pvc/aws-efs ---   4-eks-cocktail-efs-sample-pvc.yaml 샘플 PVC  # sample pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: efs-test-pvc annotations: volume.beta.kubernetes.io/storage-class: \u0026quot;shared-storage\u0026quot; spec: storageClassName: shared-storage accessModes: - ReadWriteMany resources: requests: storage: 1Mi ---   아래 명령으로 퍼시스턴트 볼륨을 확인 합니다.   kubectl get pv    "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.4-cluster-kass-eks-policy/",
	"title": "8.1.1.3 EKS 클러스터 정책 / 권한",
	"tags": [],
	"description": "",
	"content": " 사용자 권한 / 정책 만들기 AWS 관리자 권한이 없다면 필요한 역할 / 정책 / 권한 을 요청 해야 합니다.\n1. 사용자 권한 / 정책 사용자가 클러스터를 생성 / 관리할 수 있는 권한 / 정책이 필요 합니다.\n CloudFormation - 전체: 목록, 쓰기 제한: 읽기 / 이에 사용되는 리소스 권한이 있어야 합니다.\nAWS CloudFormation은 개발자와 시스템 관리자에게 관련 AWS 리소스 모음을 손쉽게 생성 및 관리하고 순서 있고 예측 가능한 방식으로 프로비저닝하고 업데이트 할 수있는 방법을 제공합니다.\n EC2 - AmazonEC2FullAccess 권한이 있어야 합니다.\nAmazon Elastic Compute Cloud (Amazon EC2)는 클라우드에서 안전하고 크기 조정이 가능한 컴퓨팅 용량을 제공하는 웹 서비스입니다.\n EKS - 모든 액세스 / 모든 리소스 권한이 있어야 합니다.\nAmazon Elastic Container Service for Kubernetes (EKS)는 자체 Kubernetes control plane을 설치, 작동 및 유지할 필요없이 Kubernetes를 AWS에서 쉽게 실행할 수 있도록 관리되는 Kubernetes 서비스입니다.\n IAM - 목록, 읽기, 쓰기, 권한 관리 / 이에 사용되는 리소스 권한이 있어야 합니다.\nAWS Identity and Access Management (IAM)를 사용하면 AWS 서비스 및 리소스에 대한 액세스를 안전하게 관리합니다.\n SNS - 목록 / 모든 리소스 권한이 있어야 합니다.\nAmazon Simple Notification Service(SNS)는 마이크로서비스, 분산 시스템 및 서버리스 애플리케이션을 쉽게 분리할 수 있게 해 주는 내구적이고 안전한 고가용성의 완전 관리형 게시/구독 메시징 서비스입니다\n VPC - AWS Management Console을 통해 Amazon VPC에 대한 완벽한 액세스를 제공합니다.\nAmazon Virtual Private Cloud(Amazon VPC)에서는 사용자가 정의한 가상 네트워크로 AWS 리소스를 시작할 수 있습니다.\n  2. 사용자 권한 / 정책 만들기  IAM(Identity and Access Management) 에서 정책(Policies)을 선택합니다.\n 정책 생성(Create policy) / 기존의 정책 편집(Edit policy)을 선택합니다.\n 권한 추가(Add additional permissions)를 선택합니다.\n 서비스 항목에서 필요한 서비스를 검색 / 선택합니다.\n CloudFormation - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAWS CloudFormation은 Amazon Web Services 리소스를 모델링하고 설정하는 데 도움이되는 서비스로, 리소스 관리 시간을 줄이고 AWS에서 실행되는 애플리케이션에 집중할 수 있습니다.\n원하는 모든 AWS 리소스 (예 : Amazon EC2 인스턴스 또는 Amazon RDS DB 인스턴스)를 설명하는 템플릿을 만들고 AWS CloudFormation은 이러한 리소스를 프로비저닝하고 구성하는 작업을 담당합니다.\nAWS 리소스를 개별적으로 생성하고 구성 할 필요가 없으며 무엇에 의존하는지 파악할 수 있습니다. Learn more\n EKS - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nKubernetes 용 Amazon Elastic Container Service (Amazon EKS)는 자체 Kubernetes control plane을 세우거나 유지할 필요없이 AWS에서 Kubernetes를 쉽게 실행할 수 있도록 해주는 관리 형 서비스입니다.\n IAM - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAWS ID 및 액세스 관리 (IAM)는 AWS 리소스에 대한 액세스를 안전하게 제어 할 수있는 웹 서비스입니다.\nIAM을 사용하여 자원을 사용하도록 인증 (로그인)되고 권한이 부여 된 (사용 권한이있는) 사용자를 제어합니다.\n SNS - 목록 에서 액세스 레벨 / 리소스 에서 필요한 권한을 선택합니다.\nAmazon SNS (Amazon Simple Notification Service)는 가입 엔드 포인트 또는 클라이언트로 메시지를 전달하거나 메시지 서비스를 조정 및 관리하는 웹 서비스입니다.\n게시자는 논리 액세스 포인트 및 통신 채널 인 주제에 메시지를 생성하고 전송하여 구독자와 비동기 적으로 통신합니다.\n  정책 검토(Review policy)를 선택합니다.\n 이름(Name) :\nAmazonEKSAdminPolicy와 같은 사용자 정책 이름을 입력 합니다.\n 설명(Description) :\n사용자 정책의 설명을 입력 합니다.\n  정책 생성을 선택합니다.\n  IAM(Identity and Access Management) 에서 사용자(Users)를 선택합니다.\n 생성한 또는 편집한 정책을 사용자에게 추가 합니다.\n 권한 추가(Add permission)를 선택합니다.\n 기존 정책 직접 연결(Attach existing policies directly )을 선택합니다.\n 생성 및 편집 한 정책을 검색 해서 사용자에게 권한 추가 합니다.\n 이 사용 설명서 에서는 다음 과 같은 정책으로 사용 하였습니다.\n AmazonEKSAdminPolicy :\nCloudFormation, EKS, IAM, SNS\n AmazonVPCFullAccess :\nProvides full access to Amazon VPC via the AWS Management Console.\nAmazon EC2를 사용하여 필요한만큼 가상 서버를 시작하고 보안 및 네트워킹을 구성하며 스토리지를 관리 할 수 있습니다.\n AmazonEC2FullAccess :\nProvides full access to Amazon EC2 via the AWS Management Console.\nAmazon Elastic Compute Cloud (Amazon EC2)는 Amazon Web Services (AWS) 클라우드에서 확장 가능한 컴퓨팅 용량을 제공합니다.\nAmazon EC2를 사용하여 필요한만큼 가상 서버를 시작하고 보안 및 네트워킹을 구성하며 스토리지를 관리 할 수 있습니다.\n     이 사용 설명서에서는 아래와 같이 권한을 사용합니다.   "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.1.kass/8.1.1.5-cluster-kass-eks-quick-start/",
	"title": "8.1.1.5 EKS 클러스터 만들기 Quick Start",
	"tags": [],
	"description": "",
	"content": "1. 필수 Role(역할) / Policy(정책) 만들기 a. EKS 관리 역할 만들기. b. IAM(services) \u0026gt; Roles \u0026gt; Create role 화면에서 i. EKS 선택 : AmazonEKSClusterPolicy , AmazonEKSServicePolicy (default로 구성되어 있다) ii. Next: Permissions \u0026gt; Next: Tags \u0026gt; Next: Revies iii. Review 화면에서 1) Role name : EKSClusterRole 2) Create role 클릭 c. EKS 관리 정책 만들기. d. IAM(services) \u0026gt; Policies \u0026gt; Create policy 화면에서 i. Choose a service \u0026gt; Service : EKS 검색 및 선택 ii. Action : All EKS actions 선택 iii. Resources : All Resources 선택 \u0026gt; Add additional permissions 클릭\u000biv. Choose a service \u0026gt; Service : SNS 검색 및 선택 v. Action : List \u0026gt; ListTopics 선택 \u0026gt; Review policy 클릭 vi. Review Policy 화면에서 1) Name* : AmazonEKSAdminPolicy 2) Create policy 클릭 2. EKS생성 하는 User 권한 / 정책 부여 하기 a. IAM(Services) \u0026gt; Users \u0026gt; EKS 생성 하는 user 클릭 i. Summary 화면에서 \u0026gt; Add permissions 클릭 ii. Grant permissions 에서 Attach existing policies directly 선택. 1) AmazonEKSAdminPolicy 2) AmazonEC2FullAccess 3) IAMFullAccess 4) AmazonEKSClusterPolicy 5) AmazonVPCFullAccess 6) AmazonEKSServicePolicy 7) AmazonElasticFileSystemFullAccess 8) AWSDeepRacerCloudFormationAccessPolicy iii. 위 목록을 검색 / 선택 해서 추가 한다. 3. 보안을 위한 아무 권한이 없는 칵테일 클러스터 등록 유져 만들기 a. IAM(services) \u0026gt; Users \u0026gt; Add user 화면에서 i. User name : acloud-client ii. Access type* : Programmatic access 선택 b. : \u0026gt; Next: Permissions \u0026gt; Next: Tags \u0026gt; Next: Review \u0026gt; Create user c. 완료되면 Download.csv를 받아서 저장한다. 4. VPC 만들기 - cloudformation template 사용 i. AWS console에서 CloudFormation 메뉴에서 \u0026quot;Create Stack\u0026quot;버튼 클릭 ii. Choose a template에서 \u0026quot;Specify an Amazon S3 template URL\u0026quot; 선택하고 아래 값을 입력 https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml iii. Stack name: eks-cocktail로 입력 iv. 2~3분정도 걸림. 5. EKS Cluster 생성 (Master 생성) a. AWS console에서 EKS 메뉴로 이동 i. Cluster name: cocktail-eks-cluster ii. Kubernetes Version : 1.12 iii. Role name : EKSClusterRole 선택 (1번에서 생성함) iv. VPC : eks-vpc-stack-VPC 선택 (4번에서 생성한 vpc) v. Subnet : default(모두 선택) vi. Security groups : eks-vpc-stack-ControlPlaneSecurityGroup 선택 (4번에서 생성된 security group) vii. Private access : Disabled (default) 선택 viii. Public access : Enabled (default) 선택 ix. Create -\u0026gt; 5분 정도 소요됨. 리플레쉬 버튼으로 확인 6. Aws-iam-authenticator, aws cli, kubectl 설치. (5번 진행되는 동안 설치 진행) a. Aws-iam-authenticator 설치: https://acornapps.github.io/cocktail-manual-3.1.1/docs/ko/8.1.1.3/ 의 3.1 참고 b. Aws cli 설치: https://acornapps.github.io/cocktail-manual-3.1.1/docs/ko/8.1.1.3/의 3.2 참고 c. Kubectl 설치: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl 참고 d. aws eks --region {region_code} update-kubeconfig --name {cluster_name} --kubeconfig {path_to_kubeconfig} e. export KUBECONFIG={path_to_kubeconfig} f. Kubectl get svc 로 접속확인 7. Node 생성 i. Cloudformation에서 Choose a template에서 \u0026quot;Specify an Amazon S3 template URL\u0026quot; 선택하고 아래 값을 입력 https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml ii. Specify Details 화면에서 i. Stack name: eks-nodes-stack ii. ClusterName : 생성한 EKS Cluster name iii. ClusterControlPlaneSecurityGroup : eks-vpc-stack-ControlPlaneSecurityGroup 선택 iv. NodeGroupName: cocktail-node-group v. NodeAutoScalingGroupMinSize : 1 (최소 1개) vi. NodeAutoScalingGroupDesiredCapacity : 2 (생성 할 노드 개수) vii. NodeAutoScalingGroupMaxSize : 2 (AutoScaling 최대 개수) viii. NodeInstanceType : t3.xlarge 선택 (설명 별첨 1) ix. NodeImageId : ami-0bfedee6a7845c26d (최신 ami 목록 별첨 2) x. NodeVolumeSize : 100 (노드 총 사이즈) xi. KeyName : 생성 한 키 페어 선택 xii. BootstrapArguments : 공백 (default) xiii. VpcId : 위에서 생성한 eks-vpc-stack-VPC 선택 xiv. Subnets : 위에서 생성한 eks-vpc-stack-Subnet01, eks-vpc-stack-Subnet02, eks-vpc-stack-Subnet03 xv. Next xvi. Options 화면은 default \u0026gt; Next xvii. Review 화면에서 I acknowledge that AWS CloudFormation might create IAM resources. 체크 xviii. Create iii. Node 생성 (5분내외 소요) 8. Node join 하기 i. curl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml ii. vi aws-auth-cm.yaml ``` apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: ( CloudFormation \u0026gt; 노드 생성 stack 에서 Outputs \u0026gt; NodeInstanceRole 의 value ) username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes mapUsers: | - userarn: ( IAM \u0026gt; Users \u0026gt; 위에서 생성한 acloud-client 선택 \u0026gt; User ARN 값 입력 ) username: admin groups: - system:masters ``` iii. kubectl apply -f aws-auth-cm.yaml iv. kubectl get cm --all-namespaces ``` NAMESPACE NAME DATA AGE kube-system aws-auth 2 20s ``` v. kubectl get nodes 9. EFS(Elastic File System) 생성 a. EFS 연동을 위한 Security Group 생성 i. EC2 \u0026gt; Security Groups 화면 에서 \u0026quot;Create Security Group\u0026quot; 버튼 클릭 -- Create Security Group -- Security group name : efs-sg Description : sfs-sg VPC : 4번에서 생성한 vpc 선택 ii. Add rule 버튼 클릭 Type : Custom TCP Protocol : TCP Port Range : 2049 Source : Custom, eks-cocktail-nodes-NodeSecurityGroup 을 선택하고 Create 버튼 클릭 b. Node에 생성된 Security Group 적용 i. EC2 \u0026gt; Instances 화면 에서 개별 노드 선택 ii. Action 클릭 \u0026gt; Networking 클릭 \u0026gt; Change Security Groups 선택 iii. Change Security Groups 화면 에서 생성된 efs-sg를 포함하여 \u0026quot;Assign Security Groups\u0026quot; 버튼을 클릭. c. EFS 생성 i. EFS \u0026gt; Create File System 버튼 클릭 ii. VPC : 4번에서 생성한 vpc 선택 iii. 하단의 Create mount targets의 security groups에 EFS 연동을 위해 생성한 security group만 선택 iv. Next Step 버튼 클릭 v. Configure optional settings 화면에서 수정없이 \u0026quot;Next Step\u0026quot; 버튼클릭. vi. Review and create 화면에서 \u0026quot;Create File System\u0026quot;버튼 클릭하여 생성함. 10. Cocktail Addon i. 제공되스 스크립트를 사용 하여 일관 apply 한다. i. ./addon.sh \u0026lt;your kubeconfig-path\u0026gt; \u0026lt;your efs-file-system-id\u0026gt; \u0026lt;your region\u0026gt; \u0026lt;your cocktail cluster-id\u0026gt; 11. Cocktail Cluster 등록 i. Cluster 등록 -- Provider -- i. Provider : Amazon Web Service ii. Type : EKS iii. Region : Tokyo iv. Cluster ID : \u0026lt;your EKS Cluster Name\u0026gt; -- Basic Infomation -- i. name : \u0026lt;your Cocktail Cluster Name\u0026gt; ii. Kubernetes Version : 1.12.6 (EKS Kubernetes Version) iii. ID : \u0026lt;your Cocktail Cluster Id\u0026gt; -- Endpoint -- i. Master address : \u0026lt;your EKS API server endpoint\u0026gt; ii. Ingress host address : \u0026lt;your Load Balancer DNS name\u0026gt;(선택사항) 1) LB 없을시 Work node의 public ip를 사용 해서 확인할 수 있다. iii. Node Port Host Address : \u0026lt;your Load Balancer DNS name\u0026gt;(선택사항) 1) LB 없을시 Work node의 public ip를 사용 해서 확인할 수 있다. iv. Node Port Range : 30000-32767 -- User Account Authentication Information -- i. Cluster CA Certification : \u0026lt;your EKS화면에서 Certificate authority 값\u0026gt; ii. AWS Access Key ID : \u0026lt;your acloud-client Access Key ID\u0026gt;(보안을 위해 생성한 더미 유저) iii. AWS Secret Access Key : \u0026lt;your acloud-client Secret Access Key\u0026gt;(보안을 위해 생성한 더미 유저) ii. Workspace Create i. Workspace Name : \u0026lt;your workspaces name\u0026gt; ii. Image Registry : \u0026lt;your workspaces name\u0026gt; iii. Cluster Add 클릭 1) 등록한 Cocktail Cluster를 선택 후 생성 / 업데이트 한다. 참조 : EKS cluster 에서 elb 생성 없이 nodepott / ingress 확인 하기 i. Work Node의 public ip로 확인 할 수 있다. 1) Cocktail cluster 등록 / 수정 화면에서 인그레스 호스트 주소 , 노트 포트 호스트 주소 를 수정 한다. ## 별첨 목록 1. AWS instance별 pod 제한 개수: https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt 2. 최신 ami 목록 참조 : https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html  "
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/8.operation/8.1.cluster-install/8.1.2.onpremise/8.1.2-cluster-on-premise/",
	"title": "8.1.2.1 클러스터 직접 구성",
	"tags": [],
	"description": "",
	"content": " Google cloud에 칵테일을 설치하는 과정은 다음과 같다.\n사전준비 설치 전 인스턴스 및 아래와 같은 프로그램들이 미리 설치 되어 있어야 하며 설치 되어 있지 않을 경우 에러 메시지가 발생한다.\n1) 제공받은 cube 바이너리를 어느 디렉토리에서든 사용할 수 있도록 환경변수 path 설정을 한다.\n2) Docker 다운로드 후 설치\nGet Docker CE for CentOS\n설치 1. 설치를 위해 빈 디렉토리를 만든 후 해당 디렉토리로 이동한다.\n# mkdir /Desktop/cube # cd /Desktop/cube  2. cube 명령의 -p Flag를 사용해서 각 Provider의 설치 script를 download 받고 초기화 한다.\n   Provider list     onpremise   azure   aws   gcp   aliyun    # cube init -p \u0026lt;Provider\u0026gt;  3. cube.toml 파일을 편집기로 열어서 Provider 정보 및 인스턴스 정보를 기입한다.\nProvider 별로 cube.toml 작성법. 1. GCP, Aliyun, Onpremise cube.toml 작성. | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-ip | Storage IP : Persistent Volume | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p gcp  [cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;gcp\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;\u0026quot; private-key-path = \u0026quot;\u0026quot; key-path = \u0026quot;\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] install = true name = \u0026quot;shared\u0026quot; storage-ip = \u0026quot;\u0026quot; storage-mount-dir = \u0026quot;/storage\u0026quot; storage-volume-size = 100 [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  2. AZURE cube.toml 과 provider_info.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | storage-account | Storage account 이름 | storage-account = \u0026ldquo;cocktailcube\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n[cube] version = \u0026quot;1.13.2-r1-m1\u0026quot; provider = true cluster-name = \u0026quot;cube-azure\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;cube-azure\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, \u0026quot;10.0.2.5\u0026quot;, ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;azure\u0026quot; location = \u0026quot;koreacentral\u0026quot; virtual-network-name = \u0026quot;cocktail-vnet\u0026quot; subnet-name = \u0026quot;cocktail-subnet\u0026quot; security-group-name = \u0026quot;worker-nsg\u0026quot; primary-availablity-set-name = \u0026quot;worker-avs\u0026quot; resource-group = \u0026quot;cube-1131\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cocktail\u0026quot; private-key-path = \u0026quot;/root/cubetest/cert/id_rsa\u0026quot; key-path = \u0026quot;/root/cubetest/cert/id_rsa.pub\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;10.0.2.10\u0026quot;, \u0026quot;10.0.2.9\u0026quot;, ] loadbalancer-ip = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; ingress-host = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-port-url = \u0026quot;cube-1131.koreacentral.cloudapp.azure.com\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;10.0.2.5\u0026quot;, ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] storage-account = \u0026quot;cocktailcube\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;10.0.2.7\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;   provider_info.toml\n참조 : Cloud provider for Azure  | [azure] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | subscription-id | 클러스터가 배포 된 Azure Subscription의 ID | subscription-id = \u0026ldquo;\u0026rdquo; | | client-id | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientID | client-id = \u0026ldquo;\u0026rdquo; | | client-secret | Azure RM API와 대화하기위한 RBAC 액세스가있는 응용 프로그램의 ClientSecret | client-secret = \u0026ldquo;\u0026rdquo; | | tenant-id | 클러스터가 배포 된 Subscription의 Tenant ID | tenant-id = \u0026ldquo;\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  [azure] subscription-id = \u0026quot;\u0026quot; client-id = \u0026quot;\u0026quot; client-secret = \u0026quot;\u0026quot; tenant-id = \u0026quot;\u0026quot;  3. AWS cube.toml 작성.  cube.toml  | [cube] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | version | kubernetes 버젼별 지원 cube version | version = \u0026ldquo;1.13.1-r2\u0026rdquo; | | provider | provider 사용 설정 | provider = false | | cluster-name | Cluster Name 부여 | cluster-name = \u0026ldquo;test-cluster\u0026rdquo; | | cluster-description | Cluster Description | cluster-description = \u0026ldquo;This is test cluster\u0026rdquo; | | cluster-type | 클러스터의 사용 규모와 HA 구성 여부에 따라 타입을 구분 | cluster-type = \u0026ldquo;small\u0026rdquo; | | cluster-id | Cluster 고유 ID | cluster-id = \u0026ldquo;test-cluster-id\u0026rdquo; |\n| [kubenetes.etcd] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ip | etcd 설치 노드 Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [node-pool.provider] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Node Pool Provider Name | name = \u0026ldquo;gcp\u0026rdquo; | | location | Node Region Zone Name | location = \u0026ldquo;asia-east1\u0026rdquo; | | virtual-network-name | 클라우드의 개인 네트워크 | virtual-network-name = \u0026ldquo;cocktail-vnet\u0026rdquo; | | subnet-name | 가상 네트워크 서브넷 | subnet-name = \u0026ldquo;cocktail-subnet\u0026rdquo; | | security-group-name | 보안 그룹 Name | security-group-name = \u0026ldquo;worker-nsg\u0026rdquo; | | primary-availablity-set-name | Primary availablity set name | primary-availablity-set-name = \u0026ldquo;worker-avs\u0026rdquo; | | resource-group | 리소스 그룹 | resource-group = \u0026ldquo;cube-1131\u0026rdquo; |\n| [node-pool.security] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | ssh-user-id | SSH User ID | ssh-user-id = \u0026ldquo;cloud\u0026rdquo; | | private-key-path | Private key path | private-key-path = \u0026ldquo;/root/cubetest/cert/id_rsa\u0026rdquo; | | key-path | Node Region Zone Name | key-path = \u0026ldquo;/root/cubetest/cert/id_rsa.pub\u0026rdquo; |\n| [node-pool.master] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | name | Master Node Name | name = \u0026ldquo;master\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | loadbalancer-ip | Loadbalancer IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | ingress-host | ingress-host IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [[node-pool.nodes]] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | node-port-url | node-port-url IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; | | name | Worker Node Name | name = \u0026ldquo;default\u0026rdquo; | | ip | Master 설치 Node Internal IP | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n| [shared-storage] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | efs-file-system-id | EFS File system id | efs-file-system-id = \u0026ldquo;\u0026rdquo; |\n| [private-registry] | | 항목 | 설명 | 예시 | | \u0026mdash;- | \u0026mdash;- | \u0026mdash;- | | registry-ip | registry IP : harbor 설치 | ip = \u0026ldquo;Your Primary internal IP\u0026rdquo; |\n cube init 명령으로 최초 cube.toml 파을을 생성후 Provider 정보 및 인스턴스 정보를 기입한다.  # cube init -p azure  \u0026ndash; cube.toml \u0026ndash;\n[cube] version = \u0026quot;1.13.1-r2\u0026quot; provider = false cluster-name = \u0026quot;test-cluster\u0026quot; cluster-description = \u0026quot;This is test cluster\u0026quot; cluster-type = \u0026quot;small\u0026quot; cluster-id = \u0026quot;test-cluster-id\u0026quot; alert-language = \u0026quot;ko\u0026quot; [kubenetes] service-cidr = \u0026quot;10.96.0.0/12\u0026quot; pod-cidr = \u0026quot;10.10.0.0/16\u0026quot; [kubenetes.etcd] ip = [ \u0026quot;\u0026quot; ] [node-pool] data-dir = \u0026quot;/data\u0026quot; [node-pool.provider] name = \u0026quot;aws\u0026quot; location = \u0026quot;default\u0026quot; [node-pool.security] ssh-user-id = \u0026quot;cloud\u0026quot; private-key-path = \u0026quot;/root/cubetest/provider_info.toml\u0026quot; key-path = \u0026quot;provider_info.toml\u0026quot; [node-pool.master] name = \u0026quot;master\u0026quot; ip = [ \u0026quot;\u0026quot; ] loadbalancer-ip = \u0026quot;\u0026quot; ingress-host = \u0026quot;\u0026quot; node-port-url = \u0026quot;\u0026quot; node-portrange = \u0026quot;30000-32767\u0026quot; [[node-pool.nodes]] name = \u0026quot;default\u0026quot; ip = [ \u0026quot;\u0026quot; ] [enterprise-product] install = true release-name = \u0026quot;cocktail\u0026quot; https = false [enterprise-product.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot; [addon] install = true [shared-storage] efs-file-system-id = \u0026quot;\u0026quot; [private-registry] install = true name = \u0026quot;harbor\u0026quot; registry-ip = \u0026quot;\u0026quot; data-dir = \u0026quot;/data\u0026quot; public-cert = false [private-registry.cert-file] ssl-certificate = \u0026quot;\u0026quot; ssl-certificate-key = \u0026quot;\u0026quot;  ※ ssh key로 접속하기\n상기 항목에서 private_key_path 와 key_path 는 서버에 ssh key로 접속하기 위한 private key와 public key의 경로를 기입한다.\n이미 존재하는 경우에는 해당 경로를 기입하면 되고, 신규로 생성할 경우에는 아래 절차대로 실행하면 된다.\n\u0026lt; ssh key 신규 발급 방법 \u0026gt;\n# ssh-keygen -f /path/to/private_file -t rsa -N ''  4. cube create 명령을 이용하여 CUBE Cluster를 설치한다.\n# cube create  5. 오류없이 설치가 완료되면 system을 구성하는 컨테이너가 정상적으로 기동되고 있는지 아래 명령으로 확인한다.\n# cube status  **6. 브라우저로 http://lb:30000으로 접속하면 login 화면으로 접속할 수 있다. (초기 접속 계정: admin/Pass0000)\n"
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://acornapps.github.io/cocktail-manual-3.3.0-public/zh/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]